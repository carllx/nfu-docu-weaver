# 【HCI Invited Talk】姜峣：机器人交互感知与自主化操作
(-- `【HCI Invited Talk】姜峣：机器人交互感知与自主化操作` [bilibili](https://www.bilibili.com/video/BV1te4y1j7nc/?t=34))

姜峣: 清华助理研究员, 特点是基于视觉（Vision-based）的触觉传感器形式.
### Technical Terms
- **Tactile Sensing:** The ability of a robot to perceive its environment through touch.
- **Autonomous Operation:** The capability of a robot to perform tasks independently, without human intervention.
- **Precision Technology:** Refers to machinery or equipment designed to operate with a high degree of accuracy.

## 局限性
- **局部感知的限制**：仅依赖触觉的操作类似于“盲人摸象”，只能感受到局部，无法获得完整的认知。这种局部感知限制了机器人在复杂环境中的操作​. 尽管触觉对于机器人操作至关重要，但仍需结合其他感知方式，如视觉、听觉等，以实现更全面、高效的操作。
- 
## Transcribe
```
[00:00.000 -> 00:02.880] 所以我的题目就叫机器人交互感知与自主化操作
[00:02.880 -> 00:05.360] 这也是我这边的主要研究方向
[00:06.880 -> 00:08.360] 我的报告主要分为三个部分
[00:08.360 -> 00:11.320] 第一个我还是对个人的基本情况做一个介绍
[00:11.320 -> 00:13.960] 第二部分是这次交流报告的一个主要部分
[00:14.280 -> 00:16.240] 就是机器人交互感知与自主化操作
[00:16.520 -> 00:18.960] 最后还有做了一两页片子
[00:18.960 -> 00:22.600] 就是想介绍一下我们后续对这个方向的思考和规划
[00:23.640 -> 00:26.000] 首先做一个个人基本情况介绍
[00:26.000 -> 00:27.280] 刚才龚老师已经说了
[00:27.520 -> 00:30.920] 我是在11年在清华读的博士
[00:30.920 -> 00:32.640] 但我在博士期间做的工作
[00:32.640 -> 00:34.600] 其实跟现在工作差异很大
[00:34.800 -> 00:37.640] 之前是做的偏基础上方向
[00:37.640 -> 00:40.960] 就是如何保证一个机器的进度保证技术
[00:41.520 -> 00:43.640] 所以当时在做这样的一个方向的时候
[00:43.640 -> 00:46.400] 因为这是我们以前大科理从97年开始
[00:46.400 -> 00:48.360] 就是汪精松副校长
[00:48.360 -> 00:50.920] 现在在电子科技大学当校长
[00:50.920 -> 00:53.600] 当时他最早开始做这个病联技术
[00:53.600 -> 00:55.560] 所以我在11年来到我们组的时候
[00:55.560 -> 00:57.520] 其实我们还是以病联技术为主
[00:57.520 -> 00:59.760] 在当时有一个核心问题
[00:59.760 -> 01:02.280] 就是怎么提升这一类技术的一个精度
[01:02.280 -> 01:04.440] 所以我在博士期间主要是做的就是
[01:04.440 -> 01:06.840] 融合结构智能化这样的一个设计思想
[01:07.280 -> 01:08.800] 提升并联机器的精度
[01:09.080 -> 01:10.800] 最终可以研发的样机
[01:10.960 -> 01:13.480] 可以达到全球定位精度优于一微米
[01:13.560 -> 01:15.680] 基本上跟高精度串联机设想相当
[01:17.520 -> 01:19.480] 所以这是我博世旗舰的一个主要工作
[01:19.880 -> 01:22.640] 然后我后面有两段博后经历
[01:23.160 -> 01:24.640] 第一段是在静音系做的
[01:26.720 -> 01:29.840] 其实还是延续了我博士经验的方向后面有一年是在哈佛那边做了博后
[01:29.840 -> 01:31.840] 其实这个对我的影响非常之大
[01:31.840 -> 01:34.720] 因为它的方向转变很大
[01:34.720 -> 01:36.720] 可以发现是人脑学习新闻特性研究
[01:36.720 -> 01:38.720] 其实这个在早的时候
[01:38.720 -> 01:41.920] 我主要不太想再从事以前的这种
[01:41.920 -> 01:43.920] 加工制造方向
[01:43.920 -> 01:45.000] 想做一些新的方向
[01:45.000 -> 01:46.560] 所以当时的跨度很大
[01:46.560 -> 01:48.040] 然后去那边以后
[01:48.040 -> 01:50.920] 虽然只待了一年的时间
[01:50.920 -> 01:52.120] 但是那边让我了解到
[01:52.120 -> 01:54.840] 就是说从人的这个行为特性来看
[01:54.840 -> 01:56.720] 以及跟机器人之间的关联关系
[01:56.720 -> 01:58.160] 所以我回来以后
[01:58.160 -> 02:01.080] 从18年8月份入职开始
[02:01.080 -> 02:02.360] 一直到现在
[02:02.360 -> 02:04.920] 主要就转向做机器人自主化操作这个方向
[02:04.920 -> 02:07.400] 这是我个人的一个基本的经历
[02:07.400 -> 02:10.320] 接下来我就想讲一下
[02:10.320 -> 02:15.200] 我们差不多有4年的时间做了一些工作
[02:15.200 -> 02:17.400] 首先介绍一个背景
[02:17.400 -> 02:20.880] 就是说我在这边提到自主化
[02:20.880 -> 02:25.120] 我们非常清楚在这一页上其实是面向一种
[02:25.120 -> 02:30.240] 尤其是汽车行业为代表的就是工业机器人最早应用的
[02:30.240 -> 02:31.600] 就是很成功的一个场景
[02:31.600 -> 02:34.240] 但他们这种场景更多的是叫自动化操作
[02:34.240 -> 02:36.480] 它的特点在任务层面的特点
[02:36.480 -> 02:41.680] 其实就是已知和确定就是这个事情事先我们都知道
[02:41.680 -> 02:43.440] 然后它的东西也是确定的
[02:43.440 -> 02:45.680] 所以我可以通过预编程通过重复的方式来干这个事情事先我们都知道然后它的东西也是确定的所以我可以通过预变程
[02:46.000 -> 02:47.600] 通过重复的方式来干这个事情
[02:47.600 -> 02:50.440] 这是自动化操作和自动化操作任务的一些特点
[02:51.000 -> 02:52.800] 但是随着我们的场景越来越复杂
[02:52.800 -> 02:54.120] 我们操作对象越来越复杂
[02:54.640 -> 02:57.320] 而这种以重复性和预变程的自动化操作模式
[02:57.320 -> 03:00.320] 已经几乎没法满足现在的一个应用需求
[03:01.040 -> 03:02.680] 比如在家用环境中
[03:02.680 -> 03:05.000] 这个事情是不太可能已知和确定的
[03:05.000 -> 03:07.000] 所以在这个情况下
[03:07.000 -> 03:10.000] 在现在的这样的一个机器人的操作
[03:10.000 -> 03:12.000] 逐步的已经发展到对自主化的要求
[03:12.000 -> 03:14.000] 当然现在还没到智能化
[03:14.000 -> 03:16.000] 因为智能化还有一些更深层次的问题
[03:16.000 -> 03:17.000] 没有被解决
[03:17.000 -> 03:18.000] 但自主化操作就是
[03:18.000 -> 03:20.000] autonomous manipulation这个事情
[03:20.000 -> 03:23.000] 在国外已经得到了很广泛的研究
[03:23.000 -> 03:24.000] 它的特点任务就是说
[03:24.000 -> 03:26.080] 这个事情一定是未知的
[03:26.080 -> 03:28.200] 除了未知而这个事情是变化的
[03:28.200 -> 03:29.840] 如果只是未知不变化
[03:29.840 -> 03:31.840] 那我可以通过辨识的方式
[03:31.840 -> 03:34.240] 辨识完以后可以通过再转到自动化
[03:34.240 -> 03:36.400] 但这个事情它不仅未知而现在不但变化
[03:36.400 -> 03:40.120] 所以在自动化这个操作层面的所有的基础
[03:40.120 -> 03:42.840] 基本上就不太可能来输入到自主化操作
[03:42.840 -> 03:45.680] 所以它需要具备一些自信和自学习
[03:45.680 -> 03:48.400] 这是这个自主化操作里面所需要的一些核心的问题
[03:48.400 -> 03:52.040] 简单来说就是左边是能做 后边是会做
[03:52.040 -> 03:53.840] 自主化操作是会做
[03:53.840 -> 03:58.240] 所以这也是我们下一代疫情操作能力的一个必然的发展趋势
[03:58.240 -> 04:00.480] 但这个事情其实非常困难
[04:00.480 -> 04:08.000] 在这个像CMU的这个到的應該是18年的愛奇博的終身成就獎
[04:08.000 -> 04:12.000] 他做了三十幾年的自主化操作
[04:12.000 -> 04:16.000] 他在有一篇宗書裡面就寫到了
[04:16.000 -> 04:19.000] 他說機器人操作是機器人領域最具挑戰性的研究之一
[04:19.000 -> 04:21.000] 並且發展研究之後
[04:21.000 -> 04:24.000] 在自主化操作能力方面與人類相比差距巨大
[04:24.000 -> 04:26.240] 在Science上最近的一篇这个
[04:26.760 -> 04:28.480] 宗书里面也提到同样的观点
[04:29.040 -> 04:31.880] 它的瓶颈挑战其实主要是包括这几个部分
[04:32.120 -> 04:33.000] 第一个是感知部分
[04:33.240 -> 04:35.680] 就这个自主化操作一定要交互感知
[04:36.240 -> 04:38.720] 但交互感知能力其实机器人与人类相比
[04:38.800 -> 04:39.680] 这个差距还是巨大的
[04:40.280 -> 04:43.600] 第二个需要有非常灵巧的一个操作的一个机械手
[04:44.160 -> 04:44.760] 就像人手一样
[04:44.760 -> 04:46.680] 就是因为我们现在不能叫机械手
[04:46.680 -> 04:48.720] 更多是工具
[04:48.720 -> 04:50.920] 就是说针对不同任务制定不同的末端
[04:51.360 -> 04:53.920] 它的灵巧性和适应性差别很大
[04:54.360 -> 04:57.480] 第三个就是说它的操作技能发挂能力很难
[04:57.480 -> 04:59.480] 就是说我可以干一种特定任务
[04:59.480 -> 05:01.000] 但是我想干很广泛的任务
[05:01.000 -> 05:02.200] 这个事情就比较困难
[05:02.960 -> 05:04.280] 所以针对这样的难点
[05:04.280 -> 05:07.080] 这也是我们科技组的一个主要方向
[05:07.080 -> 05:10.760] 其实就面向这张图可以很清晰的表现出来
[05:10.760 -> 05:12.960] 就是四个部分控、感、趋、用
[05:13.640 -> 05:15.240] 感就是做机械人的感知
[05:15.240 -> 05:16.800] 但我们追求于触觉
[05:17.240 -> 05:19.400] 然后这个趋就是做机械人的接触
[05:19.680 -> 05:20.800] 控就是里面很核心的
[05:20.800 -> 05:22.800] 就是怎么做自主化的操作技能
[05:23.120 -> 05:27.080] 然后用就是包括器械层面的这样研发和整个应用场景的拓展
[05:27.080 -> 05:29.600] 这就是我们整个的一个方向的概率
[05:30.680 -> 05:31.640] 但这个方案很大
[05:31.640 -> 05:34.200] 就是我们刚开始发现的时候就开始找气球点
[05:34.200 -> 05:36.680] 其实气球点最开始我们从抓取这个事情出发的
[05:36.680 -> 05:38.880] 但在做抓取的时候发现传感器不行
[05:38.880 -> 05:39.400] 拿不到
[05:39.400 -> 05:42.080] 本来就硬逼着自己做了对传感器
[05:42.440 -> 05:48.000] 然后以及在做我们一个项目驱动的就是说怎么在大属空间下
[05:48.000 -> 05:49.560] 能做一些高精度的装配任务
[05:49.560 -> 05:51.840] 所以这次我就分为这三个部分
[05:51.840 -> 05:53.960] 其中这个杆这一块主要是面向于
[05:53.960 -> 05:55.640] 指尖多模特推举杆支
[05:55.640 -> 05:57.480] 这也是我们做的相对来说比较系统的
[05:57.480 -> 05:59.120] 也是这次报告里面我主要介绍的
[05:59.120 -> 06:00.800] 后面哪部分是我们正在做
[06:00.800 -> 06:02.880] 但是这个比较困难
[06:02.880 -> 06:03.720] 我们更多的是
[06:03.720 -> 06:05.640] 我这边就简单谈一下
[06:05.640 -> 06:07.040] 我们在这边做的一些事情
[06:07.880 -> 06:09.760] 那第一部分我就先介绍一下
[06:09.760 -> 06:12.280] 这个动模态的触觉感知 这方面的工作
[06:15.280 -> 06:17.520] 首先我们看一下就是视觉和触觉感知
[06:17.520 -> 06:19.080] 在操作任务中它会有什么作用
[06:19.240 -> 06:21.040] 比方说这边是人在抓一个杯子
[06:21.040 -> 06:22.320] 这么非常简单的一个事情
[06:22.680 -> 06:23.800] 首先他肯定会有视觉
[06:23.840 -> 06:25.080] 他会看到一些东西
[06:25.080 -> 06:26.000] 比方说这是水杯
[06:26.000 -> 06:28.800] 圆柱形的 白色的 空的 不大 在手上
[06:28.800 -> 06:31.480] 这些一些很直观的视觉可以获取的
[06:31.480 -> 06:33.240] 同样的也可以有触觉
[06:33.240 -> 06:34.280] 触觉就是抓的时候
[06:34.280 -> 06:35.200] 发生接触以后
[06:35.200 -> 06:36.560] 我发现这个杯子很光滑
[06:36.560 -> 06:37.680] 然后表面弧形的
[06:37.680 -> 06:38.840] 但也很坚硬
[06:38.840 -> 06:39.520] 而不太重
[06:39.520 -> 06:40.680] 抓住位置也合适
[06:40.680 -> 06:44.080] 所以视触觉在一个任务里面
[06:44.080 -> 06:45.320] 会同时发生作用如果我们只有触觉因为触觉在这个一个任务里面会同时发生作用
[06:45.560 -> 06:47.000] 如果我们只有触觉
[06:47.720 -> 06:49.360] 因为触觉的探索很高效
[06:49.360 -> 06:50.520] 因为它是看到全局
[06:50.880 -> 06:52.480] 但是它因为缺少这个借助感知
[06:52.480 -> 06:54.160] 它的操作可容易比较低
[06:54.160 -> 06:55.320] 比方说我成功没成功
[06:55.320 -> 06:56.400] 或者对这个感知的
[06:56.640 -> 06:58.040] 借助反馈的东西基本上是
[06:58.680 -> 06:59.440] 一无所知的
[07:00.400 -> 07:01.680] 如果我只用触觉
[07:01.920 -> 07:02.800] 当然它操作
[07:03.360 -> 07:04.520] 是肯定稳定可靠的
[07:07.160 -> 07:08.560] 但问题它就会出现像盲人摸象它就会只能感受到局部
[07:08.560 -> 07:10.760] 整体认知比较的差
[07:10.760 -> 07:12.600] 操作效率也会比较低下
[07:12.600 -> 07:13.880] 所以从这角度来看的话
[07:13.880 -> 07:16.400] 就是视触觉在操作任务这个层面
[07:16.400 -> 07:18.440] 一定是无法互替 缺一不可的
[07:18.440 -> 07:20.920] 它跟现在积极视觉比方做一些识别
[07:20.920 -> 07:21.960] 这种非接触式任务
[07:21.960 -> 07:24.200] 做识别 做导航这些东西它是可以的
[07:24.200 -> 07:26.000] 不用触觉无所谓但是操作非接触式任务做识别 做导航 这些东西它是可以的不用识别无所谓 但是操作里面
[07:26.000 -> 07:28.000] 触觉是一定需要的
[07:28.000 -> 07:30.000] 所以从GG视觉 从学者的角度来看
[07:30.000 -> 07:32.000] 其实GG视觉更多的是
[07:32.000 -> 07:34.000] 或者计算机视觉嘛 就从图样中
[07:34.000 -> 07:36.000] 重构场景的三维结构信息
[07:36.000 -> 07:38.000] 以及从这个图样中
[07:38.000 -> 07:40.000] 提取观测对象和行为的语义
[07:40.000 -> 07:42.000] 所以这是这个GG视觉
[07:42.000 -> 07:44.000] 但它的发展也比较成熟
[07:44.000 -> 07:45.800] 供应应用当然十分广泛
[07:45.800 -> 07:48.300] 相比来说TG触觉其实想找个定义都很困难
[07:48.300 -> 07:49.700] 所以我就照不如画表
[07:49.700 -> 07:51.000] 就是说这个TG触觉
[07:51.000 -> 07:52.300] 我们更多是从信号中
[07:52.300 -> 07:54.300] 重复接触界面的医学信息
[07:54.300 -> 07:57.500] 同样的是提取出这个操作对象和行为的语音
[07:57.500 -> 07:59.300] 它的发展方面非常滞后
[07:59.300 -> 08:02.500] 就是目前来说还是很难以满足应用需求
[08:02.500 -> 08:06.240] 因为触觉这个事情
[08:06.800 -> 08:07.880] 它比较的大
[08:07.880 -> 08:10.440] 因为人身上皮肤各方面全是触觉
[08:10.880 -> 08:11.840] 所以我们来做的时候
[08:11.840 -> 08:13.560] 我们更多是面向操作来用
[08:13.800 -> 08:15.480] 并不像皮肤感觉疼痛
[08:15.600 -> 08:17.560] 或者是做一些其他方面的需求
[08:17.960 -> 08:20.760] 所以我们这边面向的是叫指尖触觉
[08:20.760 -> 08:22.440] 就是说人手上的
[08:22.800 -> 08:25.000] 然后指尖触觉从这张图上可以看出
[08:25.000 -> 08:28.000] 这是Laser上面的一个动素
[08:28.000 -> 08:31.000] 上面这个人手的触觉里面最灵敏的
[08:31.000 -> 08:33.000] 信息密度最丰富的就是在手指指尖
[08:33.000 -> 08:37.000] 它的每平方厘米有多达140个这样感受器
[08:37.000 -> 08:40.000] 所以它的感受会比任何一个地方都灵敏
[08:40.000 -> 08:44.000] 这也是人手为什么这么灵巧的一个很主要的原因之一
[08:44.000 -> 08:46.920] 所以它在抓一个东西的时候他会感受到很多的信息
[08:47.400 -> 08:48.840] 所以我们在做这个指尖测试的时候
[08:48.840 -> 08:51.680] 第一个我们得知道我们要干这个事情的需求是什么
[08:52.120 -> 08:55.880] 这个需求总体而言我们就是面向指尖动物来测试感知
[08:56.360 -> 08:57.720] 主要要有这几个需求
[08:57.720 -> 08:58.560] 第一个得紧凑
[08:58.560 -> 08:59.840] 就是这个场域不能太大
[09:00.000 -> 09:02.640] 因为它得在就是这个地界层集成
[09:03.080 -> 09:05.880] 然后中间三个是它的一个具体性的指标
[09:05.920 -> 09:08.600] 我需要这个信息感受是精确的
[09:08.760 -> 09:10.720] 精细的就是分辨率高和实时的
[09:10.960 -> 09:13.120] 因为它用来操作它必须要交互
[09:13.680 -> 09:14.520] 另外一个就可靠
[09:14.920 -> 09:17.120] 因为可靠在这个处理员里面还是挺困难的
[09:17.120 -> 09:20.280] 因为他他接触以后他他很多事情就发生变化
[09:20.800 -> 09:23.680] 所以他这个可靠性的保证是这三个性的指标基础
[09:24.360 -> 09:25.480] 然后另外还有一个叫多模态
[09:25.480 -> 09:26.880] 就多模态这个事情怎么理解
[09:26.880 -> 09:28.320] 其实大家有不同的理解
[09:28.320 -> 09:31.520] 然后这一块我看了一下就计算机视觉的这样一个分类
[09:31.520 -> 09:34.000] 我觉得一个挺好的分类就是它把它分成底层信息
[09:34.000 -> 09:35.280] 中等信息和高等信息
[09:35.280 -> 09:36.880] 我觉得触觉也可以这么干
[09:36.880 -> 09:39.440] 就地层信息就是这个最原始的触觉信息
[09:39.440 -> 09:41.320] 就像相机里面绝对就是图片
[09:41.320 -> 09:44.320] 它可以用于重构出我们需要的后续的所有信息
[09:44.320 -> 09:46.640] 但地层信息这个形式是不唯一的
[09:46.640 -> 09:49.360] 它绝对是我们所采用的传感器的形式
[09:50.720 -> 09:51.640] 第二个是重层信息
[09:51.640 -> 09:54.920] 重层信息我们定义成就是接触界面和物体及环境的力学属性
[09:55.480 -> 09:57.800] 当然这个信息种类是传感器是没关系的
[09:58.000 -> 10:00.840] 但是它是通过模型被依赖于传感器
[10:01.440 -> 10:02.720] 最后一个是高层信息
[10:03.080 -> 10:06.520] 高层信息在视觉里面高层信息可能就涉及到物体的识别
[10:06.520 -> 10:09.040] 分类识别,然后一些追踪各方面的东西
[10:09.440 -> 10:11.240] 它其实已经脱离了传感器本身
[10:11.240 -> 10:13.360] 它更多的更是操作状态行为的理解
[10:13.640 -> 10:16.400] 它是一个信息和任务之间的桥梁
[10:16.920 -> 10:18.800] 所以它是传感器的形式没有任何关联
[10:18.800 -> 10:20.680] 它是操作任务紧密关联
[10:21.160 -> 10:24.440] 所以在传感器层面上我们更关注的前面两部分
[10:24.440 -> 10:28.200] 就是地层信息和众人信息的获取高层信息更多的是已经跟任务进行关联所以在传感器层面上我们这个更关注的前面两个部分就是地层信息和重人信息的获取高人信息更多的是已经跟任务进行关联
[10:29.920 -> 10:31.520] 那接下来我们第一件事情就是要
[10:32.120 -> 10:34.800] 开始就是确定我们采用什么的传感器形式
[10:34.800 -> 10:37.000] 因为传感器的形式不同这个差异很大
[10:37.480 -> 10:41.040] 而目前研究的比较多的是类似这种测量阵列形式的
[10:41.400 -> 10:44.640] 就是他通过这种就是
[10:47.040 -> 10:48.800] 呃就是把这个以前的这种点阵传感就做的特别密集
[10:49.240 -> 10:50.480] 然后上面集成很多
[10:50.480 -> 10:52.640] 然后每个点阵的这个传感器具有
[10:52.640 -> 10:53.720] 一个表达的测力功能
[10:53.720 -> 10:55.200] 它集成起来以后它变成个分布力
[10:55.200 -> 10:55.520] 测量
[10:56.080 -> 10:57.800] 所以这种这种电子皮肤的研究非常
[10:57.800 -> 10:58.560] 非常之多
[10:58.800 -> 11:00.920] 呃尤其像在市场部的鲍德南
[11:01.080 -> 11:05.040] 这个院士他做的工作非常之系统
[11:05.040 -> 11:06.360] 然后这块电子皮肤
[11:06.360 -> 11:08.600] 其实这一块它的体积很紧凑
[11:08.600 -> 11:09.480] 可以做到非常薄
[11:09.480 -> 11:10.840] 然后测量速度也比较快
[11:11.360 -> 11:14.960] 但这个东西它确实也适合做这种大
[11:14.960 -> 11:16.480] 就是大面积的
[11:16.640 -> 11:18.800] 然后在人身体上所使用
[11:18.800 -> 11:20.120] 但这个用来指尖的时候
[11:20.120 -> 11:20.840] 它会有些问题
[11:20.840 -> 11:22.480] 然后它的吸引总体感知比较有限
[11:22.480 -> 11:23.880] 因为它取决于这样的一个
[11:24.120 -> 11:27.200] 单元所具备的性能然后吸息总被感知比较有限,因为它取决于这样的一个呃,单元所具备的性能,然后吸引感知密度
[11:27.480 -> 11:31.280] 比较低,但是现在他们已经已经好像能够突破这个
[11:31.280 -> 11:34.400] 支配工艺可以做的比较高,但都支配工艺确实对我们来说要求
[11:34.400 -> 11:37.920] 很高,这都是我们很难突破的信息采集的应用也比较复杂
[11:37.920 -> 11:41.040] 你看上面这个大家看不出来这个细线就是每个单元
[11:41.040 -> 11:43.960] 他他他也有线出来,所以后面当然可以可以集成一下
[11:43.960 -> 11:46.680] 芯片的样子,我觉得但这个事情需要一点时间
[11:47.440 -> 11:49.080] 然后另外还有一种就是大家可能比较少
[11:49.080 -> 11:50.600] 就基于模型重构的方式
[11:51.000 -> 11:53.600] 这种传感器就是它的传感器非常之简单
[11:53.840 -> 11:56.320] 就是它只它只拿一一种最原始的信息
[11:56.320 -> 12:00.360] 后面全部靠啊算法模型去处理这个事情
[12:00.840 -> 12:03.120] 呃就有点像相机吧
[12:03.680 -> 12:06.000] 相机它拿到的信息其实非常简单
[12:06.000 -> 12:09.000] 就是一个图像的信息
[12:09.000 -> 12:11.000] 明暗变化那样的东西
[12:11.000 -> 12:15.000] 但后续它可以有很多的算法的处理
[12:15.000 -> 12:17.000] 来做更多的事情
[12:17.000 -> 12:19.000] 所以我们也更倾向于用这样的东西
[12:19.000 -> 12:21.000] 因为我们在手指指尖这么小的空间范围内
[12:21.000 -> 12:24.000] 那么多的东西是不太现实的
[12:24.000 -> 12:26.840] 所以这也是基于模拟程序的方式
[12:26.840 -> 12:28.000] 也是我们更倾向的
[12:29.360 -> 12:31.720] 然后在这个模拟程序里面有一个传感器
[12:31.720 -> 12:34.120] 比较小的这种叫VM-based数学传感器
[12:34.120 -> 12:36.040] 它的传感器翻译出来就是视觉传感器
[12:36.040 -> 12:37.680] 就是用视觉来做触觉
[12:38.120 -> 12:40.480] 它的原理其实这张图就可以表现出来
[12:40.480 -> 12:42.560] 就是说它有一个像人手指这样的
[12:42.560 -> 12:44.600] 皮肤的一个软质固件弹性介质
[12:44.600 -> 12:46.660] 在发生作用的时候它会发生变形
[12:47.160 -> 12:49.980] 变形完以后用相机来拍这个上面的一些
[12:50.240 -> 12:52.020] 这个变形的表征的东西
[12:52.280 -> 12:54.580] 最后拿到一些这个
[12:54.840 -> 12:55.360] 就是
[12:55.620 -> 12:58.420] 这个图像信息然后对图像信息重复做这些所有东西
[12:58.940 -> 12:59.700] 所以它的硬件
[13:00.220 -> 13:03.040] 很简单就是我们刚才做的时候花了一周时间就把硬件复印出来
[13:04.060 -> 13:09.720] 但是这个事情想做好有点困难现在目前做最好的MIT后面会借到他们工作
[13:10.500 -> 13:13.560] 这个传感器当然我们可以分成四个部分,第一个就是信息表征
[13:13.820 -> 13:15.880] 就是说我们最开始的这个弹音介质的东西
[13:16.120 -> 13:19.200] 是最原始出现信息我们是无法处理的
[13:19.720 -> 13:21.000] 所以我们需要一些表征手段
[13:21.240 -> 13:22.280] 把它进行转换
[13:22.520 -> 13:25.320] 当然是转换这个图像能获取的一些关于信息
[13:26.120 -> 13:28.360] 然后紧接着就是这个采集信息采集
[13:28.360 -> 13:29.960] 如果我们需要通关于系统把这个表证
[13:29.960 -> 13:30.920] 后的信息给拿到
[13:31.600 -> 13:33.760] 拿到以后其实上面还有就开始进行处理
[13:33.960 -> 13:36.160] 这时候已经到了这样的一个模型阶段
[13:36.160 -> 13:38.600] 就是说从这图像信息中提出我们所需要的
[13:38.600 -> 13:39.320] 地层信息
[13:39.760 -> 13:42.000] 有这个地层信息以后我们可以再做
[13:42.000 -> 13:43.800] 进一步的这个处理性重构
[13:44.200 -> 13:47.200] 做这个综合信息去构建的这样一个事情
[13:47.200 -> 13:50.200] 所以这个传染器的原理基本上可以分成信息的表征
[13:50.200 -> 13:51.400] 采集提取和重构
[13:51.400 -> 13:54.680] 有了我们选定的传染器以后
[13:54.680 -> 13:57.200] 就是我们从2020年造疫情刚开始的时候
[13:57.200 -> 13:58.080] 我们开始做这个事情
[13:58.080 -> 13:59.800] 然后在做的时候
[13:59.800 -> 14:01.680] 我们第一个就是地震信息的获取
[14:01.680 -> 14:02.880] 就是说地震信息
[14:02.880 -> 14:05.480] 我们认为它是一个介质的一个形变
[14:06.000 -> 14:07.360] 当然这个是个人理解不一样
[14:07.360 -> 14:08.560] 像MIT认为是形贸
[14:08.560 -> 14:09.640] 但我们认为是形变
[14:10.200 -> 14:13.920] 我们对这个事情可以把它这样用一个
[14:14.440 -> 14:16.920] 这个比较清晰的图的形式
[14:16.920 -> 14:18.200] 就是说原始数学信息
[14:18.600 -> 14:20.080] 通过这个表征以后变成光学信息
[14:20.080 -> 14:21.800] 光学信息经过采集后变成像立图像
[14:21.800 -> 14:23.400] 图像以后再通过这个算法
[14:23.560 -> 14:25.800] 得到地层信息就是弹性接着行电
[14:25.800 -> 14:26.960] 所以我们要干的事情
[14:26.960 -> 14:28.920] 其实就是为了这三个环节信息表征
[14:28.920 -> 14:30.040] 采集和提取来干的
[14:30.920 -> 14:33.160] 这三个环节里面应该顺序介绍
[14:33.160 -> 14:35.480] 但是我们这边先介绍第二个环节
[14:35.480 -> 14:37.160] 因为信息表征和信息提取
[14:37.720 -> 14:39.400] 就是关键性非常之紧密
[14:39.400 -> 14:42.200] 因为不同的表征方式决定了提取的算法
[14:42.640 -> 14:44.560] 然后信息采集它更多的是一个独立的
[14:44.560 -> 14:46.160] 就是它把光线信息采集出来采的是一个独立的就是它把关于信息采集出来
[14:46.160 -> 14:47.560] 采集成一个图样信息
[14:47.560 -> 14:49.280] 所以从这边来看的话
[14:49.280 -> 14:51.600] 它的目的非常之明确
[14:51.600 -> 14:54.080] 就是说采集表征后的模式的关于信息
[14:54.080 -> 14:55.160] 并且转换了图样信息
[14:55.160 -> 14:59.040] 但这个转换它也不是说我拍张照片就行了
[14:59.040 -> 15:00.680] 它转换需要有要求
[15:00.680 -> 15:03.760] 就是说它第一个它得保证这个表征模式的
[15:03.760 -> 15:05.880] 一个特征与位置的完备性与准确性
[15:05.880 -> 15:10.120] 因为最终我需要拿到空间位置信息
[15:10.120 -> 15:13.560] 这个空间位置信息必须是完备的,不能说它缺少维度
[15:13.560 -> 15:17.800] 另外这个传感器要尽可能紧凑并且降低硬件的复杂性
[15:17.800 -> 15:23.000] 所以从这个角度来看,我们这个事情就分成了四个部分可以去做
[15:23.000 -> 15:26.000] 第一个是光学系统的形式,这个是个系统方案的选择
[15:26.000 -> 15:28.120] 后面就是如何进行设计
[15:28.120 -> 15:30.400] 优化最终标定就是把进度提升
[15:31.400 -> 15:33.200] 光学系统的形式
[15:33.200 -> 15:36.000] 形式如果不做分析的话很简单
[15:36.000 -> 15:37.160] 就是拿个相机来拍
[15:37.520 -> 15:38.720] 但拿相机来拍的话
[15:38.720 -> 15:39.760] 很明显一个问题
[15:39.760 -> 15:42.080] 就是相机是个正常的这种小相机
[15:42.080 -> 15:44.360] 它是个只能获得二维信息
[15:44.880 -> 15:46.560] 二维信息当然有各个学者只能获得二级信息二级信息当然也这个
[15:46.560 -> 15:48.000] 各学者有各个学者的方法
[15:48.000 -> 15:49.560] 他拿二维的信息以后
[15:49.560 -> 15:51.640] 他通过记忆学习和一些力学模型融合
[15:51.640 -> 15:53.240] 去做一些叫伪触觉
[15:53.240 -> 15:54.240] 伪3D触觉
[15:54.240 -> 15:55.640] 但这个事情再怎么做
[15:55.640 -> 15:57.200] 因为他缺少了一个深度信息
[15:57.200 -> 16:01.280] 他会在这个重复上网这边带来很大的问题
[16:01.280 -> 16:03.440] 第二个就是说2.5D
[16:03.440 -> 16:04.880] 2.5D为什么我们不叫3D
[16:04.880 -> 16:06.960] 因为它的深度发现的信息是通过
[16:06.960 -> 16:07.760] 另外的方式获取的
[16:07.760 -> 16:08.800] 比方说它一压完以后
[16:08.800 -> 16:11.320] 这个标志特征发生大小变化
[16:11.320 -> 16:12.480] 它通过这个大小的变化
[16:12.480 -> 16:13.600] 重复做深度信息
[16:13.960 -> 16:16.880] 但这个精度是不能跟平面
[16:16.880 -> 16:19.920] XYZ两个信息的精度对比的
[16:19.920 -> 16:20.920] 所以它精度有限
[16:21.280 -> 16:23.040] 然后最直接的就是我直接拿3D测量
[16:23.040 -> 16:23.760] 我们要双步相机
[16:23.760 -> 16:25.680] 甚至用正常的这种还有比如说双目相机甚至用这个
[16:26.560 -> 16:26.720] 正常的这种
[16:28.600 -> 16:28.960] 还有觉得用PYF相机来做
[16:30.480 -> 16:30.760] 但这个的问题就是
[16:31.800 -> 16:32.840] 它的尺度会比较大意念会比较复杂
[16:33.280 -> 16:34.760] 所以从这个新的角度来看的话
[16:34.760 -> 16:36.200] 这个光学系统的选择其实
[16:36.480 -> 16:37.680] 还是有一点讲究
[16:37.680 -> 16:39.080] 就是说我怎么能保证
[16:39.400 -> 16:40.680] 它的信息的维度
[16:40.800 -> 16:42.080] 是拿到3D的
[16:43.080 -> 16:44.280] 然后保证结构紧凑
[16:44.320 -> 16:46.000] 所以这边有一个比较巧妙的方式
[16:46.000 -> 16:48.000] 就是说我们引入了虚拟双目
[16:48.000 -> 16:50.000] 虚拟双目这边的这个就是说
[16:50.000 -> 16:51.000] 我用一个相机
[16:51.000 -> 16:53.000] 但是我这边加了一些反射镜
[16:53.000 -> 16:55.000] 所以让它感觉好像我从
[16:55.000 -> 16:57.000] 就是把这个
[16:57.000 -> 16:59.000] 接触的信息
[16:59.000 -> 17:01.000] 在镜面上进行反射以后
[17:01.000 -> 17:03.000] 让相机从不同的角度来看我
[17:03.000 -> 17:06.000] 所以就像在一个相机实际上里面会看到两张图
[17:06.000 -> 17:09.000] 这两张图正好是类似于用两个虚拟的双目相机
[17:09.000 -> 17:10.000] 在拍这个事
[17:10.000 -> 17:12.000] 所以这个位置性肯定是完备的
[17:12.000 -> 17:15.000] 结构形式因为这个架的镜面以后虽然结构变复杂
[17:15.000 -> 17:18.000] 但镜面通过优化设计以后会让结构变得更紧凑
[17:18.000 -> 17:19.000] 反而是变成一个好事
[17:19.000 -> 17:22.000] 然后当硬件系统因为它没有增加相机的数量
[17:22.000 -> 17:26.120] 它同样可以降低这个硬件的复杂性
[17:27.480 -> 17:27.840] 所以第二个就是万一系统设计
[17:30.040 -> 17:31.720] 就是说这个虚拟双目里面最大的困难就是说我的镜片怎么布置
[17:31.720 -> 17:32.800] 然后怎么优化呢
[17:33.360 -> 17:36.400] 如果自己完全是靠经验去设计的话
[17:36.400 -> 17:37.400] 这个还是有点困难的
[17:37.400 -> 17:39.080] 因为它的镜片当多了以后
[17:39.400 -> 17:40.480] 在什么地方放
[17:40.920 -> 17:42.160] 还是有点难度
[17:42.160 -> 17:43.720] 但这个领域其实非常小动
[17:43.720 -> 17:44.880] 就是做人也不多
[17:44.880 -> 17:49.720] 所以我们就自己来做了整个的一个光路突破结构的一个标准
[17:49.720 -> 17:53.240] 然后并且定义就是将光的模型分成成像锥
[17:53.240 -> 17:54.200] 反射域加紫光路
[17:54.200 -> 17:55.240] 这边我就不展开介绍了
[17:55.240 -> 17:57.520] 就是说它里面更重要是要保证两个事情
[17:57.520 -> 17:58.880] 第一个是光路一定要可成
[17:58.880 -> 18:01.720] 就是光路一定要满足这个镜面反射的这样的一个条件
[18:01.720 -> 18:02.960] 第二个非干涉
[18:02.960 -> 18:05.000] 就是说镜面之间不能相互割挡
[18:05.000 -> 18:07.920] 因为要不然就他的相机就拍不全了
[18:07.920 -> 18:10.040] 所以在这个基础上我们就进行了设计
[18:10.040 -> 18:12.280] 最典型的一个例子就是说在平面里面
[18:12.280 -> 18:14.480] 有两个镜子
[18:14.480 -> 18:17.680] 这两个镜子怎么放能够让它这个相机能拍完整
[18:17.680 -> 18:21.200] 这个软件部件
[18:21.200 -> 18:23.480] 当然这边设计的结果
[18:23.480 -> 18:24.640] 比方说这四种结果
[18:24.640 -> 18:26.760] 就这边每个点都是一个设计结果
[18:26.800 -> 18:28.240] 当然我们在设计结果
[18:28.280 -> 18:29.800] 就画出来就这样
[18:29.800 -> 18:32.120] 从这边可以看出有些形状
[18:32.760 -> 18:35.200] 就是还挺规整的
[18:35.200 -> 18:36.800] 像最下面两个
[18:36.800 -> 18:37.440] 尤其最右边的
[18:37.440 -> 18:39.480] 这个靠人设计是根本设计不出来的
[18:39.480 -> 18:41.080] 但是它的性能也会非常差
[18:41.080 -> 18:43.160] 就是说它只是满足了这个事情可以用
[18:43.160 -> 18:45.600] 但是它的角度太刁钻了
[18:45.600 -> 18:47.080] 就是说从相机布置来看
[18:47.080 -> 18:48.280] 我们不会这么选的
[18:48.800 -> 18:51.000] 所以关于系统设计是保证功能性
[18:51.240 -> 18:52.240] 然后后面我们就优化
[18:52.240 -> 18:53.200] 其实就保证性能
[18:53.640 -> 18:54.400] 所以在性能这块
[18:54.400 -> 18:55.480] 我们重点是关注两个
[18:55.480 -> 18:57.360] 一个是信息采集要精确
[18:57.360 -> 19:00.000] 第二个整个的传感器要尽可能紧凑
[19:00.520 -> 19:01.960] 所以我们提出三个指标
[19:01.960 -> 19:02.760] 第一个是双目角
[19:02.760 -> 19:05.280] 就是拍摄的双目角尽可能呈正焦
[19:05.280 -> 19:07.920] 这个是双目相机里面的一些知识
[19:07.920 -> 19:08.800] 就是说正焦的时候
[19:08.800 -> 19:10.360] 它在抗感染人水墙
[19:10.360 -> 19:12.520] 然后这个幅面率就是说我的相机
[19:12.520 -> 19:14.720] 这个画面尽可能占满整个画面
[19:14.720 -> 19:17.760] 这样尽可能就是保证利用它的一个
[19:17.760 -> 19:19.640] 相机的整个分辨率
[19:19.640 -> 19:20.920] 最后一个就是紧凑性
[19:20.920 -> 19:22.120] 就是我们提出一个比靠图概念
[19:22.120 -> 19:24.520] 让它尽可能的这样一个
[19:24.520 -> 19:26.160] 就是比较的紧凑
[19:26.160 -> 19:27.560] 所以这是一些设计案例
[19:27.560 -> 19:29.480] 比方说这边我是保证了双目角和比厚度
[19:29.480 -> 19:30.960] 可以发现它的高度确实挺窄
[19:30.960 -> 19:33.880] 然后整个的派对角也近90度
[19:33.880 -> 19:36.520] 但是显然它的这个幅面率非常差
[19:36.520 -> 19:40.160] 就通过整个这一块都是无效的视角
[19:40.160 -> 19:42.360] 如果我们以幅面率和比厚度来看
[19:42.360 -> 19:44.000] 它发现这个东西确实是
[19:44.000 -> 19:45.600] 就是它的整个的紧凑和幅面率和比较度来看他发现这个东西确实是就是他的整个的
[19:45.600 -> 19:48.320] 这个紧凑和幅面率
[19:48.320 -> 19:49.800] 中间只有非常小的一个
[19:49.800 -> 19:51.320] 这样的一个
[19:51.320 -> 19:52.800] 母校的画面
[19:52.800 -> 19:55.120] 但是他的这个双目角太小了
[19:55.120 -> 19:56.800] 所以我们有一些综合指标
[19:56.800 -> 19:58.000] 然后来同时优化
[19:58.000 -> 19:59.800] 最后我们就是基于这样的方案
[19:59.800 -> 20:00.800] 来做了这个相机
[20:00.800 -> 20:02.400] 做了这个传感器
[20:02.400 -> 20:04.600] 可以发现这是传感器最终的一个样子
[20:04.600 -> 20:06.640] 结构其实挺像手指的
[20:06.640 -> 20:09.360] 然后在它的拍摄范围内
[20:09.360 -> 20:10.800] 这是它的一个相机开放视野
[20:10.960 -> 20:14.080] 就左右两个其实是拍到同一个这个画面
[20:14.440 -> 20:17.600] 然后整个的一个幅面占比也特别的高
[20:17.920 -> 20:20.840] 所以这是一个光路优化设计
[20:20.840 -> 20:23.760] 最后就是标定 标定这边我就不展开介绍了
[20:23.760 -> 20:24.760] 就是说就相机标定
[20:24.760 -> 20:27.680] 怎么让它的整个的一个内参外参都做得更精准
[20:27.680 -> 20:32.280] 第二个环节就是重点介绍的心意表征和心意提取
[20:32.280 -> 20:34.600] 心意表征和心意提取是这里面非常关键的一部分
[20:34.600 -> 20:37.660] 也是被现在研究者所忽略的部分
[20:37.660 -> 20:41.500] 就是这个第一部分和这个同样处理的部分
[20:41.500 -> 20:44.060] 心意表征它的目的很明确
[20:44.060 -> 20:47.240] 就是说我要将弹音介质的表征为可补足的光源信号
[20:47.240 -> 20:48.560] 因为如果没法补足的话
[20:48.560 -> 20:52.640] 这个后面的图像再重构就很困难
[20:53.480 -> 20:55.200] 它的要求就是说
[20:55.720 -> 20:57.720] 里面有个很不同要求
[20:57.720 -> 20:58.560] 跟相机不太一样的
[20:58.560 -> 20:59.680] 比如说我拍一个场景
[20:59.680 -> 21:01.440] 它的要求是因为传感器
[21:01.440 -> 21:03.280] 所以传感器一定要接触的时候才发生
[21:03.280 -> 21:04.640] 才开始工作
[21:04.640 -> 21:06.840] 所以它的接触功能是不断发生变化的
[21:06.840 -> 21:08.840] 所以它的表征怎么具有适应性
[21:08.840 -> 21:10.520] 因为它表征需要完备
[21:10.520 -> 21:12.560] 就是我尽可能把所有的信息都给它
[21:12.560 -> 21:15.560] 就是我需要的形面信息给它
[21:15.560 -> 21:17.760] 就是完备的表征出来
[21:17.760 -> 21:19.640] 不发生信息的缺失
[21:19.640 -> 21:21.320] 最后要精确与精细
[21:21.320 -> 21:22.760] 然后吸引提取
[21:22.760 -> 21:25.240] 就是说我将相机里面的图像
[21:25.680 -> 21:27.240] 提出弹性截图的行列信息
[21:28.080 -> 21:30.480] 它的要求就是说要尽可能具有适用性
[21:30.480 -> 21:32.680] 然后提取算法要可靠精确实识
[21:33.280 -> 21:35.520] 所以这个要求当然可以分成四个部分去考虑
[21:35.520 -> 21:37.120] 第一个部分当然也是表征方式
[21:37.120 -> 21:38.120] 这是方案的选择
[21:38.640 -> 21:39.600] 后面就是分成两块
[21:39.600 -> 21:40.760] 一个是表征模式设计
[21:40.760 -> 21:41.960] 一个是新一提取专网设计
[21:42.200 -> 21:46.160] 最后就是把它支配出来进行新生测试然后第一部分我们看一下新一表征模式设计和信息设计专案设计最后就是把它制备出来进行新的测试
[21:47.680 -> 21:49.680] 然后第一个我们看一下信息表征方式信息表征方式是现在大家不太
[21:49.960 -> 21:51.160] 就是我们这些很奇怪
[21:51.160 -> 21:53.760] 就一直没有就是被大家关注
[21:54.200 -> 21:56.720] 我们最早开我们最开始时候跟MIT
[21:56.880 -> 21:59.640] 呃就是在CMU留学老师去沟通
[21:59.640 -> 22:01.440] 他们发现他们对这个事情根本不太理解
[22:01.440 -> 22:02.840] 为什么我们我们要干这个事情
[22:03.440 -> 22:05.120] 呃我先讲讲一下什么是信誉表征
[22:05.600 -> 22:09.160] 信誉表征就是说我要把它这个表面
[22:09.160 -> 22:10.640] 变成一个光学信息
[22:11.200 -> 22:12.920] MIT用的叫光录立体声法
[22:13.160 -> 22:15.640] 光录立体声就是我从不同的方向照录下
[22:15.640 -> 22:17.640] 拍出一个图像重新走表面的法向
[22:18.560 -> 22:20.160] 这是MIT一直在用的
[22:20.160 -> 22:21.480] 他们从09年开始做
[22:22.360 -> 22:24.920] 然后做了10年多
[22:24.920 -> 22:26.360] 然后他们的产业叫GeoSight
[22:26.360 -> 22:28.360] 这个还是个挺有名的一个产业
[22:29.040 -> 22:30.960] 这个光度立体法
[22:31.200 -> 22:33.080] 它是当然是计算机视频里面的一种
[22:33.080 -> 22:36.160] 就是做三维形状重构的一种方法
[22:36.840 -> 22:38.320] 它的方法就是它主要利用了
[22:38.320 -> 22:40.320] 这个像素的亮度进行细腻的表征
[22:40.600 -> 22:42.280] 并且它可以具备像素体的表面
[22:42.280 -> 22:43.320] 法像重现分辨率
[22:43.320 -> 22:45.760] 在MIT的一些文档里面的重建的效果
[22:45.760 -> 22:46.800] 我也发现非常惊喜
[22:46.800 -> 22:50.720] 甚至美元上面的20都可以清晰的
[22:50.720 -> 22:51.880] 重过了非常好
[22:51.880 -> 22:55.640] 所以他们这一块的应用主要是面向一些
[22:55.640 -> 22:56.320] 区间检测
[22:56.320 -> 22:57.880] 可以做微米级别的区间检测
[22:57.880 -> 22:59.800] 但这个事情它也带来一个问题
[22:59.800 -> 23:08.520] 就是说MIT它其实是重过了型貌型貌其实它是一个呃就是呃
[23:08.800 -> 23:10.120] 它并不需要过程
[23:10.480 -> 23:13.200] 但是行电量它毕竟它是要求
[23:13.200 -> 23:16.040] 追踪这个整个的一个呃就是
[23:16.360 -> 23:19.160] 嗯就是它的一些点的就特征点的
[23:19.160 -> 23:19.720] 一个运动
[23:19.960 -> 23:21.720] 但是在光追递力身上是显然是没有
[23:21.720 -> 23:22.240] 特征点的
[23:22.360 -> 23:25.400] 所以它是缺少了一些维度信息
[23:26.080 -> 23:28.280] 然后它的制备工艺和进度标准其实比较复杂
[23:28.280 -> 23:29.120] 我们尝试做过
[23:29.360 -> 23:30.680] 做不出来他们的效果
[23:30.680 -> 23:33.320] 就是他们这个效果我觉得还是有点动机在里面
[23:33.320 -> 23:34.600] 就是这个工艺在里面
[23:34.920 -> 23:35.680] 不太好弄
[23:36.000 -> 23:37.200] 所以除了MIT可以做
[23:37.200 -> 23:39.440] 好像其他没有任何的团队我们发现
[23:39.680 -> 23:44.120] 能做光轮力士身做得像MIT这么好
[23:45.240 -> 23:47.160] 所以光轮力士身它确实有自己的优点
[23:47.320 -> 23:51.640] 但是它用来做这种弹性截图形面的图构
[23:51.760 -> 23:55.360] 其实还是不能够完美表征的
[23:55.960 -> 23:57.480] 然后第二个方式是叫标准图案法
[23:57.960 -> 23:59.920] 标准图案法这个事情最开始的时候
[24:01.120 -> 24:03.400] 好像很多团队都在用这个事情
[24:03.760 -> 24:06.440] 但大家基本上也就用
[24:06.440 -> 24:08.440] 也没有说是深入的思考这个问题
[24:08.840 -> 24:10.840] 他的这种做法就是说
[24:11.040 -> 24:13.200] 我的弹性介质其实是没有任何
[24:13.800 -> 24:15.720] 没有任何特征的一个像硅胶
[24:16.160 -> 24:19.280] 所以他通过锚定在弹性介质上的一些图案特征
[24:19.480 -> 24:20.520] 进行识别和追踪
[24:20.560 -> 24:21.840] 然后重建它的运动位移
[24:23.240 -> 24:24.920] 比方典型的这个标注图案
[24:25.280 -> 24:27.920] 实际上第一个其实是MIT当时在做洗面的时候
[24:27.920 -> 24:29.400] 他们发现洗面也很重要
[24:29.760 -> 24:32.040] 所以他们在光轮地心的基础上又加了这些点
[24:32.640 -> 24:34.960] 就是然后包括这种
[24:35.280 -> 24:38.240] 矩形的圆形的或者散漫的其实都可以
[24:38.240 -> 24:40.040] 它都属于典型的这样一个标准图案
[24:41.920 -> 24:42.680] 然后这种标准图案法
[24:42.680 -> 24:45.240] 它就是全部利用了图案的特征信息进行标准它可以完备的标准出弹性介贺迫利用图案的特等信息进行标准
[24:45.240 -> 24:47.640] 它可以完备的标准出太阴界质的新面
[24:47.640 -> 24:49.000] 因为这个点是跟它锚接的
[24:49.000 -> 24:52.920] 所以这个点运动就可以反映出太阴界质的运动
[24:52.920 -> 24:54.320] 然后标准形式也多样化
[24:54.320 -> 24:55.160] 智慧工艺很简单
[24:55.160 -> 24:56.960] 打一个点就可以了
[24:56.960 -> 24:59.240] 但是它的一个问题就是图案的特等尺度
[24:59.240 -> 25:00.600] 其实限制了标准信息的密度
[25:00.600 -> 25:04.400] 它的信息的密度根本是没法跟着光临地神致力的
[25:04.400 -> 25:06.120] 但是其实另外一层层面我们好像也不要光临地神那么高的密度,它的信息的密度是根本是没法跟着光临底深的致力的
[25:09.160 -> 25:11.880] 但是其另外一层层面我们好像也不要光临底深那么高的进度因为它的分辨率非常之高
[25:11.880 -> 25:15.160] 如果用它的这样的一个信息去做后处理的话
[25:15.160 -> 25:16.240] 其实算法是很慢的
[25:17.480 -> 25:18.720] 所以从我们的应用角度来看
[25:18.720 -> 25:21.120] 这个标准图案法其实是能满足我们的应用需求
[25:21.360 -> 25:25.520] 所以我们就开始进行标准图案的信息的
[25:25.520 -> 25:26.640] 表征和信息的提取
[25:26.640 -> 25:28.840] 它的挑战性就在前面说过
[25:28.840 -> 25:31.160] 我们会发现这个接触的时候
[25:31.160 -> 25:35.360] 它的接触功能是不断发生变化的
[25:35.360 -> 25:37.200] 所以这个点的形状你会发现
[25:37.200 -> 25:37.880] 如果定了一个点
[25:37.880 -> 25:40.200] 看这个点是不断的发生了形状的变化
[25:40.200 -> 25:44.160] 并且整个的就一直在动的环境中
[25:44.160 -> 25:48.960] 所以在信息表征这这块它需要两个东西
[25:48.960 -> 25:50.400] 一个是要标准特征层
[25:50.400 -> 25:52.480] 就特征标准层这个事情很明确
[25:52.480 -> 25:54.200] 就是说这个点就是这种特征
[25:54.200 -> 25:56.560] 第二个是引源量里面的一个突破层
[25:56.560 -> 25:58.160] 就是如果没有突破的话
[25:58.160 -> 25:59.840] 它后面想做追踪这个事情就变了
[26:00.520 -> 26:01.720] 就是基本上就没法干了
[26:01.720 -> 26:03.640] 就是说我就是说追踪它
[26:03.640 -> 26:07.440] 所以这个光轮率层它里面就缺少了拓补连接层
[26:07.440 -> 26:10.560] 所以导致它没法做这个形验性的提取
[26:10.560 -> 26:13.480] 然后这个性仪提取就对应了要识别与定位
[26:13.480 -> 26:16.640] 对特征识别定位,然后拓补连接层进行匹配与追踪
[26:16.640 -> 26:20.840] 然后他们的共同要求就是要精确、精细、可靠
[26:20.840 -> 26:22.840] 来干这个事情
[26:22.840 -> 26:26.400] 那我们来看一下现在的比较
[26:26.800 -> 26:28.480] 就是整个学术界在用的一个东西
[26:28.480 -> 26:29.440] 叫离散标凿图案
[26:30.200 -> 26:32.360] 离散标凿图案就是它的特征
[26:32.360 -> 26:34.200] 就是通过圆点啊小球散弹
[26:34.360 -> 26:40.760] 来表征弹性介质的一个运动状态
[26:41.120 -> 26:44.440] 它的拓扑其实是一个非刚性的拓扑
[26:44.440 -> 26:47.160] 它是用相应点的非刚性匹配 待会会讲到
[26:47.680 -> 26:48.200] 就是
[26:48.440 -> 26:50.760] 它的这个心意提取非常简单
[26:51.000 -> 26:56.640] 它就是一个图案以后 然后说用它的形状轮廓来做这个离散 就是把这个心理点提示出来
[26:56.900 -> 26:58.180] 然后得到一些离散点的特征点
[26:58.440 -> 26:59.960] 然后匹配追踪就是说
[27:00.220 -> 27:01.000] 它相当于是
[27:01.240 -> 27:01.760] 我利用
[27:02.020 -> 27:02.780] 两帧之间
[27:03.040 -> 27:04.060] 找最相宁的点
[27:04.320 -> 27:05.360] 就是它认为这个因为我采集的速率相当于30帧这个点�之间找最相邻的点就是它认为这个
[27:05.360 -> 27:07.720] 因为我采集的速率相当于30帧
[27:07.720 -> 27:10.440] 这个点运动的距离肯定不会太快
[27:10.440 -> 27:11.840] 所以我就找最靠近的点
[27:11.840 -> 27:14.040] 认为就是跟上一帧基因匹配就可以
[27:14.040 -> 27:15.960] 然后可以得到戒指的形变性性
[27:16.440 -> 27:17.440] 但这个问题
[27:17.960 -> 27:19.680] 第一个就是它的特征性的点
[27:19.800 -> 27:21.680] 其实它的锚定并不唯一
[27:21.680 -> 27:23.520] 就是它在发生形变以后
[27:23.520 -> 27:25.320] 它开始是个圆
[27:25.320 -> 27:27.600] 它最后可能会变成一个非常畸变的形状
[27:28.080 -> 27:30.480] 所以这时候我们怎么通过这个点
[27:30.880 -> 27:32.360] 里面找到这样一个锚定的点
[27:32.680 -> 27:34.680] 其实是会发生一些偏差的
[27:35.240 -> 27:36.600] 如果我有重心的话
[27:36.960 -> 27:37.960] 所以这个点的话
[27:38.440 -> 27:39.600] 当发生形变以后
[27:39.600 -> 27:42.400] 它这个点可能跟最开始锚定的点
[27:42.400 -> 27:43.560] 可能是已经发生了变化
[27:44.440 -> 27:47.200] 第二个最严重的是这个特征匹配这种可发性比较低
[27:47.880 -> 27:50.800] 就是说当发生比较距离的变现的时候
[27:51.360 -> 27:55.520] 这时候这个点会发现周边如果有好几个点跟自己都一样远
[27:55.800 -> 27:59.200] 这时候我怎么知道我上一阵是在哪边
[27:59.480 -> 28:02.720] 甚至有时候变形过大了的时候
[28:03.000 -> 28:04.400] 它已经匹配错了点
[28:04.960 -> 28:08.640] 所以这是离散标准图案里面的一个问题所在
[28:08.640 -> 28:13.280] 所以我们在这个基础上提出了
[28:13.280 -> 28:15.320] 连续的标准图案
[28:15.320 -> 28:17.280] 当时我们开始在投文当中的时候
[28:17.280 -> 28:19.400] 就是我们当时也比较忐忑
[28:19.400 -> 28:22.160] 就是因为觉得这个东西就是改变一个图案的形式
[28:22.160 -> 28:23.400] 这个东西能不能值得认可
[28:23.400 -> 28:24.720] 但是最后审考结果非常好
[28:24.720 -> 28:29.200] 就是就我这边讲一下就是连绁标准图案的一个概念
[28:29.200 -> 28:32.320] 首先我们回顾一下这个离散标准点
[28:32.320 -> 28:33.960] 我这边举一个比较形象的例子
[28:33.960 -> 28:36.600] 就是说我们把这点就可能看成一些
[28:36.600 -> 28:39.080] 这种在路上的这种车
[28:39.080 -> 28:42.120] 这个车开始排布的特别的规整
[28:42.120 -> 28:45.880] 行列的这个就是都特别整齐
[28:45.880 -> 28:48.240] 但是如果当时发生街头行骗以后
[28:48.240 -> 28:50.200] 会发现首先这个点变乱
[28:50.200 -> 28:53.240] 甚至有点被挤的都已经不再是汽车了
[28:53.240 -> 28:54.520] 甚至变得像那些摩托车
[28:54.520 -> 28:57.600] 就是这时候大家再去看哪个点和哪个点对应
[28:57.600 -> 28:59.640] 这个事情就很复杂
[28:59.640 -> 29:01.800] 但如果我们有另外一个思路
[29:01.800 -> 29:03.840] 就是说我可不可以增加一些实际道路
[29:03.840 -> 29:08.280] 这个概念就是说我的车如果放在道路上
[29:08.280 -> 29:11.200] 它的物理元素保证它是不会离开这个道路的
[29:11.200 -> 29:12.880] 所以它始终在这个道路上开
[29:12.880 -> 29:16.200] 这样的好处就是说我可以通过这些东西把这个点都看过来了
[29:16.200 -> 29:18.200] 然后当地质上发生形变
[29:18.200 -> 29:20.600] 但是就我这个道路是在的
[29:20.600 -> 29:23.000] 我会顺着这个路会找到所有的点
[29:23.000 -> 29:25.120] 所以它是增加一种刚性的拓扑
[29:25.440 -> 29:27.600] 甚至我又发现最后一个的时候好像少了一个点
[29:27.600 -> 29:30.000] 就是这个点上甚至被挤掉了 就是被遮挡了
[29:31.040 -> 29:33.360] 如果我们对这个概念机抽象
[29:33.760 -> 29:35.360] 会把它抽象成这几个概念
[29:35.360 -> 29:36.480] 第一个是离下网格线
[29:36.560 -> 29:38.520] 这个网格线其实就是类似的道路
[29:38.520 -> 29:39.760] 就是提供一个拓扑关系
[29:40.280 -> 29:42.280] 然后证实的标志点我们并没有用这种
[29:42.600 -> 29:45.320] 就是直接的一个图像的轮廓的定义
[29:45.320 -> 29:47.040] 我们是用这个路和路之间的交点
[29:47.400 -> 29:49.920] 所以它的交点这块它是没有尺寸大小的
[29:49.920 -> 29:52.040] 所以它的位置是唯一保定的
[29:52.480 -> 29:54.200] 然后另外我们的路上的路征
[29:54.200 -> 29:56.280] 我们可以增加一些叫虚拟标志点
[29:56.280 -> 29:57.640] 可以增加它的信息特征密度
[29:59.080 -> 30:01.160] 然后这种设计其实最简单就是奇葩格
[30:01.480 -> 30:03.160] 我们开始想的奇葩格其实它
[30:04.200 -> 30:06.080] 它是里面是有一些这样的一个
[30:06.560 -> 30:09.520] 呃这个轮廓提取出来以后它会有一些这个虚拟变现可以
[30:09.560 -> 30:10.080] 会发现
[30:10.360 -> 30:12.840] 然后这些红的点就是它的一个焦点叫正式标志点
[30:13.760 -> 30:15.200] 当然我们还可以提供一些彩色的
[30:15.760 -> 30:19.080] 然后它的这个呃轻易提取跟刚才的就不太一样了
[30:19.080 -> 30:21.360] 就是它里面就就有了这个特征能够拓补层
[30:21.400 -> 30:22.960] 就是实体的特征能够拓补层
[30:23.320 -> 30:27.360] 会在这里面可以相互的这个�充,可以保证能够
[30:27.360 -> 30:30.880] 特征的提取精确和突破追踪的一个可靠
[30:30.880 -> 30:34.560] 当然我们也做了一些实验,比如说我们制作的DMP
[30:34.560 -> 30:36.960] 就是离散表的模式和连续表的模式
[30:36.960 -> 30:39.640] 发现在测量的进行性评估这一块
[30:39.640 -> 30:41.920] 是显著优于离散表的图案的
[30:41.920 -> 30:45.600] 然后在测量进行性这一块,就虚拟标准变多了以后
[30:45.880 -> 30:47.360] 它的对于这个
[30:48.120 -> 30:50.280] 信息的敏感度和它的一个整体的
[30:50.280 -> 30:51.760] 响应能力就会提升
[30:52.600 -> 30:54.320] 然后测量可玩性这边很明显的
[30:54.320 -> 30:56.920] 就是说当我们是抓了一个扳手
[30:56.920 -> 30:59.160] 然后扳手用很大的力去让它晃动
[30:59.400 -> 31:01.120] 这时候尼泰尔表录人这边显然出现了
[31:01.120 -> 31:01.960] 这个匹配错误
[31:03.480 -> 31:04.680] 然后在重归分母例的时候
[31:04.680 -> 31:06.000] 发现这边分母例重�了匹配错误然后在重过分布力的时候这边分布力重过了一趟
[31:06.000 -> 31:08.000] 就是它已经
[31:08.000 -> 31:10.000] 因为它的整个的最终发生处
[31:10.000 -> 31:12.000] 但是这个离散标准模式同样的
[31:12.000 -> 31:14.000] 就是它这边非常的可靠
[31:14.000 -> 31:16.000] 在重过力这一块可以使用
[31:16.000 -> 31:18.000] 保证很高的可靠性
[31:18.000 -> 31:20.000] 在连续标准模式
[31:20.000 -> 31:22.000] 最近我们又发现连续标准模式
[31:22.000 -> 31:24.000] 这块又有些更深层次的问题
[31:24.000 -> 31:25.320] 这个深层次的问题是这样的其实这个离散标准模式我最近我们又发现连续标准模式对于一些更深层次的问题这个深层次的问题是这样的
[31:25.320 -> 31:27.640] 就是说其实这个理性标准模式
[31:27.640 -> 31:29.000] 它是利用一种过程拓扑
[31:29.000 -> 31:30.080] 就是说我在运动的时候
[31:30.080 -> 31:33.360] 它可以通过对它的过程找最灵敏的点
[31:33.360 -> 31:34.360] 它是个过程拓扑
[31:34.360 -> 31:36.640] 它的好处就是说它并没有影响到
[31:36.640 -> 31:38.880] 这个原先的特征结构
[31:39.080 -> 31:39.720] 它的结果的
[31:39.720 -> 31:42.120] 所以它识别就比较容易
[31:42.560 -> 31:43.920] 但是它的
[31:43.920 -> 31:45.400] 这边写错了就是它的最终就会比较低但是它的那个这边写错了
[31:45.400 -> 31:46.640] 就是它的最终就会比较低
[31:46.640 -> 31:49.120] 因为它没有这个刚性的匹配
[31:49.120 -> 31:50.800] 然后装软拓扑正好相反
[31:50.800 -> 31:53.400] 就是它是把特征和拓扑合了
[31:53.400 -> 31:55.640] 它带来的问题就是说
[31:55.640 -> 31:59.880] 它的这个导致这个特征的识别变得更复杂
[31:59.880 -> 32:00.840] 它的可玩性降低
[32:00.840 -> 32:01.600] 然后这个
[32:01.600 -> 32:04.040] 但是它的这个最终可玩性会提升
[32:04.040 -> 32:06.480] 所以在这种情况下就是说各有各的优点
[32:07.120 -> 32:08.880] 所以我们提出一个融合式的方式
[32:08.880 -> 32:12.600] 融合式方式就是说我这边通过利用泥沙样式
[32:12.600 -> 32:15.800] 特征的一个方便识别
[32:16.000 -> 32:18.240] 然后用连续拓补的这样一个拓补结构
[32:19.080 -> 32:21.560] 所以另外就是要它的工艺
[32:21.560 -> 32:22.600] 当然工艺制度比较复杂
[32:22.600 -> 32:25.600] 我们这边就是首先提出这个雷调工艺
[32:25.600 -> 32:29.000] 它发现它制备的精度和这个效果非常好
[32:29.000 -> 32:33.200] 像这个融合式像我们制备的这样一个离散的连续的融合式的
[32:33.200 -> 32:34.600] 它的分辨率是一样高的
[32:34.600 -> 32:37.400] 但是你也可以发现这个离散点很密
[32:37.400 -> 32:39.600] 在追踪的时候肯定容易发生这个失焦
[32:39.600 -> 32:42.800] 连续式的它的一个棋盘格已经很小了
[32:42.800 -> 32:44.000] 当它发生变形的时候
[32:44.000 -> 32:46.160] 这个棋盘格想识别出来就非常非常困难但融合式它好处就是说它棋盘格已经很小了当它发生变形的时候这个棋盘格想识别实在就非常非常困难
[32:46.560 -> 32:49.200] 但融合式它好处就是说它棋盘格比较大
[32:49.640 -> 32:51.360] 然后就即使发生很大的变形
[32:51.360 -> 32:53.440] 它的特征同样比较容易提取
[32:53.520 -> 32:57.160] 然后中间点它就可以用来做相同类似性仪补分
[32:57.520 -> 33:00.480] 然后这个变形线就可以用来对这点
[33:00.760 -> 33:02.640] 它提供一些高清的拓扶
[33:02.960 -> 33:04.840] 所以它是一种特别好的一种补分方式
[33:04.920 -> 33:06.600] 这个工作我们最近正在做
[33:06.600 -> 33:09.200] 然后它更有希望是实现这种最右边的
[33:09.200 -> 33:10.400] 就是我们也制作出来了
[33:10.400 -> 33:12.200] 就它通过散班
[33:12.200 -> 33:14.000] 因为散班之前的问题就是散班
[33:14.000 -> 33:15.800] 它是无法实现这个
[33:16.400 -> 33:17.200] 刚性匹配的
[33:17.200 -> 33:18.400] 然后算法也比较复杂
[33:18.400 -> 33:20.200] 如果我们通过这种方式可以
[33:20.200 -> 33:22.400] 就是实现非常高的分辨率和
[33:22.400 -> 33:24.200] 足够可以的可高性
[33:24.200 -> 33:28.240] 所以这是在反感器这个表征方面做的工作
[33:28.240 -> 33:31.520] 第二部分就是做众层触觉信息
[33:31.520 -> 33:37.600] 就是说我们刚才已经有了传感器的弹音介质的形面
[33:37.600 -> 33:41.000] 然后如果通过这个形面来获得我们所需要的
[33:41.000 -> 33:42.560] 就是真正的触觉信息
[33:42.560 -> 33:45.240] 因为形面这个事情其实还算不上触觉信息
[33:45.240 -> 33:47.840] 它只能是一个我们认为是来
[33:47.840 -> 33:49.160] 通过其他信息的一个基础
[33:50.400 -> 33:52.880] 我们看下这个就是动棉信息
[33:52.880 -> 33:55.280] 重构的这样的一个
[33:55.760 -> 33:57.200] 就是整个框图
[33:57.200 -> 33:58.840] 就是说当我们有外力作用的时候
[33:59.040 -> 34:00.360] 这个弹性介质会发生形变
[34:00.360 -> 34:02.960] 形变以后通过相机测得TG以后
[34:03.280 -> 34:07.000] 可以用来做外力的分布力估计
[34:07.000 -> 34:09.000] 最后用来可以拿到多膜带
[34:09.000 -> 34:12.000] 进一步的构建多膜带信息
[34:12.000 -> 34:14.000] 多膜带信息可以分成这几个部分
[34:14.000 -> 34:16.000] 就是接触信息 物体信息和环境信息
[34:16.000 -> 34:18.000] 接触信息分成分布力和环境信息
[34:18.000 -> 34:21.000] 物体信息当然就是物体的质量 摩擦 材质 软硬
[34:21.000 -> 34:23.000] 这些东西靠视觉是hold不住的
[34:23.000 -> 34:26.200] 还有些更高的就是环境信息跟环境交互的时候
[34:26.200 -> 34:29.200] 环境的一些主抗环境的对物体的一些特征特性
[34:30.400 -> 34:31.600] 首先我们看一下戒毒特性
[34:31.600 -> 34:33.600] 第一个是我们做的结构分布力重构
[34:33.600 -> 34:35.400] 结构分布力重构我们开始以为很简单
[34:35.400 -> 34:37.800] 就是求个刚度矩阵然后取力
[34:37.800 -> 34:39.800] 后来发现这个事情很冒作
[34:39.800 -> 34:43.000] 当我们表述这个是分布力
[34:43.000 -> 34:44.800] 当没有任何测量误差的时候
[34:45.400 -> 34:47.200] 我这边用一个C来压它的时候
[34:47.200 -> 34:48.440] 这是它的一个效果
[34:48.720 -> 34:50.280] 当有0.1%的误差的时候
[34:50.400 -> 34:51.600] 这个效果会变得很差
[34:51.800 -> 34:53.240] 当有0.2%的误差的时候
[34:53.240 -> 34:55.120] 这个已经几乎没法成功了
[34:55.280 -> 34:57.200] 但是0.2%的误差对相对来说
[34:57.200 -> 34:58.560] 这是非常小的一个误差
[34:59.160 -> 35:02.320] 所以我们做的一些算法的研究
[35:02.320 -> 35:03.160] 提升它的乳房性
[35:03.160 -> 35:08.740] 可以发现即使20%都不差的时候它同样可以看出这个
[35:08.740 -> 35:10.120] 逆的整个轮廓
[35:10.120 -> 35:12.040] 然后基于它我们也来做了一些
[35:12.040 -> 35:15.040] 这个传感器的实时的逆重构
[35:15.040 -> 35:16.720] 那些效果还是可以的
[35:16.720 -> 35:20.000] 有了分布力的这个算法
[35:20.000 -> 35:21.480] 另外一方面我们可以从本质上解决
[35:21.480 -> 35:23.760] 就是说这个传感器的结构这一块
[35:23.760 -> 35:26.280] 我们怎么能保证它对这个造成降低敏感性
[35:26.280 -> 35:27.040] 提升阻防性
[35:27.040 -> 35:28.440] 对传感器进行结构设计
[35:30.760 -> 35:31.680] 这边不展开介绍
[35:31.680 -> 35:32.280] 我们就是
[35:32.640 -> 35:33.320] 基于我们的方法
[35:33.320 -> 35:36.160] 我们后来对这个MIT-JSite的传感器进行优化
[35:36.400 -> 35:38.600] 发现它以前的设计方案的问题在于
[35:38.600 -> 35:39.680] 它的月收表面过高
[35:39.880 -> 35:41.720] 所以我们通过这个移动上网优化以后
[35:41.960 -> 35:43.680] 给它提了一些优化方案
[35:44.080 -> 35:47.720] 整个的对物态敏感度可以降低60%
[35:48.720 -> 35:50.400] 所以这是接触分布力程度方面的工作
[35:50.880 -> 35:52.160] 然后第二个是接触滑移
[35:52.440 -> 35:54.240] 接触滑移这个事情可能大家就是了解的
[35:54.240 -> 35:55.240] 相对来说比较少一点
[35:55.240 -> 35:56.560] 因为我们以前只要知道
[35:56.800 -> 35:58.360] 就是物体接触的时候滑和不滑
[35:58.920 -> 36:00.240] 但是在弹性体接触的时候
[36:00.520 -> 36:03.600] 它状态其实并不只是滑动或者不滑动
[36:03.880 -> 36:06.680] 它有一个中间状态叫微滑效应
[36:06.680 -> 36:08.000] 这个微滑效应就是说
[36:08.000 -> 36:10.120] 我开始有一部分已经开始滑
[36:10.120 -> 36:11.800] 但中间还有部分其实没有滑
[36:11.800 -> 36:13.920] 随着你气象力载合不断增加的时候
[36:13.920 -> 36:15.880] 这个连接区域越来越小
[36:15.880 -> 36:18.160] 最后逐渐接近洪荒洼溢
[36:18.160 -> 36:21.720] 这个接触微滑就是说气象力载合不足以导致洪荒洼溢
[36:21.720 -> 36:24.440] 导致连接区域和滑翼区域同时存在的状态
[36:24.440 -> 36:27.000] 这个在我们人手抓取里面也发现同样的现象
[36:27.000 -> 36:30.000] 并且这一块是人手抓取调控非常关键
[36:30.000 -> 36:32.000] 因为有文献也表明这个事情
[36:32.000 -> 36:33.000] 甚至是唯一的参照依据
[36:33.000 -> 36:36.000] 这个后面我们会拿出一些证据来表明这个事情
[36:37.000 -> 36:38.000] 但它的难点就是
[36:38.000 -> 36:40.000] 微滑的测量是非常困难的
[36:40.000 -> 36:42.000] 就是这个现在就是说
[36:42.000 -> 36:44.000] 你怎么能测出这个传感器中间
[36:44.000 -> 36:46.760] 跟物体接触的时候哪不这么滑了
[36:46.760 -> 36:47.320] 如果没滑
[36:47.880 -> 36:51.600] 就是这个在以前几乎没什么办法能够测量这个事情
[36:52.160 -> 36:57.720] 所以我们这边其实是基于这样的传感器提出了一个分布类比例上的连接区的理科模型
[36:58.920 -> 37:01.520] 然后这做了效果就是说当它发生这个
[37:01.800 -> 37:04.440] 我们可以发现当它逆慢慢变大的时候很明显的可以
[37:05.160 -> 37:06.560] 不仅可以得到华裔区域
[37:06.560 -> 37:07.880] 并且可以得到里面的华裔厂
[37:07.880 -> 37:09.560] 就是说这个相对华裔的多少
[37:10.240 -> 37:12.120] 针对于原度级 原万里
[37:12.120 -> 37:13.840] 其实都可以得到很好的一个效果
[37:14.400 -> 37:15.240] 然后我们也跟
[37:15.280 -> 37:16.400] 因为只有两家
[37:16.840 -> 37:18.080] 两个大学做了这个事情
[37:18.080 -> 37:19.360] 一个是日本的名武大学
[37:19.360 -> 37:20.480] 你们方法在进入这块
[37:21.000 -> 37:21.960] 有明显的优势
[37:22.080 -> 37:23.120] 像它这块形状这块
[37:23.120 -> 37:24.760] 就是其实出现了很大的一个偏差
[37:26.000 -> 37:28.400] 第二个MIT因为MIT一直做传感器
[37:28.400 -> 37:31.920] 但他们在重复这块其实没有做太深入的工作
[37:31.920 -> 37:33.360] 这一块他们也提这些方法
[37:33.360 -> 37:36.960] 这个方法在华仪比较小的时候
[37:36.960 -> 37:38.240] 其实它的进度跟我们相当
[37:38.240 -> 37:39.760] 但是当华仪变大的时候
[37:39.760 -> 37:40.960] 他们的方法几乎就失效了
[37:40.960 -> 37:42.200] 就是误差飙升
[37:44.200 -> 37:46.040] 然后另外我们还最近幹了一個事情
[37:46.040 -> 37:48.360] 就是說當這個傳感器摸特別軟的
[37:48.360 -> 37:49.640] 是軟的物體的時候
[37:49.920 -> 37:51.600] 這時候想測量滑翼更困難了
[37:51.600 -> 37:53.240] 就是因為人手都覺得比較困難
[37:53.240 -> 37:56.160] 因為軟的物體自己也會發生很大的變異
[37:56.160 -> 37:58.640] 所以這時候我們對這個中間的這個接觸的
[37:58.640 -> 38:00.440] 滑翼狀態的感知會變得很困難
[38:00.800 -> 38:02.520] 我們後來發現一個很有意思的現象
[38:02.520 -> 38:05.400] 就是說它雖然兩個的這個
[38:06.800 -> 38:07.960] 就中間都在滑
[38:07.960 -> 38:10.560] 但是它的連接區和滑移區的變量
[38:10.560 -> 38:11.720] T度是不太一樣的
[38:12.560 -> 38:15.040] 所以這個如果做一個形象解釋
[38:15.040 -> 38:17.120] 就是說人手在摸東西的時候
[38:17.120 -> 38:18.520] 它皮膚的牽扯能力
[38:18.960 -> 38:20.800] 在滑移區和連接區是不一樣的
[38:21.440 -> 38:22.240] 所以通過這個方法
[38:22.240 -> 38:23.520] 如果我們再做多點研究
[38:23.520 -> 38:24.760] 當那個物體比較硬的時候
[38:24.760 -> 38:25.280] 以前的方法
[38:25.280 -> 38:29.000] 其实都可以很正式的拿到这个滑移场
[38:29.160 -> 38:31.080] 但这个物体当慢慢变软的时候
[38:31.120 -> 38:32.160] 可以发现以前的方法
[38:32.160 -> 38:33.080] 已经开始
[38:33.200 -> 38:34.680] 好像已经出现一些误差了
[38:34.680 -> 38:36.400] 但变软就同样特别的
[38:36.400 -> 38:37.960] 如果这个物体很软
[38:37.960 -> 38:39.360] 跟传感机的硅胶一样的
[38:39.960 -> 38:40.960] 可以发现以前的方法
[38:40.960 -> 38:43.560] 几乎已经完全失效了
[38:43.760 -> 38:44.960] 就是说它的形状
[38:44.960 -> 38:45.000] 这个滑移场包括这个连接器估计已经是差了特别多可以发现这个引擎的号码几乎已经完全失效了就是说它的形状
[38:45.000 -> 38:49.680] 这个华语厂的包括这个年龄的估计已经是差的特别多
[38:50.680 -> 38:51.840] 所以这样的变档的气度
[38:51.840 -> 38:53.480] 就是我们后面再看一下
[38:53.480 -> 38:55.360] 就是如果这个微博效应它有什么用
[38:55.680 -> 38:56.960] 微博效应它有个很重要的作用
[38:56.960 -> 38:59.440] 就是说我们这边有一个例子
[38:59.440 -> 39:02.680] 就是说我们用机械手来用这个长远镜来抓一个黑板舱
[39:03.200 -> 39:05.680] 当我们开始的时候把黑板舱提起来
[39:05.680 -> 39:08.000] 然后提起来以后我们用手来压它
[39:08.000 -> 39:10.000] 就是往下压
[39:10.000 -> 39:13.760] 在这种状态下它的状态就是会
[39:13.760 -> 39:16.160] 就是掉落的风险会越来越大
[39:16.160 -> 39:19.760] 然后我们这边就给出它的一个连接率
[39:19.760 -> 39:21.280] 我们可以看一下这个趋势
[39:21.280 -> 39:23.440] 这个南克线就是它的连接率的变化
[39:23.440 -> 39:27.560] 可以发现它三个状态非常好的展现出来
[39:27.560 -> 39:28.720] 就是它这个值越低
[39:28.720 -> 39:29.400] 说明它越危险
[39:29.400 -> 39:30.920] 到0的时候就全部发生化学
[39:31.920 -> 39:33.280] 然后我们也做了其他东西发现
[39:33.600 -> 39:36.440] 这个现象就是跟物体的种类形状
[39:36.440 -> 39:37.720] 表明特性没有任何关系
[39:37.720 -> 39:40.000] 就是这个事情是跟物体的特性是没关的
[39:40.000 -> 39:41.560] 所以这是一个非常好的一个
[39:44.800 -> 39:45.600] 一个表征手段
[39:45.600 -> 39:47.200] 就是说我们在抓据的时候
[39:47.560 -> 39:49.080] 就是你的物体重力不一样
[39:49.080 -> 39:50.000] 表面材质不一样
[39:50.000 -> 39:50.640] 软硬不一样
[39:50.640 -> 39:55.280] 但是它的粘连率的变化跟它的危险状态是完全吻合的
[39:55.960 -> 40:00.280] 所以这是威华效应可以在后续做抓据的调控里面
[40:00.280 -> 40:01.080] 一个很重要的应用
[40:02.200 -> 40:05.240] 第二部分我们做了物体的信息的这样一个
[40:05.240 -> 40:05.760] 重构
[40:05.960 -> 40:07.480] 第一个我们做了物体摩大特性
[40:07.640 -> 40:09.400] 因为以前的时候我们集中力
[40:09.400 -> 40:12.080] 散热器只能拿到一个摩大系数
[40:12.080 -> 40:14.920] 比如0.5或者0.8
[40:15.120 -> 40:17.440] 代表我们看这个改锥
[40:17.600 -> 40:18.960] 它表面既有塑料
[40:18.960 -> 40:19.960] 它又有橡胶
[40:19.960 -> 40:21.120] 它的摩大系数是不一样的
[40:21.400 -> 40:22.960] 人手摸起来也会感觉不一样
[40:23.400 -> 40:24.960] 所以我们在摸的时候可以发现
[40:25.000 -> 40:25.280] 它整个就是我们可以把分布摩大特系统是不一样的就人手摸起来也会感觉不一样所以我们在摸的时候可以发现
[40:27.360 -> 40:27.760] 它整个就是我们可以把这个
[40:29.720 -> 40:30.360] 分布摩擦特性给摸出来
[40:32.320 -> 40:32.800] 然后这是一个水杯
[40:36.080 -> 40:40.960] 就是它的这个水杯的这边的一个橡胶凸点和它的一个这个叫防滑圈
[40:41.000 -> 40:43.040] 可以发现它这个重过的效果还是非常好
[40:43.360 -> 40:45.000] 就是我们这边也重固了很多东西
[40:45.000 -> 40:49.000] 所以这也是第一次实现了这个物体的分布摩擦特性的测量
[40:49.000 -> 40:52.000] 所以分布摩擦特性其实在人手做这个
[40:52.000 -> 40:54.000] 这个当然靠视觉是很难看出来的
[40:54.000 -> 40:59.000] 就是说它可以看出这个表面图案
[40:59.000 -> 41:01.000] 但是很难看出这个摩擦特性
[41:01.000 -> 41:04.000] 所以这也是这个特殊传感器的一个优势
[41:08.000 -> 41:14.000] 然后另外我们做的这个物体的质量可以探索某些特性,这也是特殊传感器的优势另外我们做的物体质量推理探索就是重心抓重心这个东西,我们在用视觉来看一个物体的时候
[41:14.000 -> 41:16.000] 其实我们并不知道它的重心在什么地方
[41:16.000 -> 41:22.000] 正常的做法就是通过形状重心来找重心
[41:22.000 -> 41:25.080] 但是在偏移物体,质量不均衡物体的时候
[41:25.080 -> 41:27.000] 其实次学这个方法基本上都失效了
[41:27.000 -> 41:28.840] 所以怎么做这个重心的探索
[41:28.840 -> 41:31.760] 这个事情我们开始以为很简单
[41:31.760 -> 41:35.080] 但后来发现这个还是挺困难的东西
[41:35.080 -> 41:38.120] 所以我们提出一个叫TouchLift的这个抓取基本模型
[41:38.120 -> 41:40.280] 这个基本模型我们在做这个时候
[41:40.280 -> 41:45.440] 这个事情对我们的这个认知还是有很大的一个
[41:46.040 -> 41:47.320] 就是
[41:50.120 -> 41:52.120] 就是一个就是这个怎么做心理探索我们这边直接把这个我们看出结果
[41:52.120 -> 41:52.720] 可以发现出来
[41:53.160 -> 41:54.760] 那这边我们再做一个偏心物体
[41:55.680 -> 41:56.640] 表形一个锤子
[41:57.040 -> 41:58.520] 在抓的时候它的物体很偏心
[41:58.920 -> 42:01.800] 然后包括抓一些就是一些费年月表面
[42:01.800 -> 42:02.400] 重因可变
[42:03.000 -> 42:04.920] 我们可以发现就是我们一开始其实是
[42:04.920 -> 42:06.120] 为了探索这个重心
[42:06.520 -> 42:07.800] 但其实在探索东西的时候
[42:07.800 -> 42:09.120] 这个物体它也被抓起来了
[42:09.720 -> 42:12.640] 所以在这里面有一个事情就是信息的探索
[42:12.640 -> 42:15.360] 和任务的规划并不是结果的
[42:15.360 -> 42:17.840] 就是说其实这个事情对我们的触动
[42:17.840 -> 42:20.920] 就是后面咱们做自主化操作这个事情的影响很大
[42:20.920 -> 42:22.560] 因为自主化操作是把这个事情做
[42:22.560 -> 42:24.320] 事先弄好了然后再开始操作
[42:24.760 -> 42:26.320] 它的信息的探索和任务
[42:26.320 -> 42:27.400] 规划是完全分开的
[42:28.000 -> 42:29.080] 但是人在抓东西的时候
[42:29.080 -> 42:33.480] 其实它是边通过触觉获取信息
[42:33.480 -> 42:35.200] 又通过这个触觉信息反过来
[42:35.200 -> 42:36.600] 更好地指导抓取
[42:36.600 -> 42:38.720] 抓取的同时又会帮助他获取信息
[42:38.720 -> 42:41.320] 所以这边在这个层面上
[42:41.640 -> 42:43.560] 就是说通过触觉来获取信息
[42:43.840 -> 42:47.560] 跟我们来完成任务中间已经几乎融合在一起了
[42:48.560 -> 42:53.840] 它已经不单单只是一个信息通过这样的事情了
[42:54.880 -> 42:58.600] 这个事情在后面一个任务里面也会再提到
[42:58.840 -> 43:01.240] 所以这时候我们在做物体信息探索
[43:01.240 -> 43:02.560] 当然我们也做过软硬
[43:02.560 -> 43:04.400] 但环境探索我们现在正在开始
[43:04.400 -> 43:06.280] 就是说怎么通过这个东西来感知
[43:06.560 -> 43:09.640] 视觉没法感知这些环境特性
[43:11.800 -> 43:13.280] 然后第三部分就是我们把这个传感器
[43:13.280 -> 43:15.840] 来做成就是更像样机的一个东西
[43:16.120 -> 43:17.440] 我们在开发这样的产品
[43:17.440 -> 43:21.800] 就是说能提供能更紧凑的分辨率更高的
[43:21.800 -> 43:24.400] 以及多种模式感知的这样一个传感器
[43:24.920 -> 43:26.400] 然后这个传感器当然我们以前
[43:26.400 -> 43:27.600] 也去调研过传感器
[43:27.600 -> 43:29.600] 包括这种点阵式的
[43:29.600 -> 43:31.800] 然后这边是BioTech开发的
[43:31.800 -> 43:32.600] 价格比较贵
[43:32.600 -> 43:34.200] 然后它也不太能满足我们的需求
[43:34.200 -> 43:37.000] 所以我们就来做这样的一个事情
[43:37.000 -> 43:38.200] 其实这个传感器
[43:38.200 -> 43:41.600] 它是一个非常上有的一个东西
[43:41.600 -> 43:42.600] 我感觉这个传感器做完以后
[43:42.600 -> 43:43.800] 它会有很多想象空间
[43:43.800 -> 43:46.120] 就是说它可以在很多的领域
[43:46.120 -> 43:47.520] 比如说医疗 智能甲
[43:47.520 -> 43:49.080] 就是发展机器人 增强现实
[43:49.080 -> 43:51.400] 护理这块去做很多的应用
[43:51.400 -> 43:53.160] 但这个我们也是正在开始来做
[43:53.160 -> 43:55.560] 就是我们要把我们这样的一个前面的
[43:55.560 -> 43:58.160] 在传感器的设备
[43:58.160 -> 44:00.960] 信息重构这块东西进行融合
[44:00.960 -> 44:03.080] 然后来开发这样的一个传感器
[44:03.080 -> 44:07.720] 希望能够为机器人提供像机一样的传感器
[44:09.880 -> 44:12.200] 总结一下我们针对指尖触觉
[44:12.600 -> 44:15.600] 然后采用这种VM-BASE数学传感器的形式
[44:15.920 -> 44:17.880] 重点是构建了他们一个
[44:18.240 -> 44:19.960] 心意重构的完美性的研究框架
[44:19.960 -> 44:22.360] 在心意表层这一块提出连续式的表层模式
[44:23.200 -> 44:25.080] 在心意体系这一块引入了虚拟双目
[44:25.800 -> 44:27.160] 然后心意重构就构建出
[44:27.680 -> 44:29.120] 形贸形变接触分布力
[44:29.120 -> 44:30.320] 环域场模式特性
[44:30.320 -> 44:31.200] 职场分布特性
[44:32.280 -> 44:33.280] 就是这一些
[44:34.560 -> 44:36.680] 应该是力学信息
[44:36.680 -> 44:38.600] 靠视觉所无法拿到的一些信息
[44:40.120 -> 44:42.600] 然后就我们在布局这块其实已经
[44:43.040 -> 44:43.600] 其实就是
[44:44.000 -> 44:45.200] 第一个是在最底层干低层处理信息和综合处理其实就是第一个是在最底层干
[44:45.200 -> 44:47.480] 低层处理信息和综合处理信息重构
[44:47.720 -> 44:49.560] 在地主层开发出这样的产品
[44:49.720 -> 44:54.560] 但这个产品它还并不是一个能够直接面向应用的东西
[44:54.800 -> 44:56.800] 它更多还需要再做很多的应用探索
[44:56.840 -> 44:59.080] 但这个应用探索就特别取决于不同的学科
[44:59.080 -> 45:00.560] 不同应用场景的一些想象力
[45:00.800 -> 45:02.320] 比如说我们后面想做的就是用这个
[45:02.320 -> 45:04.160] 传感器来做生物行为特性的探索
[45:04.800 -> 45:08.000] 然后刚刚在远程医疗这一块推荐信息的更正常的反馈
[45:08.000 -> 45:13.000] 然后在物体检测这一块来弥补视觉所无法能够获取的东西
[45:13.000 -> 45:18.000] 甚至现在正向现实这一块能够获取一些更为精细的教育的感知
[45:18.000 -> 45:30.000] 这部分就是比较详细的介绍了我们在指尖动脉带推的感知方面的工作后面我再快速的花点时间把后面两个工作进行的介绍
[45:30.000 -> 45:31.200] 第一个是抓取
[45:31.200 -> 45:32.800] 其实抓取是我们最开始做的
[45:32.800 -> 45:33.800] 其实除了权衡的介绍
[45:33.800 -> 45:35.800] 我们后续就是为了服务抓取
[45:35.800 -> 45:38.800] 然后从中间停下来做了一些期间的研究
[45:38.800 -> 45:42.000] 但抓取它开始我们觉得很简单
[45:42.000 -> 45:43.400] 但是抓取任务其实真的想做
[45:43.400 -> 45:47.680] 其实人类块涉及到视觉人手构造,触觉和大脑控制,
[45:47.680 -> 45:51.000] 它是现在一个非常就是并不简单的任务,
[45:51.000 -> 45:53.400] 虽然它是一个最基础的抓取任务,
[45:53.400 -> 45:58.240] 很多这个学者都在怎么能做到自信和自愿,抓起来就很简单,
[45:58.240 -> 46:00.440] 如何具备抓取技能这个事情很复杂,
[46:00.440 -> 46:03.960] 所以我们可以看一下抓取任务其实主要可以分成这两部分,
[46:03.960 -> 46:05.400] 第一个是抓取卫星的选取
[46:05.400 -> 46:07.080] 我怎么能够选到最好的卫星
[46:07.720 -> 46:09.560] 第二个是抓取过程的调控
[46:09.560 -> 46:13.320] 就是说我在抓捕中怎么把力调控的刚好合适
[46:14.880 -> 46:16.800] 然后我们先看这个抓捕的调控
[46:16.800 -> 46:17.840] 其实它相对来说简单一点
[46:18.640 -> 46:21.840] 但是它的抓捕力的大小其实取决于它的质量是多少
[46:21.840 -> 46:24.080] 摩扎系数以及这个物体被破坏的极限
[46:24.960 -> 46:25.000] 它们决定了抓�量是多少 摩大系数以及这个物体被破坏的极限
[46:28.000 -> 46:29.000] 他们决定了这个抓冰的上限和下限
[46:31.000 -> 46:35.000] 它的最大难点就是物体信息在抓之前是不知道的就是说我没法先辨识完这个重量 摩大系数然后再抓
[46:35.000 -> 46:37.000] 这样就有点恐怖导致就是我已经抓起来了
[46:37.000 -> 46:40.000] 我为什么还要再反复去辨识来重新抓
[46:41.000 -> 46:44.000] 所以这个问题就是基层单单是你的感觉
[46:44.000 -> 46:46.480] 在这个基础上我们主要做了三个事情
[46:46.480 -> 46:48.000] 一个是摩大推进的在线面试
[46:48.280 -> 46:51.560] 第二个是通过未完来判定这个发掘稳定性
[46:51.560 -> 46:53.600] 最后通过未完核定来做实时调控
[46:54.160 -> 46:56.600] 最后它的效果就是说我们可以利用这个
[46:56.600 -> 46:58.280] 以所需要的最小的力
[46:58.280 -> 47:01.400] 只要高那么一点的力就可以把这事情抓起来
[47:01.880 -> 47:05.200] 所以它就可以就是既能保证抓取的稳定性
[47:05.200 -> 47:09.240] 又能够并不浪费过多能力
[47:09.240 -> 47:11.680] 就造成你的浪费以及或者破坏物体
[47:11.680 -> 47:13.080] 当然抓各种各样的物体
[47:13.080 -> 47:14.600] 比如我们抓的很奇怪的东西
[47:14.600 -> 47:17.520] 这个都可以保证抓取的安全预值
[47:17.520 -> 47:18.880] 控制在3%以内
[47:18.880 -> 47:21.160] 人手基本上是在10%到40%
[47:21.160 -> 47:23.000] 比方这个物体需要10天就抓起来
[47:23.000 -> 47:24.880] 人可能11到14天就抓起来了
[47:24.880 -> 47:25.840] 当然我们只是在某个方面可以总是在10%到40%比方说这个物体需要10年过去抓起来人可能11到14年过去抓起来
[47:29.880 -> 47:31.040] 当然这个我们只是在某个方面可以匹配人手的抓捕能力其实人手它很强
[47:31.040 -> 47:34.960] 它即使在传染器的有噪声和很大的延时
[47:34.960 -> 47:35.760] 它可以抓得很好
[47:35.760 -> 47:39.080] 所以这也是我们目前正在研究的事情
[47:39.960 -> 47:41.400] 第二个就是做抓捕微型规划
[47:41.400 -> 47:42.720] 这个是我们现在正在做
[47:42.720 -> 47:44.760] 但是我们感觉很困难的事情
[47:45.160 -> 47:48.360] 因为抓捕微型规划它里面它的解非常之多
[47:48.360 -> 47:49.880] 就无穷多解
[47:50.360 -> 47:52.640] 然后这边都是一些可以选的抓取卫星
[47:52.640 -> 47:53.960] 我们通过一些评价
[47:54.320 -> 47:56.520] 比方说我需要用锤子来锤东西
[47:56.520 -> 47:58.000] 那最后我们就会选择这样的卫星
[47:58.320 -> 48:00.000] 但如果我只想抓的比较可靠
[48:00.000 -> 48:02.000] 我可能会抓的更偏重心一点
[48:02.000 -> 48:02.760] 这样更神秘
[48:03.800 -> 48:05.320] 但卫星评价会涉及到很多的方面
[48:06.960 -> 48:08.840] 第二个就是说这个物体的差别非常大
[48:09.760 -> 48:12.800] 以及这个一个物体的它的特性很复杂
[48:13.160 -> 48:18.360] 所以如果想通过积极视觉的方式去学习的话
[48:18.720 -> 48:20.760] 这边最大的问题就是它的学习的数据是
[48:21.040 -> 48:24.480] 产业围堵灾难很难去收敛
[48:24.480 -> 48:28.440] 以及学到非常鲁莽的可放化技能
[48:28.440 -> 48:33.840] 所以这个卫星评价直接影响抓取效果,不同的卫星评价会抓取的卫星是不一样的
[48:33.840 -> 48:36.640] 以及物体的底核与力学性也复杂多样
[48:36.640 -> 48:41.760] 所以在这个过程中,抓取的本质其实就是实现物体的某种的理想卫星约束
[48:41.760 -> 48:47.800] 所以它就构建了这样的惯例关系,大家也也也基本都在干这个事情,其中
[48:47.800 -> 48:50.600] 非常具有代表性的,比方说是这个博克力的一个DSNet,
[48:50.600 -> 48:54.000] 他们19年发明了Science Robotics,并且被评为这个
[48:54.000 -> 48:58.600] 科波音技术,它的这个整个框架就通过视觉
[48:58.600 -> 49:01.000] 去做的,只不过他们做了一个很重要的工作,就是他们
[49:01.000 -> 49:04.480] 在虚拟环境中,呃,是模拟抓取,跟Google提供了
[49:04.480 -> 49:05.280] 一千五百个虚拟机,就是中呃是模拟抓取跟google提供了1800个虚拟机
[49:05.280 -> 49:06.320] 就是各种训练
[49:06.800 -> 49:08.680] 然后在呃
[49:08.800 -> 49:10.600] 然后通过一些概率模型来给予这个
[49:10.600 -> 49:11.400] 抓取经验评价
[49:11.400 -> 49:13.880] 比方说0.8就是抓住十次有八次
[49:13.880 -> 49:14.560] 可能会成功
[49:14.840 -> 49:15.720] 然后反复的训练
[49:15.720 -> 49:17.440] 最后把这个宣扬的东西来应用
[49:17.440 -> 49:18.440] 实际里面去
[49:18.720 -> 49:20.360] 这是他们呃他们
[49:20.680 -> 49:21.680] 呃干了一个事情
[49:22.760 -> 49:25.200] 然后第二个就是这个比较有钱的Google
[49:27.040 -> 49:30.400] 大家知道Google的机器人工程他就是让机器人在现实生活中去抓
[49:30.400 -> 49:32.080] 然后直接获取大量的数据
[49:32.080 -> 49:34.080] 用量流渗透相关的技能
[49:34.960 -> 49:39.960] 但这块一般的我觉得团队很难
[49:39.960 -> 49:42.800] 你像这个酷网意外机器人一台卖五六十万
[49:42.800 -> 49:44.320] 他们整个一大排
[49:44.640 -> 49:46.360] 就是说所以
[49:47.800 -> 49:49.720] 所以这个就是这个SIM2real
[49:49.720 -> 49:52.000] 以及或者是在线上直接学习
[49:52.000 -> 49:54.400] 这个当然都是大家获取数据的方式
[49:54.880 -> 49:57.680] 其实在做的过程中就是因为
[49:58.240 -> 49:59.800] 我们的专业不是做学习
[49:59.800 -> 50:00.920] 我们对学习了解很少
[50:01.240 -> 50:03.600] 但这个过程中我们更关注的是数据的本身
[50:04.160 -> 50:06.200] 我们会发现这边会有两个问题
[50:06.200 -> 50:10.000] 第一个是觉得这边大家都是用视觉来获取信息的
[50:10.000 -> 50:12.680] 但视觉获取的信息其实我们在前面谈了
[50:12.680 -> 50:15.120] 就是说它不能够取来处决
[50:15.120 -> 50:17.200] 它获取的信息在操作里面其实是有限的
[50:17.200 -> 50:20.120] 它缺乏有效的融合视觉感
[50:20.120 -> 50:21.240] 在抓取位移的评价
[50:21.240 -> 50:23.000] 第二个就是刚才谈到了
[50:23.000 -> 50:24.960] 就是说探索与规划融合是不够的
[50:24.960 -> 50:25.360] 一般就是这个就是博克力的工作感受在抓取微信的评价第二个就是刚才谈到了就是说探索与规划融合是不够的
[50:28.080 -> 50:28.600] 一般就是这个就是博客这个工作
[50:30.080 -> 50:31.680] 会发现他在真正执行的时候就是这个执行的阶段
[50:32.000 -> 50:33.760] 他的他并不是一个loop
[50:33.760 -> 50:35.480] 他还是这个学完以后直接用
[50:35.480 -> 50:37.320] 他并没有把这个实际中的
[50:37.640 -> 50:39.720] 这个抓取结果进行充分的发掘利用
[50:40.280 -> 50:42.560] 所以他这边还是认为就是我先规划完
[50:42.840 -> 50:43.720] 然后再去执行
[50:44.040 -> 50:48.200] 他并没有在实现过程中不断的进行探索和融合
[50:48.200 -> 50:50.600] 所以这边的问题其实也在于他无法对这个
[50:50.600 -> 50:51.800] 抓取的结果进行评判
[50:51.800 -> 50:53.400] 然后把里面有用的信息给提出来
[50:53.400 -> 50:58.600] 所以这边就是我们觉得必须要构建出这个触觉的内部空间
[50:58.600 -> 51:00.000] 跟外部空间之间的关联关系
[51:00.000 -> 51:03.400] 这是我们认为它是一个非常底层的一个东西
[51:03.400 -> 51:08.280] 那这边举个例子比方说就是说有些人觉得我通过学习可以干所有事情
[51:08.280 -> 51:12.080] 但是这个在世界上有七八十亿的这个人
[51:12.080 -> 51:13.400] 他在抓取的时候
[51:13.400 -> 51:15.240] 其实每个人的这个成长经历不一样
[51:15.240 -> 51:16.400] 文化的背景也不一样
[51:16.400 -> 51:18.480] 大家会发现每个人抓取的行为是一样的
[51:18.480 -> 51:20.840] 所以这边有个东西是这个DNA里面的东西
[51:20.840 -> 51:25.040] 就是说它是一些非常我们认为是本能的东西
[51:25.040 -> 51:27.040] 而在本能的东西它会作为一个基础
[51:27.040 -> 51:29.400] 当然最终也能用巧架
[51:29.400 -> 51:30.240] 它需要通过学习
[51:30.240 -> 51:33.480] 比方说你在打磨刨光
[51:33.480 -> 51:34.600] 有些人可以打磨得更好
[51:34.600 -> 51:36.200] 这个可以通过不断的学习
[51:36.200 -> 51:37.760] 但有些特别基础的东西
[51:37.760 -> 51:38.680] 如果没有的话
[51:38.680 -> 51:42.200] 想纯靠数据构建出这样的一个技能
[51:42.200 -> 51:43.440] 我觉得就很困难
[51:43.440 -> 51:46.360] 所以我们一直想在构建这个内在的联系关系
[51:47.240 -> 51:49.360] 然后把这个手内信息感知机
[51:49.360 -> 51:51.040] 如果我们这个学的东西一定是一个
[51:51.320 -> 51:52.640] 不是最原始的那个
[51:52.640 -> 51:54.760] 比方说拿到了这个相机的图像信息
[51:54.760 -> 51:57.240] 那么它的有用的信息就太少了
[51:57.600 -> 51:59.960] 就是不足以能帮助我们构建出这个
[52:00.160 -> 52:02.840] 传感器跟外部的这个行为
[52:02.840 -> 52:03.840] 它这边的关系
[52:04.320 -> 52:12.120] 所以这边再回到我们之前抓物体的旋转中心
[52:12.120 -> 52:15.040] 其实旋转中心就是找出粮食里面的最后抓住卫星
[52:15.040 -> 52:16.360] 就是说挪得最省力
[52:16.880 -> 52:19.200] 所以这个过程中我们发现我们并没有通过学习
[52:19.600 -> 52:21.760] 我们通过一些最本能的东西
[52:22.160 -> 52:24.640] 就是以前绿茜的特性可以把它构建出来
[52:24.640 -> 52:25.680] 也就是说人其实也可以这样
[52:25.680 -> 52:26.480] 他可以一点点抓东西
[52:26.480 -> 52:27.440] 然后一点点去挪
[52:27.680 -> 52:29.800] 然后一直挪到最合适的部分
[52:29.800 -> 52:35.480] 这部分它并不需要通过数据去做的
[52:35.480 -> 52:36.600] 它可以通过一些
[52:36.840 -> 52:40.560] 我觉得一些比较内部的技能的东西
[52:40.560 -> 52:41.320] 或者本能的东西
[52:41.320 -> 52:42.920] 我们现在也说不清楚
[52:43.160 -> 52:44.480] 但我们最后的希望就是说
[52:44.760 -> 52:49.200] 能够把现实的数据非常高效的,非常高质量的学会了。
[52:49.600 -> 53:02.720] 这样我们希望就是我们把技巧扔在一个他完全没有见过的东西,我事先也不需要去训练他,但我只是把他本能构建起来以后,他可以通过自己去参考,然后高效的获取数据很快来进行收敛,然后这样的学习。
[53:02.720 -> 53:04.120] 很快来进行收链,然后这样的一个学习。
[53:06.040 -> 53:07.760] 所以总结一下就是说,新有的这种学习,它可能需要大量的信息,
[53:07.760 -> 53:10.040] 但是数据维度在那里比较的严重,
[53:10.040 -> 53:11.920] 然后另外它有些信息是无法获取的。
[53:12.800 -> 53:16.960] 所以我们更多的是采用这种就是构建技能的方式去干这个事情。
[53:16.960 -> 53:23.400] 无论是在这个抓取力调控和这个抓取的危险选取这一块,
[53:23.400 -> 53:24.920] 我们给团队的提问就是说,
[53:24.920 -> 53:30.600] 我们能不能现在这个模型机率层面上能做到我们的极限,然后这样的东西再教会学习去做。
[53:32.280 -> 53:47.280] 最后,这个是我在哈尔博的时候,当时做人文学习新的科学研究,就是人在做这个工作的时候,他其实是一个copy,然后有个预测,这个预测会带来一个感官能力的增强
[53:47.280 -> 53:50.080] 所以这个增强其实是一个非常重要的东西
[53:50.080 -> 53:51.680] 就像我们投篮球,我们投了多了以后
[53:51.680 -> 53:54.560] 在这个篮球出手之前,我们其实就有些预判
[53:54.560 -> 53:59.840] 所以我们也想到的是,其实是最终我们训练增强的
[53:59.840 -> 54:04.000] 应该是把这样的一个抓取更本能的一些预测能力
[54:04.000 -> 54:05.160] 给它布�的提升
[54:05.640 -> 54:07.520] 然后来干这个事情
[54:07.520 -> 54:09.520] 但这个事情怎么干我们现在也不太清楚
[54:09.520 -> 54:12.640] 因为我们对学习现在也不是太了解
[54:13.680 -> 54:15.920] 可以总结一下就是说我们做了传感器
[54:15.920 -> 54:17.840] 做了后面的做机械手的干这个事情
[54:18.400 -> 54:20.000] 也能获得一些动物战略信息
[54:20.520 -> 54:22.400] 最终我们其实想做一个事情
[54:22.400 -> 54:23.680] 就是做智能甲质
[54:23.960 -> 54:27.800] 就智能甲质这一个其实其实是一个比较困难的东西
[54:27.800 -> 54:29.160] 但我们调研完以后发现
[54:29.160 -> 54:30.560] 现在智能甲质里面有这样一个问题
[54:30.560 -> 54:31.840] 就是说这边是一个患者
[54:31.840 -> 54:32.680] 他说了一句话
[54:32.680 -> 54:35.200] 说这个带完智能甲质以后
[54:35.200 -> 54:37.400] 让他感觉到就更残疾
[54:37.400 -> 54:40.560] 这个原因就在于他好像觉得这个手已经不是自己的
[54:40.560 -> 54:42.280] 这里面问题其实是两个
[54:42.280 -> 54:46.220] 第一个他没办法获得更真实的这样的学习反馈
[54:46.220 -> 54:48.380] 第二个是他的操作很复杂
[54:48.380 -> 54:49.780] 他不能做自我的操作
[54:49.780 -> 54:54.900] 所以我们如果能通过触觉给他提供更丰富的反馈
[54:54.900 -> 54:59.300] 以及通过自我化和自学习的这样的操作能力
[54:59.300 -> 55:01.140] 让他能具备底层智能
[55:01.140 -> 55:06.120] 就不用通过人残余的神经信号去非常困难的去克服它
[55:06.120 -> 55:07.600] 所以这样的话就
[55:07.600 -> 55:11.080] 但这个事情应该对我们来说可能五到十年以后
[55:11.080 -> 55:12.800] 才能真正的能把这个事情也启动起来
[55:12.800 -> 55:13.880] 因为设计的东西太多了
[55:13.880 -> 55:17.880] 所以这就是我们在做触觉
[55:17.880 -> 55:21.760] 做自身就是自觉一抓去里面
[55:21.760 -> 55:24.680] 后面我们觉得可以去做进一步探索的地方
[55:24.680 -> 55:27.040] 最后很快的就是讲一下我们
[55:27.040 -> 55:29.720] 另外一个跟触觉不太相关的是高视觉的东西
[55:29.720 -> 55:33.120] 就这块其实是从建筑行业里面引出了一个需求
[55:33.120 -> 55:34.680] 就是说当时的建筑行业
[55:34.680 -> 55:35.640] 体量很大
[55:35.640 -> 55:37.000] 但知识化程度非常低
[55:37.000 -> 55:38.840] 跟器械相比这个差距太大了
[55:38.840 -> 55:41.080] 所以我们开始觉得建筑机型
[55:41.080 -> 55:44.920] 最开始是B股员找到我们来做这个事情
[55:44.920 -> 55:48.040] 我们一般来说觉得这个事情有什么难做吗
[55:48.040 -> 55:49.800] 因为电路它的进度也不高
[55:49.800 -> 55:52.320] 所以我们给人家弃墙任务
[55:52.800 -> 55:57.120] 但是后来我们分析完以后发现这个事情很困难
[55:57.120 -> 55:58.600] 比方说我们在弃这个墙
[55:58.680 -> 55:59.840] 因为它尺度空间比较大
[55:59.840 -> 56:01.520] 所以它只能用移动机器人
[56:02.000 -> 56:03.920] 在移动机器人的时候在运动中
[56:04.080 -> 56:06.400] 比方它有障碍物和定位偏差
[56:06.480 -> 56:07.560] 这个基本上都在0米级别
[56:07.920 -> 56:08.960] 所以导致它的计准没了
[56:09.560 -> 56:11.080] 因为做机器人我们知道机器人需要有计准
[56:11.080 -> 56:12.280] 如果它不知道自己计准的话
[56:12.280 -> 56:14.440] 它根本就没法做控制
[56:14.920 -> 56:15.920] 第二个它是浮动的
[56:16.200 -> 56:17.320] 它最后会有偏白
[56:18.080 -> 56:20.680] 所以这个气墙任务是在1毫米 0.5毫米级别
[56:21.080 -> 56:22.880] 所以这种情况下就是这三个误差
[56:22.880 -> 56:24.000] 都会都在0米级别
[56:24.040 -> 56:25.400] 所以它几乎不太可能能实现1毫米级别的这些计论所以这个误差都会都在0米级别所以它几乎不太可能能实现
[56:25.400 -> 56:26.600] 1毫米级别的这些进度
[56:27.600 -> 56:28.600] 所以这个误差来源就是
[56:28.600 -> 56:30.000] 基准确实受到偏白
[56:30.000 -> 56:32.600] 这个事情就是说怎么来做高精度移动机器人
[56:33.200 -> 56:34.800] 这个事情我们也一直在探索
[56:35.200 -> 56:37.400] 我们后来觉得人怎么操作的
[56:37.400 -> 56:39.000] 人其实不管自己脚在什么地方
[56:39.000 -> 56:41.400] 他就眼睛看着这个操作对象
[56:41.600 -> 56:42.800] 靠这个目标来引导他
[56:43.200 -> 56:44.400] 所以我们提出这样一个叫
[56:45.040 -> 56:47.080] 融合视觉的目标特征测量与自引导
[56:47.080 -> 56:50.840] 就是说我把所有的这个记轮都放到木栏去
[56:51.280 -> 56:53.600] 然后我记成在什么地方其实我不用关注
[56:54.520 -> 56:56.880] 它的这个记轮当然可以靠这个
[56:57.240 -> 56:59.360] 我们主动用的特征 比方说靠激光
[56:59.520 -> 57:01.560] 然后靠这个建筑施工里面的面积光
[57:02.320 -> 57:05.360] 发现它的效果确实比较好
[57:05.560 -> 57:06.400] 就是即使
[57:06.600 -> 57:07.480] 就像这边我们
[57:07.680 -> 57:08.720] 发现它在运动过程中
[57:08.720 -> 57:10.640] 如果你看到它的基础其实放很大
[57:10.640 -> 57:11.720] 甚至人给它一些
[57:12.080 -> 57:12.680] 这个老东
[57:12.880 -> 57:14.320] 其实它并不太关注
[57:14.560 -> 57:15.600] 然后最近我们研究
[57:15.640 -> 57:18.200] 呃我们在做这个理论部分研究的时候发现
[57:18.440 -> 57:21.240] 呃提出就是以前有图样视觉制度
[57:21.520 -> 57:23.320] 那图样视觉制度其实在这个场景里面
[57:23.320 -> 57:24.920] 其实还是有些局限
[57:29.040 -> 57:30.880] 我们后来发现就是如果能获得6D位置信息它即使基准有很大的偏差
[57:30.880 -> 57:33.440] 它都可以通过自己的不断的迭代
[57:33.440 -> 57:36.040] 然后让它的墨量可以保证很高的进度
[57:36.040 -> 57:40.000] 这个就有点符合我们的期望
[57:40.000 -> 57:42.400] 就是人在做工作的时候不断的调控
[57:43.000 -> 57:45.400] 然后这个技术我们也用的这个航天装配
[57:47.400 -> 57:47.680] 然后装配里面我们就是只看它的一些
[57:49.560 -> 57:50.080] 这个观念特征然后去引导它
[57:53.600 -> 57:53.640] 然后因为马斯克他就是说他通过造机车
[57:55.800 -> 57:59.840] 造卫星嘛我们也来做了这样的一个事情就是说把这个就是流水线来造东西
[57:59.920 -> 58:02.400] 但流水线上面的这个这个技术其实它的
[58:02.800 -> 58:03.920] 进度是不高的
[58:04.040 -> 58:07.000] 所以通所以说通过视觉引导来干这样的事情
[58:08.520 -> 58:09.520] 就是为了做这个事情
[58:09.520 -> 58:10.720] 我们也做了一些引领平台
[58:10.720 -> 58:11.600] 这边快速过一下
[58:11.600 -> 58:12.840] 就是说我们针对这个
[58:13.520 -> 58:15.280] 就是电路场景做了
[58:15.280 -> 58:17.240] 颈头型的这样移动平台
[58:17.840 -> 58:19.200] 主要是做一些悬架设计和
[58:19.200 -> 58:20.520] 这个车体位置性融合
[58:21.920 -> 58:22.960] 这边很重要的东西就是说
[58:22.960 -> 58:27.200] 我需要对这个位置编排进行补偿,就比方说这边的小车会
[58:27.200 -> 58:30.040] 很难晃动,但末端可以始终保持这个姿态不变
[58:30.640 -> 58:33.160] 然后你就在运营过程中,如果没有补偿
[58:33.160 -> 58:35.160] 它这个末端变化很大,但是补偿完以后
[58:35.160 -> 58:37.600] 它这个末端可以始终的保持一个比较好的进度
[58:37.920 -> 58:41.640] 这个在装修机器人里面,最近我们有一个项目是做装修机器人
[58:41.880 -> 58:44.400] 装修机器人就是它在不同运动中,比方说做喷涂啊
[58:44.400 -> 58:45.760] 做打磨
[58:45.760 -> 58:48.520] 它就要保证末端能够尽可能高到精度
[58:48.520 -> 58:51.360] 所以可以通过对极度的进步来实现
[58:52.200 -> 58:53.920] 然后我们也研发了GAB
[58:53.920 -> 58:55.560] 其实GAB现在市面上很多
[58:55.560 -> 58:57.680] 但它的需求非常的奇特
[58:57.680 -> 58:58.880] 它的次载要求很大
[58:58.880 -> 58:59.960] 比方50公斤的负载
[59:00.160 -> 59:02.160] 但它的本体载重不能超过150公斤
[59:02.160 -> 59:03.760] 就正常的GAB50公斤负载
[59:03.760 -> 59:04.960] 负重在500公斤
[59:06.640 -> 59:08.640] 他们要求轻的原因是因为建筑的楼板承重是有限的
[59:08.640 -> 59:10.760] 所以我们开发了这个轻量的重量GAB
[59:10.760 -> 59:15.280] 这个GAB重点是一个大扭矩的集中化驱动模组
[59:15.280 -> 59:16.360] 和一些突破优化
[59:16.360 -> 59:20.080] 这个是跟国际的这样一个指标比较酷的ABB
[59:20.080 -> 59:22.720] 这个整个的体积比他们小很多
[59:22.720 -> 59:25.800] 其实就是造了一个很大负载的血栓基因病
[59:25.800 -> 59:26.800] 这个概念
[59:28.240 -> 59:32.240] 最后这是一些跟本国航天的一些总装单位
[59:32.240 -> 59:34.960] 和碧果严这边来做了一些合作
[59:34.960 -> 59:37.640] 所以这个可能是更贴近于实际应用
[59:37.640 -> 59:42.440] 所以这个时候我们就是正在所保留的一个研究方向之一
[59:44.720 -> 59:48.640] 以上就介绍完了我们的整个三大部分工作
[59:48.640 -> 59:52.520] 推产产品、刷具和这样的一个特产自营党装备
[59:52.960 -> 59:56.400] 最后有两个片子就是我们对我们研究方向的一个思考
[59:56.960 -> 59:58.600] 这个我们也没有太想清楚
[59:58.600 -> 01:00:00.920] 但是大概的思考就是我们还是以
[01:00:00.920 -> 01:00:03.360] 因为我们是这个基础学科
[01:00:03.360 -> 01:00:05.560] 我们最后还是应该以应用为载体
[01:00:05.560 -> 01:00:08.320] 所以我们第一个思考规划其实就是基于
[01:00:08.320 -> 01:00:08.960] 这样的一个
[01:00:09.520 -> 01:00:10.440] 硬件的载体
[01:00:10.440 -> 01:00:12.080] 就是说我们已经有了这个机械臂
[01:00:12.080 -> 01:00:12.880] 我们也有
[01:00:12.880 -> 01:00:14.120] 水传感器
[01:00:14.120 -> 01:00:17.240] 我们也有这个集成视觉测量的一体化模担
[01:00:17.560 -> 01:00:20.840] 所以我们研发了一个集成化的复合机器人
[01:00:21.160 -> 01:00:22.040] 就是来做
[01:00:22.480 -> 01:00:23.080] 就是
[01:00:23.640 -> 01:00:25.600] 一些移动操作
[01:00:25.600 -> 01:00:27.600] 包括一些交互操作
[01:00:27.600 -> 01:00:29.400] 它的特点就是说
[01:00:29.400 -> 01:00:32.200] 我手术空间比较大
[01:00:32.200 -> 01:00:35.000] 但是我的精度也可以保证比较高
[01:00:35.000 -> 01:00:40.000] 另外在做一些需要更自主化的工作
[01:00:40.000 -> 01:00:43.400] 触觉可以提供非常多的一些信息补充
[01:00:44.200 -> 01:00:47.000] 然后从整个方向这块其实我们就要有机器人
[01:00:47.280 -> 01:00:49.320] 教育感知与自主智能的操作
[01:00:49.840 -> 01:00:52.440] 其实废弃过化主要体现在工作场景和操作对象
[01:00:53.000 -> 01:00:54.840] 这一块就是目前的两个
[01:00:55.040 -> 01:00:57.520] 一个在大数据供应下的机器人的一个高级操作
[01:00:57.800 -> 01:01:00.200] 第二个是机器人自适应自学习的操作的激励
[01:01:01.040 -> 01:01:02.920] 然后这块当然应用场景这块我们是
[01:01:03.320 -> 01:01:06.400] 主要是找一些现在还没有被机器人很好应用的
[01:01:06.400 -> 01:01:08.800] 比如进入施工啊 零装备啊 制造工程
[01:01:08.800 -> 01:01:11.600] 包括未来的智能甲子 处理这些现实的
[01:01:11.600 -> 01:01:13.600] 最后就是一个使命
[01:01:13.600 -> 01:01:16.600] 就是说机器人他被提出
[01:01:16.600 -> 01:01:19.600] 要拓展他的应用场景
[01:01:19.600 -> 01:01:24.600] 更好的去帮助人类去来做一些比较枯燥的
[01:01:24.600 -> 01:01:26.160] 或者比较危险的一些任务
[01:01:26.160 -> 01:01:29.800] 所以这也是我们在做的工作的一个使命
[01:01:29.800 -> 01:01:32.000] 这就是我的社会报告
[01:01:32.000 -> 01:01:33.360] 时间有点长
[01:01:33.360 -> 01:01:34.760] 谢谢
```