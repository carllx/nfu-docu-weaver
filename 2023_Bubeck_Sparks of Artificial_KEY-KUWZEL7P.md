---
ä½œè€…: SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang
ä¸­æ–‡æ ‡é¢˜: SparksofArtificialGeneralIntelligence
åˆ†ç±»: Intelligent creation, art-making machines
å½±å“å› å­: 
---



# Sparks of Artificial General Intelligence: Early experiments with GPT-4
> [!info]+ <center>Metadata</center>
> 
> |<div style="width: 5em">Key</div>|Value|
> |--:|:--|
> |æ–‡çŒ®ç±»å‹|preprint|
> |æ ‡é¢˜|Sparks of Artificial General Intelligence: Early experiments with GPT-4|
> |çŸ­æ ‡é¢˜|SparksofArtificialGeneralIntelligence|
> |ä½œè€…|[[SÃ©bastien Bubeck]]ã€ [[Varun Chandrasekaran]]ã€ [[Ronen Eldan]]ã€ [[Johannes Gehrke]]ã€ [[Eric Horvitz]]ã€ [[Ece Kamar]]ã€ [[Peter Lee]]ã€ [[Yin Tat Lee]]ã€ [[Yuanzhi Li]]ã€ [[Scott Lundberg]]ã€ [[Harsha Nori]]ã€ [[Hamid Palangi]]ã€ [[Marco Tulio Ribeiro]]ã€ [[Yi Zhang]]|
> |æœŸåˆŠåç§°||
> |DOI|[10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712)|
> |å­˜æ¡£ä½ç½®|66 ğŸ“Š|
> |é¦†è—ç›®å½•|arXiv.org|
> |ç´¢ä¹¦å·||
> |ç‰ˆæƒ||
> |åˆ†ç±»|[[Intelligent creation, art-making machines]]|
> |æ¡ç›®é“¾æ¥|[My Library](zotero://select/library/items/KUWZEL7P)|
> |PDF é™„ä»¶|[2023_Sparks of Artificial General Intelligence.pdf](zotero://open-pdf/library/items/BCA4EPXC)|
> |å…³è”æ–‡çŒ®||
> ^Metadata

> [!example]- <center>æœ¬æ–‡æ ‡ç­¾</center>
> 
> `$=dv.current().file.tags`

> [!quote]- <center>Abstract</center>
> 
> Artificial intelligence (AI) researchers have been developing and refining [[LLMs|large language models]] (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.

> [!tldr]- <center>éšè—ä¿¡æ¯</center>
> 
> itemType:: preprint
> title:: Sparks of Artificial General Intelligence: Early experiments with GPT-4
> shortTitle:: SparksofArtificialGeneralIntelligence
> creators:: [[SÃ©bastien Bubeck]]ã€ [[Varun Chandrasekaran]]ã€ [[Ronen Eldan]]ã€ [[Johannes Gehrke]]ã€ [[Eric Horvitz]]ã€ [[Ece Kamar]]ã€ [[Peter Lee]]ã€ [[Yin Tat Lee]]ã€ [[Yuanzhi Li]]ã€ [[Scott Lundberg]]ã€ [[Harsha Nori]]ã€ [[Hamid Palangi]]ã€ [[Marco Tulio Ribeiro]]ã€ [[Yi Zhang]]
> publicationTitle:: 
> journalAbbreviation:: 
> volume:: 
> issue:: 
> pages:: 
> language:: 
> DOI:: [10.48550/arXiv.2303.12712](https://doi.org/10.48550/arXiv.2303.12712)
> ISSN:: 
> url:: [http://arxiv.org/abs/2303.12712](http://arxiv.org/abs/2303.12712)
> archive:: 
> archiveLocation:: 66 ğŸ“Š
> libraryCatalog:: arXiv.org
> callNumber:: 
> rights:: 
> extra:: 66 citations (Semantic Scholar/arXiv) [2023-04-30] 66 citations (Semantic Scholar/DOI) [2023-04-30] arXivï¼š2303.12712 [cs]
> collection:: [[Intelligent creation, art-making machines]]
> tags:: [[reading]] [[Computer_Science_-_Artificial_Intelligence]] [[Computer_Science_-_Computation_and_Language]]
> related:: 
> itemLink:: [My Library](zotero://select/library/items/KUWZEL7P)
> pdfLink:: [2023_Sparks of Artificial General Intelligence.pdf](zotero://open-pdf/library/items/BCA4EPXC)
> qnkey:: 2023_Bubeck_Sparks of Artificial_KEY-KUWZEL7P
> date:: 2023-04-13
> dateY:: 2023
> dateAdded:: 2023-04-29
> dateModified:: 2023-04-29
> 
> abstract:: Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.


%--------------Ï‰--------------%

## âœï¸ ç¬”è®°åŒº

> [!WARNING]+ <center>ğŸ£ æ€»ç»“</center>  
>
>ğŸ¯ ç ”ç©¶é—®é¢˜::  
ğŸ” ç ”ç©¶èƒŒæ™¯::  
ğŸš€ ç ”ç©¶æ–¹æ³•::  
ğŸ” ç ”ç©¶æ€è·¯::  
ğŸ“º ä¸»è¦å†…å®¹::  
ğŸ‰ ç ”ç©¶ç»“è®º::  
ğŸ—ï¸ åˆ›æ–°ç‚¹::  
ğŸ’© ç ”ç©¶å±€é™::  
ğŸ¾ ç ”ç©¶å±•æœ›::  
âœï¸ å¤‡æ³¨::  

> [!inbox]- <center>ğŸ“« å¯¼å…¥æ—¶é—´</center>
>
> â° importDate:: 2023-04-30

%--------------Ï‰--------------%

## ğŸ“ æ³¨é‡Šç¬”è®° BCA4EPXC

> <span style="font-size: 15px;color: gray">ğŸ“ 2023-Bubeck-Sparks of Artificial General Intelligence: Early experiments with GPT-4</span>

^KEYrefTitle

> <span class="highlight" style="background-color: [[ffd400]]">Text-to-image synthesis models have been widely explored in recent years, but they often suffer from a lack of spatial understanding capabilities and the inability to follow complex instructions [GPN+22]. For example, given a prompt such as â€œdraw a blue circle on the left and a red triangle on the rightâ€, these models may produce images that are visually appealing but do not match the desired layout or colors. On the other hand, GPT-4 can generate code from a prompt, which can be rendered as an image, in a way that is true to the instructions to a higher degree of accuracy. However, the quality of the rendered image is usually very low. Here, we explore the possibility of combining GPT-4 and existing image synthesis models by using the GPT-4 output as the sketch. As shown in Figure 2.8, this approach can produce images that have better quality and follow the instructions more closely than either model alone. We believe that this is a promising direction for leveraging the strengths of both GPT-4 and existing image synthesis models. It can also be viewed as a first example of giving GPT-4 access to tools, a topic we explore in much more depth in Section 5.1.</span>  
> æ–‡æœ¬åˆ°å›¾åƒçš„åˆæˆæ¨¡å‹å·²ç»è¢«å¹¿æ³›æ¢ç´¢ï¼Œä½†å®ƒä»¬å¾€å¾€ç¼ºä¹ç©ºé—´ç†è§£èƒ½åŠ›ï¼Œæ— æ³•éµå¾ªå¤æ‚çš„æŒ‡ä»¤[GPN+22]ã€‚ä¾‹å¦‚ï¼Œç»™å®šä¸€ä¸ªæç¤ºï¼Œå¦‚ "åœ¨å·¦è¾¹ç”»ä¸€ä¸ªè“è‰²çš„åœ†ï¼Œåœ¨å³è¾¹ç”»ä¸€ä¸ªçº¢è‰²çš„ä¸‰è§’å½¢"ï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ä¼šäº§ç”Ÿè§†è§‰ä¸Šå¸å¼•äººçš„å›¾åƒï¼Œä½†ä¸ç¬¦åˆæ‰€éœ€çš„å¸ƒå±€æˆ–é¢œè‰²ã€‚å¦ä¸€æ–¹é¢ï¼ŒGPT-4å¯ä»¥ä»æç¤ºä¸­ç”Ÿæˆä»£ç ï¼Œè¿™äº›ä»£ç å¯ä»¥è¢«æ¸²æŸ“æˆå›¾åƒï¼Œå…¶æ–¹å¼ä¸æŒ‡ä»¤çš„çœŸå®ç¨‹åº¦æ›´é«˜ã€‚ç„¶è€Œï¼Œæ¸²æŸ“çš„å›¾åƒçš„è´¨é‡é€šå¸¸å¾ˆä½ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨GPT-4çš„è¾“å‡ºä½œä¸ºè‰å›¾ï¼Œæ¢ç´¢å°†GPT-4å’Œç°æœ‰çš„å›¾åƒåˆæˆæ¨¡å‹ç›¸ç»“åˆçš„å¯èƒ½æ€§ã€‚å¦‚å›¾2.8æ‰€ç¤ºï¼Œè¿™ç§æ–¹æ³•å¯ä»¥äº§ç”Ÿè´¨é‡æ›´å¥½çš„å›¾åƒï¼Œå¹¶ä¸”æ¯”å•ç‹¬çš„ä»»ä½•ä¸€ä¸ªæ¨¡å‹éƒ½æ›´æ¥è¿‘æŒ‡ä»¤ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ï¼Œå¯ä»¥åˆ©ç”¨GPT-4å’Œç°æœ‰å›¾åƒåˆæˆæ¨¡å‹çš„ä¼˜åŠ¿ã€‚å®ƒä¹Ÿå¯ä»¥è¢«çœ‹ä½œæ˜¯è®©GPT-4è·å¾—å·¥å…·çš„ç¬¬ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬5.1èŠ‚ä¸­æ›´æ·±å…¥åœ°æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚ ([p18](zotero://open-pdf/library/items/BCA4EPXC?page=18&annotation=ETMTBXAW))

^KEYETMTBXAW

