# UI 理解-可靠的交互逐字稿

Jeff_Nichols_HCML_Workshop_2024
[ 🍎 apple Apple Workshop on Human-Centered Machine Learning 2024: UI Understanding Machine Learning Research](@https://machinelearning.apple.com/video/ui-understanding)

![bg fit left:50% vertical](https://i.imgur.com/YwD2O7Q.webp)

## 基本信息

- 视频时长：[35:21]
    
- 讲者数量：[1位]
    
- 转录时间：[2024-08-10 11:30]
    

## 💡 我的学习收获

### 我学到的核心内容

我从这个视频中学到了UI理解这一前沿领域的系统性框架。该领域的目标是让机器具备四种核心能力：理解、交互、评估和创建UI。演讲清晰地展示了从基础的感知任务（如屏幕识别）到复杂的交互（如自动化测试）和生成（如代码生成）任务的演进路径。我认识到，实现这些目标需要解决一系列挑战，包括数据稀疏性、类别不平衡以及如何有效评估UI质量等。

### 我的理解和思考

我认为最重要的观点是，UI理解的四个核心能力是相互依存、层层递进的。没有对UI元素的准确“理解”，就不可能有可靠的“交互”；没有有效的“交互”能力，就难以进行深入的“评估”；而高质量的“评估”反馈，则是实现智能“创建”的前提。此外，“永无止境学习”的概念让我印象深刻，它通过自动化爬虫与真实应用交互来持续收集高质量的动态数据，解决了传统静态截图标注的局限性，为模型的持续迭代提供了一条可行路径。这是一种将数据飞轮效应应用于UI模型训练的巧妙思路。

### 我的实践计划

我计划将视频中提到的评估方法应用到我的工作中。例如，在进行UI设计评审时，可以思考如何借鉴UIClip项目的思路，建立更量化的设计评估标准，而不仅仅依赖主观判断。对于开发流程，可以探索如何集成类似AXNav的自动化测试工具，来提前发现可访问性问题，从而提高产品质量和包容性。我也会关注Ferret-UI这类多模态模型的发展，思考如何将其用于自动化UI检查、设计合规性验证等场景。

### 我想进一步探索的

我希望深入了解大型多模态模型（如Ferret-UI）在UI设计评估和生成中的具体实现细节。特别是，它们是如何在没有明确设计规则的情况下，学习到“好设计”的内在模式的？此外，我也对UICoder项目中使用的自动化反馈循环很感兴趣，想了解其背后的技术细节，比如如何定义和量化“高质量”的代码输出，以及这种方法是否可以推广到其他代码生成领域。最后，随着AI能力的增强，如何平衡自动化生成与人类设计师的创造力，将是一个值得长期思考的课题。

## 完整逐字稿

### [00:01] 主讲者 Jeff Nichols

大家好。今天我将概述我的团队在过去四年中所做的工作，这部分工作处于机器学习和 Human-Computer Interaction（人机交互，简称HCI）的交叉领域。我们将这个研究领域称为 UI Understanding（用户界面理解）。

### [00:16] 主讲者 Jeff Nichols

那么，究竟什么是用户界面理解呢？对我而言，我喜欢用一个宏大且长期的目标来定义它。具体来说，我认为我们工作的目标是赋予机器达到人类水平的能力，去**理解**、**交互**、**评估**和**创建**用户界面。

### [00:35] 主讲者 Jeff Nichols

你们中有些人可能之前见过这个框架，但会发现“评估”和“创建”是我们相对较新加入的目标。我认为这些目标是层层递进的。我发现，这也是我们团队的一个新工作领域。当我们审视这些目标时，会发现它们是相互依赖、逐级递进的。

### [00:54] 主讲者 Jeff Nichols

你需要先理解用户界面才能与之交互。与界面的交互能极大地帮助我们进行评估，而评估对于构建能够创建优秀用户界面的系统至关重要，因为我们需要在创建过程中判断它们的好坏。

### [01:08] 主讲者 Jeff Nichols

现在，让我们来逐一分解这四个目标。

### [01:15] 主讲者 Jeff Nichols

首先，“理解”主要是一个感知问题。在我们团队，我们专注于图形用户界面（GUI）。具体来说，我们希望机器能够通过查看屏幕截图或运行中的帧缓冲区，识别屏幕上存在的元素，理解它们的结构，判断其功能和整体设计等。

### [01:37] 主讲者 Jeff Nichols

其次，“交互”则需要更进一步。这不仅要求感知界面，还需要能够操作它。这需要理解的不仅仅是单个屏幕，而是整个应用程序或体验，包括应用的结构、哪些任务是可行的、完成特定任务需要哪些步骤等等。

### [01:56] 主讲者 Jeff Nichols

近年来，Large Language Models (大型语言模型) 和多模态模型的进步使我们能够处理以前无法完成的任务，这在我的演讲中你们将会看到。我团队的大部分工作，正如你们将看到的，都集中在前两个类别：理解和交互。但我们也在另外两个领域有项目，并且我认为它们在我们团队内部以及整个研究社区中都变得越来越重要。

### [02:26] 主讲者 Jeff Nichols

第三，“评估”则涉及到一个在人机交互领域长期存在的重大挑战：我们能否自动评估用户界面的各个维度，就像在用户研究中所做的那样？并由此判断一个界面是否可以在视觉上做得更有吸引力、更高效，或者是否能更好地匹配用户的心理模型。

### [02:49] 主讲者 Jeff Nichols

最后是“创建”。这实际上让我想起了我的博士论文，很多年前我研究过用户界面的自动生成。我很高兴Don没有告诉你们我何时拿到的博士学位。

### [03:01] 主讲者 Jeff Nichols

这里的问题是，我们能否根据用户需求，生成新的用户界面或修改现有界面？我们的重点尤其放在生成能够产生用户界面的代码上。我认为这是一个更具挑战性的问题，甚至比一般的代码生成任务更难。例如，我们已经看到像 Microsoft（微软）的 Copilot for Visual Studio 这样的工具。我们不只是为有明确输入输出的数据结构和算法生成代码，而是要生成一个持续运行、需要处理事件的用户界面，我認為这是一个复杂得多的问题。

### [03:42] 主讲者 Jeff Nichols

那么，为什么我认为从事用户界面理解的工作很重要呢？答案很简单，因为我们认为这项技术有非常多的潜在应用。这包括可访问性、各种自动化与辅助功能、软件测试等等，还有很多其他的应用。

### [04:01] 主讲者 Jeff Nichols

今天，我将以这四个目标的分解为指导，向大家介绍我团队过去四年来为实现这些长期目标所做的一些工作。我真的要感谢我的团队，他们在这么短的时间内取得了如此多的成就。这也意味着我会讲得很快，并且会跳过其中几个项目，因为Amanda会在稍后的闪电演讲中详细介绍它们。

### [04:26] 主讲者 Jeff Nichols

让我们从一些旨在实现“理解”目标的项目开始。这要从我们在这个领域的第一个项目说起，它叫做 Screen Recognition（屏幕识别）。

### [04:35] 主讲者 Jeff Nichols

这是我们团队的第一个重大成功。这个功能叫做“屏幕识别”，你可以在 iOS 中找到它。我想Jeff昨天也提到了。它最早在 iOS 14 中推出，我记得是2020年。

### [04:49] 主讲者 Jeff Nichols

它的工作原理是利用计算机视觉和机器学习模型来理解并向屏幕阅读器用户传达界面的内容。这对于那些没有指定各种可访问性元数据的应用特别有用，这些应用的屏幕阅读器以前无法描述界面内容。但现在，通过我们提供的计算机视觉技术，它能够理解屏幕并向用户描述这些信息。这项工作也发表在 CHI'21（人机交互顶级会议'21）的一篇论文中，并且获得了最佳论文奖。

### [05:25] 主讲者 Jeff Nichols

屏幕识别的实现包含三个主要部分：一个用于检测屏幕上UI元素的计算机视觉模型；一个用于提取图标语义的图标识别模型；以及一个将相关元素组合在一起的组件，以便我们能理解哪些东西是相互关联的。

### [05:55] 主讲者 Jeff Nichols

后两个组件在后续的工作中得到了改进。所以，我想先谈谈屏幕识别，然后再介绍我们后来的改进。

### [06:00] 主讲者 Jeff Nichols

屏幕识别的元素检测部分，是基于一个在标注好的iOS屏幕截图数据集上训练的目标检测模型。我们从超过4239个应用中收集了超过80,945个屏幕，所有屏幕上的UI元素位置都由人工标注员进行了标注。

### [06:20] 主讲者 Jeff Nichols

在构建一个鲁棒的系统时，我们遇到了一些挑战。一个相对不意外的问题是我们的类别非常不平衡。某些类型的元素，比如文本和图标，非常常见；而另一些，比如滑块、对话框、复选框则非常少见。这导致模型在不同类别上的表现差异很大。

### [06:56] 主讲者 Jeff Nichols

第二个更令人意外的挑战是类别之间的混淆。我们发现很多标准元素之间存在相似性，比如图片和图标，或者图片和容器。最终，我们得到了一个性能不错的模型。从那时起，我们通过更好的数据和改进的模型架构对其进行了优化。这个预训练的目标检测骨干网络已经成为我们许多其他模型的基础。

### [08:02] 主讲者 Jeff Nichols

好的，现在我们来谈谈图标识别。我们在一篇 CHI'22 的论文中改进了这项工作，这篇论文由我们的实习生 Jieshan Chen（陈杰山）主导。

### [08:14] 主讲者 Jeff Nichols

大家可能不会惊讶，图标是移动应用中第二常见的UI元素类型（仅次于文本）。因此，它们对UI理解至关重要。但在可访问性方面，它们很难处理，因为它们常常缺少可访问性标签。一项研究分析了1万个安卓应用，发现77%的应用中超过三分之二的图像按钮缺少标签。

### [08:42] 主讲者 Jeff Nichols

因此，我们决定分析我们收集的数据集，以更多地了解图标类型的分布。我们发现，存在一个长尾分布。有98种常见图标类型，以及一个包含331种长尾图标类型的长尾部分。这种分布非常不均匀。

### [09:08] 主讲者 Jeff Nichols

基于此分析，我们设计了一个流程来改进图标识别。系统首先接收截图作为输入，然后为屏幕上的每个图标返回一个标签。首先，我们运行元素检测，然后将其输入一个图像分类模型来检测98种常见的小部件。如果检测到不是常见类型，我们就会转到一个少样本学习方法，尝试分配长尾图标类型。最后，我们应用一些启发式规则，比如检测附近的文本或识别图标上的修饰符（如通知红点），这能提供关于图标可能代表什么的额外线索。

### [10:22] 主讲者 Jeff Nichols

我们对整个端到端的识别流程进行了评估。结果发现，综合来看，我们能够正确分类约90.7%的图标，包括常见和长尾类型。如果只看长尾类型，我们的分类准确率约为78.6%。所以结果相当不错，我们对此非常满意。

### [10:43] 主讲者 Jeff Nichols

接下来，让我们讨论分组组件。在最初版本的屏幕识别中，这完全是通过启发式规则实现的，没有机器学习。但在UIST'21的一篇论文中，我们探索了一种使用机器学习模型来推断元素分组的方法。这项工作由我们的长期实习生 Jason Wu（吴杰森）主导，你会在接下来的演讲中多次听到他的名字。

### [11:17] 主讲者 Jeff Nichols

分组推理之所以必要，是因为我们的元素检测模型只产生一个扁平的检测列表，包含位置和大小，并没有揭示它们的结构关系。然而，UI本质上是分层的。了解结构能帮助我们理解UI，而屏幕识别最初使用启发式方法来推断结构。我们的问题是：用机器学习能做得更好吗？

### [11:50] 主讲者 Jeff Nichols

我们将这种推断结构的方法称为“屏幕解析”（Screen Parsing），因为它借鉴了自然语言处理（NLP）中传统解析模型的建模技术。我们的方法接收一个屏幕截图作为输入，并生成一个树状结构，这个结构对应用有很多有用的属性。重要的是，树中的所有叶节点都与屏幕上出现的元素对应，所有屏幕上的元素都出现在树中，而树的分支节点则代表了抽象的列表、导航栏等概念。

### [12:31] 主讲者 Jeff Nichols

为了生成结果，我们使用了多阶段系统，包含三个组件。猜猜第一个是什么？是一个UI元素检测器。然后我们进入一个层级预测器，最后是一个组标签器。

### [12:44] 主讲者 Jeff Nichols

这个过程的第一步是处理截图，生成一个UI元素的检测列表。接着，我们使用一个基于堆栈指针的转换解析器（stack-pointer transition-based parser）来推断检测到元素的结构。它从一个LSTM编码器开始，编码所有元素，然后一个解码器网络使用堆栈数据结构和指针机制，输出一系列用于构建树的动作。最终的结果就是一个树形输出，它让我们理解了元素之间的关系。

### [13:20] 主讲者 Jeff Nichols

我们使用一系列基于分类准确率和图距离的指标对我们的系统进行了训练和评估。我们使用的主要比较基准是屏幕识别中用于可访问性元数据的启发式方法。我们发现，屏幕解析在推断UI层级方面要准确得多。我们还发现，我们的最终训练过程，使用了一种动态预言机（dynamic oracle）方法，带来了巨大的性能提升。

### [13:50] 主讲者 Jeff Nichols

接下来，我想谈谈我们做的两个与“交互”相关的项目。第一个是“用户界面的永无止境学习”（Never-Ending Learning of User Interfaces）。

### [14:21] 主讲者 Jeff Nichols

这个项目是由Jason Wu和实习生Rebecca Krosnick以及我们团队共同完成的，去年在UIST'23上发表。这个项目源于我们意识到，对静态屏幕截图进行人工标注可能非常嘈杂，特别是对于某些类型的数据。例如，如果你只是看一个屏幕截图，被问到某个元素是否可点击或可拖动，这可能很难判断。

### [15:00] 主讲者 Jeff Nichols

标注者别无选择，只能猜测。他们无法接触到正在运行的界面来验证。因此，这会引入很多噪音。

### [15:13] 主讲者 Jeff Nichols

为了更好地理解这一点，请看这两个屏幕。你能仅凭截图猜出这些屏幕上的交互方式吗？你可能已经识别出一些，比如有一些明显的开关按钮，顶部有按钮，底部栏也有按钮。你甚至可能猜到，在天气应用中点击那些卡片会跳转到另一个屏幕，在这种情况下你是对的。但也有很多应用，卡片是不可点击的。所以，一个标注者对天气应用中的这些特定元素会怎么猜呢？这就不清楚了。

### [16:03] 主讲者 Jeff Nichols

除此之外，还有一些动态的方面你可能没有预料到。也许如果你对iOS有所了解，你会知道在iOS的列表中，你通常可以向左滑动来显示一个删除按钮，这在静态截图中是看不到的。还有其他类型的可拖动性。所以，如果你只看静态截图，你会预料到这些东西是可拖动的吗？可能不会。这给我们的感知模型带来了挑战。

### [16:37] 主讲者 Jeff Nichols

那么，我们如何才能比当前这种仅使用人工标注员和屏幕截图的方法更便宜地收集更好、更可靠的用户界面数据呢？我们认为，理想的技术应该需要更少的人力，利用真实UI的动态行为，而不是依赖于容易出错的静态截图标注，并且应该能够轻松地从最新的应用中持续收集数据和改进模型。因为一个担忧是概念漂移：UI会随着时间变化，我们的模型会失效吗？

### [17:39] 主讲者 Jeff Nichols

我们为此构建了一个系统，实现了用户界面的永无止境学习。这项工作受到了Tom Mitchell（汤姆·米切尔）等人在NLP社区工作的启发。他们大约十年前开创了一种用于自然语言处理的永无止境学习方法。在他们的工作中，一个网络爬虫持续地探索互联网，收集各种语义关系。

### [18:06] 主讲者 Jeff Nichols

我们的工作类似，我们构建了UI爬虫，它们持续下载并与应用交互。但我们收集的数据类型不同。当它们与应用交互时，我们的系统会收集屏幕截图和标注标签，用于标记可点击性、可拖动性等特性。基本上就是打开一个界面，选择一个东西点击，看它是否做了什么，然后记录下来作为一个标签。

### [18:32] 主讲者 Jeff Nichols

这些数据反过来被用来训练更新的UI理解模型，这反过来也能提高应用爬虫的质量。因此，随着应用被更多地爬取，随着模型改进，我们希望收集到的数据能进一步改进，从而形成一个正向的反馈循环。

### [19:12] 主讲者 Jeff Nichols

要收集好的标注标签，这是我们实现可点击性标注的过程。我们建立一个基线，在系统执行动作前截屏，然后执行一个模拟点击，之后再截一张屏，通过比较两张截图来推断动作的效果。当然，这里面有些棘手的地方。我们想确保我们的启发式规则是好的。于是我们把截图和操作视频给内部的人工标注员进行评估。我们发现，我们设计的启发式规则的整体准确率达到了0.934。

### [20:33] 主讲者 Jeff Nichols

当然，这个系统的核心前提是它会随着时间推移而改进。那么，随着更多爬取任务的运行和更多数据的收集，它真的会改进吗？对于可点击性和可拖动性，我们都看到了一个积极的上升趋势。更重要的是，这些模型超过了我们用静态截图数据从头训练的模型的性能，这正是我们的目标。我们证明了，基于动态交互来推断可点击性的启发式方法，确实产生了噪音更少的数据。

### [21:10] 主讲者 Jeff Nichols

总结一下这个系统。我们看到了从主动UI训练中获得的显著好处，而不是标准的静态截图标注方法。我们能够训练出改进的可点击性和可拖动性模型。现在我们正在探索如何长期最好地运行这个系统，找到能最大化学习率的新爬取策略，并探索除可点击性和可拖动性之外的其他训练任务。

### [21:35] 主讲者 Jeff Nichols

现在我想谈谈AXNav项目，它刚刚在CHI'24上展示，是我们团队和实习生Miriam Taeb以及Yuhan Jiang合作的成果。AXNav的目标是改进交互式可访问性测试。

### [21:51] 主讲者 Jeff Nichols

目前，很多这类测试是手动完成的，需要额外的工作，因为它通常需要测试一个功能在没有可访问性工具的情况下如何使用，然后再开启各种可访问性工具（如VoiceOver）进行测试。

### [22:05] 主讲者 Jeff Nichols

这个系统的架构如这张图所示。它以一组测试指令作为输入，通常不只是指令，而是一个短语，比如“分享一个播客单集”。这个短语被发送到一个基于LLM的规划器，它控制一个真实设备来执行任务，无论是否开启了特定的可访问性功能。最终的输出是一个视频，展示了任务以两种方式执行的过程，并带有分章节的标签，测试人员可以跳转查看动作是如何执行的，并分析以确保一切都成功完成。

### [24:07] 主讲者 Jeff Nichols

让我们来看看这个系统的一些细节。如我所说，系统使用真实设备来执行任务。我认为这个系统一个非常关键的设计方面是，我们为基于LLM的规划组件设计了三代理架构。这是一个三代理系统：有一个规划代理，它创建执行导航的初始计划；在计划的每一步，动作代理尝试根据计划执行该步骤。如果它无法执行，比如因为某个UI元素不可用，它会触发规划代理重新规划，生成新计划并尝试下一步。如果它能够执行动作，它就会在真实设备上执行，然后触发第三个代理——评估代理，它会判断该步骤是否成功完成。如果评估代理认为动作不成功，它也可以触发重新规划。我认为这个方法一个很好的地方是，它不是一次性的规划方法。我们在开始时生成一个计划，并尝试执行它，但如果失败了，我们还有机会重新审视计划并进行修正。

### [25:28] 主讲者 Jeff Nichols

系统的另一个部分是设备控制模块，它与真实设备交互。它既可以执行常规操作，如点击或在文本字段中输入文本，也可以执行一系列特殊的VoiceOver命令，就像你们在之前的演示中看到的那样，它自动化了屏幕阅读器。

### [25:46] 主讲者 Jeff Nichols

最后，我们有一系列启发式方法被应用于生成后处理结果，这些结果可以被测试人员检查。

### [25:53] 主讲者 Jeff Nichols

为了评估AXNav，我们测试了来自App Store的免费应用和我们内部测试库的回归测试。对于免费应用，我们成功重放了70%的测试用例；对于回归测试，我们成功重放了85.5%的用例。这涵盖了从简单到困难的各种任务。

### [26:12] 主讲者 Jeff Nichols

AXNav展示了使用大型语言模型进行UI理解应用的前景。自从这项工作以来，我们对探索大型模型的其他用途产生了浓厚的兴趣，特别是创建大型多模态模型。这是我们内部一个合作项目的一部分，该项目产生了Ferret-UI（一个多模态模型），你可能已经在arXiv上看到过，这项工作很快将在ECCV'24会议上发表。

### [26:38] 主讲者 Jeff Nichols

正如你所见，在我们之前描述的大多数工作中，我们创建了一些更传统的模型来执行各种UI理解任务。对于大型多模态模型，我们的一个问题是，我们能否教一个单一模型来完成所有这些事情？通过Ferret-UI，我们特别关注了三种能力：指称（referring）、定位（grounding）和推理（reasoning）。

### [27:01] 主讲者 Jeff Nichols

指称是描述图像中特定位置事物的任务，例如“左上角的图标是什么？”。定位基本上是相反的任务，比如“打开按钮在哪里？”，然后在屏幕上找到那个东西的位置。推理任务则更复杂，比如“我可以用这个元素做什么？”或“我如何在这个屏幕上完成一个任务？”。

### [27:23] 主讲者 Jeff Nichols

这项工作建立在之前一个名为Ferret的模型之上，它本身也是一个多模态模型，由我们的合作团队创建，可以对非UI图像执行这些任务。我们与那个团队合作，将UI数据添加到训练混合物中，然后评估其UI能力。

### [27:41] 主讲者 Jeff Nichols

我们在一系列文献中的任务上对其进行了评估，发现它的表现与许多其他系统相比都非常好。特别是在iOS上，它的表现非常好，甚至可以与GPT-4V相媲美。在安卓任务上，它也优于我们比较的大多数模型（除了GPT-4V），尽管它并没有在任何安卓数据上进行过训练。所以我们看到了一些跨平台的泛化能力，我认为这很重要。

### [28:10] 主讲者 Jeff Nichols

总的来说，我认为Ferret-UI和这些多模态模型似乎是UI理解的一个有前途的方向。

### [28:19] 主讲者 Jeff Nichols

最后，我将非常快速地介绍我们最近的两个项目，它们开始探索“创建”和“评估”的子目标。这两个项目都由Jason Wu主导。

### [28:33] 主讲者 Jeff Nichols

第一个是UICoder。UICoder中的问题是：我们能否改进用户界面的代码生成？我们还增加了难度，提出能否在不支付人类专家来制作高质量示例或从其他专有模型中提取数据的情况下做到这一点。

### [28:50] 主讲者 Jeff Nichols

答案是：我们可以！我们实现这一目标的方法是这样的：我们有一个初始的LLM模型，让它生成大量的不同程序或用户界面。然后我们评估所有这些界面，丢掉所有我们认为质量低的，只对高质量的进行微调，然后重复这个过程。

### [29:21] 主讲者 Jeff Nichols

有趣的是，即使对于开源模型，这种方法也相当有效。

### [29:26] 主讲者 Jeff Nichols

我讲得很快，但这里是实验结果。我们发现这种方法效果很好。我们使用的两个过滤器是编译率和相关性分数。我们发现编译率在每次迭代中都显著提高，尽管我们开始的开源模型甚至没有在Swift上训练过，但最终它能够很好地编译Swift代码。

### [30:03] 主讲者 Jeff Nichols

另外一件事是，相关性分数是基于一个CLIP模型。我们会给出屏幕的描述，然后我们拍下生成的屏幕截图，让CLIP模型评估这两者匹配得有多好。

### [30:20] 主讲者 Jeff Nichols

这里是一些生成的界面示例。它们还行，比我们最初得到的要好得多，但显然还有很长的路要走。我认为其中一个原因是CLIP模型在评估用户界面方面还不是很好。例如，如果你要求一个iPad界面，然后给它一个屏幕，上面用大字写着“iPad”，它会给出一个很高的相关性分数，但那不是一个好的用户界面。

### [30:50] 主讲者 Jeff Nichols

为了解决这个问题，我们推出了一个新系统，它将出现在UIST'24，叫做UIClip。我没有时间详细讨论这个，但基本上，我们从这个问题开始：机器学习模型能准确评估UI设计吗？我们创建了一些例子，把它们展示给GPT-4V，结果它编造了很多东西，做得并不好。

### [31:18] 主讲者 Jeff Nichols

所以，我们想看看能否做得更好。我们从生成数据集开始。Jason想出了一个很聪明的方法。我们抓取了网页，将屏幕渲染成移动端形式，我们称之为“好”的屏幕。然后，我们对这些屏幕应用了各种“抖动”（jitters），比如改变颜色、字体大小等，这些操作几乎在所有情况下都会让界面变得更糟。这样，我们就生成了230万对“好”与“坏”的屏幕。我们还进行了一些人工数据收集，让设计师对UI进行排名。

### [31:57] 主讲者 Jeff Nichols

我们用这些数据来训练一个新的CLIP模型，它对UI数据和UI屏幕有更好的理解。我们对此进行了评估。论文在arXiv上有，你可以查看更多细节。我们发现，我们经过UI优化的CLIP模型在理解和评估用户界面方面要好得多，甚至与GPT-4V和其他专有模型相比也是如此。

### [32:35] 主讲者 Jeff Nichols

好了，我要总结了。我超时了两分半，但我们开始得早，所以还好。

### [32:42] 主讲者 Jeff Nichols

我希望你们看到的是，我们已经为实现UI理解这个宏大的长期目标做了很多工作。当然，还有很多工作要做。

### [32:56] 主讲者 Jeff Nichols

在“理解”方面，我们已经建立了支持我们其余工作的基础技术，但仍有很大的改进空间。比如跨平台的泛化能力，以及大型多模态模型与小型传统模型之间的权衡。

### [33:23] 主讲者 Jeff Nichols

在“交互”方面，我们看到自动爬取可以为我们的感知模型生成有用的训练数据，而且多代理/提示方法在导航方面很有前景。但规划仍然是一个具有挑战性的方面，我们需要更多的数据来在导航方面取得进展。幸运的是，这个领域有很多新工作出现，我们希望将来也能发布我们自己的数据集。

### [33:52] 主讲者 Jeff Nichols

最后，“创建”和“评估”对我们来说是新领域，但它们都是HCI领域的长期重大挑战。大型模型能在多大程度上推进这个问题？合成数据生成可以提供帮助，但能持续多久？评估技术可以帮助推动生成技术，反之亦然。如何扩展到真实规模的创建和评估问题？我们还需要更多数据。

### [34:39] 主讲者 Jeff Nichols

我认为UI理解是一个非常棒的研究领域。在过去几年里，研究社区发展迅速。大型模型加速了这项工作，尤其是在导航方面。新出现的对代理导航用户界面的兴趣，为这个领域带来了很多新人。多模态模型可能会改变游戏规则，但要让这些模型达到人类水平的能力，还有很多工作要做。

### [35:02] 主讲者 Jeff Nichols

我很高兴看到接下来会发生什么。在结束之前，我想感谢所有在苹果公司为UI理解工作做出贡献的人，包括我们的实习生、全职员工，他们中的许多人今天都在这里。我真的很幸运能和这群优秀的人一起工作，没有他们，我今天展示的一切都不可能实现。

## 🔍 图像补充说明

- **[00:16]** 幻灯片展示了UI理解的长期目标：赋予机器达到人类水平的能力，以实现对用户界面的**理解(understand)**、**交互(interact with)**、**评估(evaluate)**和**创建(create)**。这四个动词用不同颜色高亮，构成了演讲的核心框架。
    
- **[01:15]** 幻灯片将“理解”与“感知”(perception)关联，并列出了关键问题，如：存在哪些元素？每个元素意味着什么？元素如何结构化？哪些元素是可交互的？它们可能做什么？
    
- **[01:38]** 幻灯片将“交互”与“驱动”(actuation)关联，并列出关键问题，如：这个应用的结构是什么？我当前在应用的哪个位置？在这个应用中完成一个任务是否可行？一个代理将如何完成该任务？
    
- **[02:26]** 幻灯片将“评估”与“评估”(evaluation)关联，并列出关键问题，如：视觉外观是否清晰且吸引人？给定任务能否高效完成？设计是否符合用户的心理模型？
    
- **[02:48]** 幻灯片将“创建”与“生成”(generation)关联，并列出关键问题，如：我们如何生成复杂的UI代码？代码是否能创建功能性和美观的布局？代码是否有效利用了可用的UI工具包和API？
    
- **[03:42]** 幻灯片展示了UI理解的潜在应用，将其与四个核心目标对应，包括个人与情境辅助、可访问性、自动化、软件测试、设计辅助、开发者辅助等。
    
- **[04:34]** 幻灯片展示了“屏幕识别”功能的实例，一个手机音乐应用界面上的所有UI元素（如专辑封面、按钮、文本）都被红色边框高亮标出，展示了模型识别元素的能力。
    
- **[05:25]** 幻灯片通过三个并列的手机截图，展示了屏幕识别的三个核心组件：UI元素检测（所有元素被框出）、图标识别（返回和搜索图标被识别）、分组（图片、标题和文本被识别为一个逻辑单元）。
    
- **[05:55]** 幻灯片详细介绍了屏幕识别模型，提到其基于一个包含80,945个屏幕截图的大型数据集，并指出了两个主要挑战：类别不平衡（某些UI元素非常罕见）和类别混淆（如图片与图标难以区分）。
    
- **[09:09]** 幻灯片展示了一个系统流程图，描述了从目标检测开始，通过通用图标分类和长尾图标分类，再结合附近文本检测和修饰符识别，最终完成图标识别的完整过程。
    
- **[11:17]** 幻灯片用一个分解图形象地展示了UI的内在层级结构，从最外层的窗口（Window）到导航视图（Navigation View）、列表视图（List View）和标签栏视图（Tab Bar View）。
    
- **[11:49]** 幻灯片展示了“屏幕解析”的核心思想，左侧是一个电商应用的截图，右侧是该截图被解析成一个层次分明的树状结构图，根节点下分出列表视图、导航栏和标签栏。
    
- **[17:39]** 幻灯片展示了“永无止境学习”的系统架构：一个协调器服务器（Coordinator Server）分发任务给大量的爬虫工作者（Crawler Workers），这些爬虫在设备上安装应用、与之交互并标注数据，然后将截图和标签传回服务器以更新模型，形成一个持续学习的闭环。
    
- **[22:05]** 幻灯片展示了AXNav系统的架构，测试指令（如“VoiceOver：分享一个播客单集”）被输入一个基于LLM的多代理规划器，该规划器通过设备控制模块来执行动作和控制辅助功能（如VoiceOver），最终生成带章节的视频。
    
- **[26:35]** 幻灯片展示了大型视觉语言模型在UI理解中的应用，右侧的手机截图中，模型的注意力集中在不同的UI元素上，并回答了关于这些元素的各种问题，涵盖了指称、定位和推理三种能力。
    
- **[28:55]** 幻灯片展示了UICoder-SFT的流程图，说明了如何通过一个现有的LLM模型生成大量UI代码，然后通过自动化的过滤器筛选出高质量的数据集，再用这个高质量数据集来微调一个新的、性能更好的LLM模型。
    

## 📚 重要术语和人物

- **UI Understanding** (用户界面理解): 一个结合机器学习和人机交互的领域，旨在赋予机器像人一样理解、交互、评估和创建用户界面的能力。
    
- **Screen Recognition** (屏幕识别): 一项利用计算机视觉和机器学习来识别和描述屏幕上UI元素的技术，主要用于提升应用的可访问性。
    
- **Screen Parsing** (屏幕解析): 一种通过机器学习模型推断UI元素之间层级结构的技术，将扁平的UI元素列表转换为树状结构。
    
- **Never-Ending Learning** (永无止境学习): 一种机器学习范式，系统通过持续与环境交互来自动收集数据并不断改进自身模型。
    
- **AXNav**: 一个自动化系统，它使用大型语言模型来解析自然语言指令，并自动在移动设备上执行可访问性测试任务。
    
- **Ferret-UI**: 一个大型多模态模型，专为UI理解任务设计，能够执行指称、定位和推理等多种复杂的UI分析任务。
    
- **UICoder**: 一个通过自动化反馈和迭代精调来提升大型语言模型生成UI代码能力的项目。
    
- **UIClip**: 一个数据驱动的模型，专门用于评估用户界面的设计质量。
    
- **Jeff Nichols (杰夫·尼科尔斯)**: 本次演讲的主讲者，来自苹果公司的专家。
    
- **Jason Wu (吴杰森)**: 讲者团队中的一位杰出实习生，领导了多个关键项目，如屏幕解析和UICoder。
    

## ⏰ 关键时间节点

- **[00:16]** 核心概念：提出UI理解的四大目标：理解、交互、评估和创建
    
- **[04:34]** 核心项目介绍：开始介绍“屏幕识别”项目，这是UI理解的基础
    
- **[11:50]** 关键技术：讲解“屏幕解析”，即从屏幕截图推断UI的层级结构
    
- **[17:39]** 创新方法：介绍“永无止境学习”系统，通过自动化交互收集高质量数据
    
- **[21:36]** 应用演示：展示AXNav项目，利用LLM自动化执行可访问性测试
    
- **[26:12]** 前沿方向：探讨大型多模态模型（Ferret-UI）在UI理解中的巨大潜力
    
- **[28:19]** 总结与展望：对UI理解领域的现状进行总结，并展望未来的研究方向