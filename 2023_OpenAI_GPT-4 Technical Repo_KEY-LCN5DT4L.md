---
作者:  OpenAI
中文标题: 
分类: Intelligent creation, art-making machines
影响因子: 
---



# GPT-4 Technical Report
> [!info]+ <center>Metadata</center>
> 
> |<div style="width: 5em">Key</div>|Value|
> |--:|:--|
> |文献类型|preprint|
> |标题|GPT-4 Technical Report|
> |短标题||
> |作者|[[ OpenAI]]|
> |期刊名称||
> |DOI|[10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)|
> |存档位置|82 📊|
> |馆藏目录|arXiv.org|
> |索书号||
> |版权||
> |分类|[[Intelligent creation, art-making machines]]|
> |条目链接|[My Library](zotero://select/library/items/LCN5DT4L)|
> |PDF 附件|[2023_GPT-4 Technical Report.pdf](zotero://open-pdf/library/items/377SC9DN)|
> |关联文献||
> ^Metadata

> [!example]- <center>本文标签</center>
> 
> `$=dv.current().file.tags`

> [!quote]- <center>Abstract</center>
> 
> We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.

> [!tldr]- <center>隐藏信息</center>
> 
> itemType:: preprint
> title:: GPT-4 Technical Report
> shortTitle:: 
> creators:: [[ OpenAI]]
> publicationTitle:: 
> journalAbbreviation:: 
> volume:: 
> issue:: 
> pages:: 
> language:: 
> DOI:: [10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
> ISSN:: 
> url:: [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)
> archive:: 
> archiveLocation:: 82 📊
> libraryCatalog:: arXiv.org
> callNumber:: 
> rights:: 
> extra:: 82 citations (Semantic Scholar/arXiv) [2023-04-30] 82 citations (Semantic Scholar/DOI) [2023-04-30] arXiv：2303.08774 [cs]
> collection:: [[Intelligent creation, art-making machines]]
> tags:: [[reading]] [[Computer_Science_-_Artificial_Intelligence]] [[Computer_Science_-_Computation_and_Language]]
> related:: 
> itemLink:: [My Library](zotero://select/library/items/LCN5DT4L)
> pdfLink:: [2023_GPT-4 Technical Report.pdf](zotero://open-pdf/library/items/377SC9DN)
> qnkey:: 2023_OpenAI_GPT-4 Technical Repo_KEY-LCN5DT4L
> date:: 2023-03-27
> dateY:: 2023
> dateAdded:: 2023-04-29
> dateModified:: 2023-04-29
> 
> abstract:: We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.


%--------------ω--------------%

## ✏️ 笔记区

> [!WARNING]+ <center>🐣 总结</center>  
>
>🎯 研究问题::  
🔎 研究背景::  
🚀 研究方法::  
🐔 研究思路::  
📺 主要内容::  
🎉 研究结论::  
🗝️ 创新点::  
💩 研究局限::  
🐾 研究展望::  
✏️ 备注::  

> [!inbox]- <center>📫 导入时间</center>
>
> ⏰ importDate:: 2023-04-30

%--------------ω--------------%

## 📝 注释笔记 377SC9DN

> <span style="font-size: 15px;color: gray">📍 2023-OpenAI-GPT-4 Technical Report</span>

^KEYrefTitle

> <span class="highlight" style="background-color: [[ffd400]]">2.2 Hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=IFMYB4HT))

^KEYIFMYB4HT

> <span class="highlight" style="background-color: [[ffd400]]">close domain hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=ZYPRR26Z))

^KEYZYPRR26Z

> <span class="highlight" style="background-color: [[ffd400]]">open domain hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=RDN2ZFAE))

^KEYRDN2ZFAE

