---
ä½œè€…:  OpenAI
ä¸­æ–‡æ ‡é¢˜: 
åˆ†ç±»: Intelligent creation, art-making machines
å½±å“å› å­: 
---



# GPT-4 Technical Report
> [!info]+ <center>Metadata</center>
> 
> |<div style="width: 5em">Key</div>|Value|
> |--:|:--|
> |æ–‡çŒ®ç±»å‹|preprint|
> |æ ‡é¢˜|GPT-4 Technical Report|
> |çŸ­æ ‡é¢˜||
> |ä½œè€…|[[ OpenAI]]|
> |æœŸåˆŠåç§°||
> |DOI|[10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)|
> |å­˜æ¡£ä½ç½®|82 ğŸ“Š|
> |é¦†è—ç›®å½•|arXiv.org|
> |ç´¢ä¹¦å·||
> |ç‰ˆæƒ||
> |åˆ†ç±»|[[Intelligent creation, art-making machines]]|
> |æ¡ç›®é“¾æ¥|[My Library](zotero://select/library/items/LCN5DT4L)|
> |PDF é™„ä»¶|[2023_GPT-4 Technical Report.pdf](zotero://open-pdf/library/items/377SC9DN)|
> |å…³è”æ–‡çŒ®||
> ^Metadata

> [!example]- <center>æœ¬æ–‡æ ‡ç­¾</center>
> 
> `$=dv.current().file.tags`

> [!quote]- <center>Abstract</center>
> 
> We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.

> [!tldr]- <center>éšè—ä¿¡æ¯</center>
> 
> itemType:: preprint
> title:: GPT-4 Technical Report
> shortTitle:: 
> creators:: [[ OpenAI]]
> publicationTitle:: 
> journalAbbreviation:: 
> volume:: 
> issue:: 
> pages:: 
> language:: 
> DOI:: [10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
> ISSN:: 
> url:: [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774)
> archive:: 
> archiveLocation:: 82 ğŸ“Š
> libraryCatalog:: arXiv.org
> callNumber:: 
> rights:: 
> extra:: 82 citations (Semantic Scholar/arXiv) [2023-04-30] 82 citations (Semantic Scholar/DOI) [2023-04-30] arXivï¼š2303.08774 [cs]
> collection:: [[Intelligent creation, art-making machines]]
> tags:: [[reading]] [[Computer_Science_-_Artificial_Intelligence]] [[Computer_Science_-_Computation_and_Language]]
> related:: 
> itemLink:: [My Library](zotero://select/library/items/LCN5DT4L)
> pdfLink:: [2023_GPT-4 Technical Report.pdf](zotero://open-pdf/library/items/377SC9DN)
> qnkey:: 2023_OpenAI_GPT-4 Technical Repo_KEY-LCN5DT4L
> date:: 2023-03-27
> dateY:: 2023
> dateAdded:: 2023-04-29
> dateModified:: 2023-04-29
> 
> abstract:: We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.


%--------------Ï‰--------------%

## âœï¸ ç¬”è®°åŒº

> [!WARNING]+ <center>ğŸ£ æ€»ç»“</center>  
>
>ğŸ¯ ç ”ç©¶é—®é¢˜::  
ğŸ” ç ”ç©¶èƒŒæ™¯::  
ğŸš€ ç ”ç©¶æ–¹æ³•::  
ğŸ” ç ”ç©¶æ€è·¯::  
ğŸ“º ä¸»è¦å†…å®¹::  
ğŸ‰ ç ”ç©¶ç»“è®º::  
ğŸ—ï¸ åˆ›æ–°ç‚¹::  
ğŸ’© ç ”ç©¶å±€é™::  
ğŸ¾ ç ”ç©¶å±•æœ›::  
âœï¸ å¤‡æ³¨::  

> [!inbox]- <center>ğŸ“« å¯¼å…¥æ—¶é—´</center>
>
> â° importDate:: 2023-04-30

%--------------Ï‰--------------%

## ğŸ“ æ³¨é‡Šç¬”è®° 377SC9DN

> <span style="font-size: 15px;color: gray">ğŸ“ 2023-OpenAI-GPT-4 Technical Report</span>

^KEYrefTitle

> <span class="highlight" style="background-color: [[ffd400]]">2.2 Hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=IFMYB4HT))

^KEYIFMYB4HT

> <span class="highlight" style="background-color: [[ffd400]]">close domain hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=ZYPRR26Z))

^KEYZYPRR26Z

> <span class="highlight" style="background-color: [[ffd400]]">open domain hallucinations</span> ([p46](zotero://open-pdf/library/items/377SC9DN?page=46&annotation=RDN2ZFAE))

^KEYRDN2ZFAE

