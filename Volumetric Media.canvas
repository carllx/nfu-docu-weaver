{
	"nodes":[
		{"id":"e3e4e15821089ea3","type":"group","x":-8080,"y":-5680,"width":1440,"height":1296,"label":"1.Introduction (VPCCRP)"},
		{"type":"group","id":"b447bed2ffc0714f","x":463,"y":-4560,"width":802,"height":1616,"label":"挑战"},
		{"type":"group","id":"c1b03e01e3eda849","x":3400,"y":-4930,"width":669,"height":986,"label":"The Source Side"},
		{"type":"text","text":"```\n目前在直播视频中, 建议使用多少像素? 如果有报告请注明数据来源\n```\n==来源未证实==\n总的来说，目前在直播视频中使用的像素分辨率多为 720p 和 1080p。\n根据不同的报告，目前在直播视频中使用的像素分辨率可能会有所差异。例如，根据 Streaming Media 发布的数据，2020 年最常用的像素分辨率为 720p，占比约为 60%。其次是 1080p，占比约为 30%。还有一些其他的像素分辨率，如 480p、4K 等，但占比较少。\n","id":"f562465a8e380551","x":5888,"y":-4055,"width":552,"height":278,"color":"2"},
		{"type":"file","file":"VP9.md","id":"80c94c9e96c077a7","x":6425,"y":-4593,"width":271,"height":148},
		{"type":"text","text":"M3U8 \n是一个用于[[流媒体视频的播放列表文件]]，但它不包含实际的视频数据。因此，它也不支持alpha通道。","id":"6f5000a4746f5c74","x":6775,"y":-3997,"width":250,"height":157},
		{"type":"text","text":"可被封装的[[Video Container formats(容器格式)]]","id":"300e890e57946fd9","x":6790,"y":-4550,"width":219,"height":88},
		{"type":"text","text":"MP4","id":"d53f7d6cab04509b","x":7084,"y":-4475,"width":250,"height":60},
		{"type":"text","text":"MKV 容器格式(开源)","id":"134dc55cf745e4d0","x":7084,"y":-4395,"width":250,"height":60},
		{"type":"text","text":"AVI","id":"52375d39cce4b97c","x":7084,"y":-4315,"width":250,"height":60},
		{"type":"text","text":"FLV","id":"734a157dbcc61df1","x":7084,"y":-4171,"width":250,"height":60},
		{"type":"text","text":"TS","id":"3beaf3b674a505e0","x":7084,"y":-4091,"width":250,"height":60},
		{"type":"text","text":"WebM 容器格式(开源)","id":"d73aac9527916b17","x":7084,"y":-4555,"width":250,"height":60},
		{"type":"text","text":"不支持alpha通道。 ","id":"fd7ee14aec08fdb5","x":7368,"y":-4171,"width":191,"height":140},
		{"type":"file","file":"Video compression codec (视频压缩编解码器).md","id":"3cac0d9a4fecc45a","x":6696,"y":-4871,"width":463,"height":235},
		{"type":"text","text":"PSNR","id":"5d3ebd7bc2103a64","x":5763,"y":-5093,"width":250,"height":119},
		{"type":"text","text":"# Six Degrees of Freedom (6DoF) 传输方式\n\nSix Degrees of Freedom (6DoF) 传输方式是指将数据通过六个自由度(即沿三个旋转轴和三个位移轴)进行传输的方式. 这种方式能够提供更加真实的三维体验, 并且在许多应用场景中都有广泛应用.\n\n对于体积媒体(Volumetric Media)而言, 6DoF 传输方式可以通过以下方式来传输体积媒体编码:\n\n1.  使用三维传感器(如激光扫描仪)来采集体积媒体的三维信息, 并将其编码成6DoF 格式.\n2.  将编码后的6DoF 数据通过网络或其他传输方式进行传输.\n3.  在接收端使用6DoF 解码器将传输的6DoF 数据解码为体积媒体的三维信息.\n\n通过这种方式, 6DoF 传输方式可以实现体积媒体的高效传输. 但是, 需要注意的是, 6DoF 传输方式的效率和体验质量可能受到传输速率和网络环境的影响.","id":"dd17bb5d023eb908","x":3381,"y":-5375,"width":686,"height":375},
		{"type":"text","text":"## The Network Domain\n- Application And Transport Layer Optimizations\n- Intelligent Network Components\n这篇论文的 \"The Network Domain\" 部分讨论了用于流式传输六自由度内容到客户端的几种技术。作者提到了 [[HAS (HTTP Adaptive Streaming)]]和基于 UDP 的协议，如 WebRTC 和 HTTP/3。作者还提到了边缘计算 (MEC) 和内容缓存，以及网络加速器，可以帮助减少延迟并提高流畅度。此外，作者还提到了需要进一步研究的技术，如结合应用和传输层优化的协议，以及用于六自由度内容流式传输的负载均衡和分布式存储方法。\n\n### Application And Transport Layer Optimizations \n“Application And Transport Layer Optimizations” ([Hooft et al., 2020, p. 51](zotero://select/library/items/Z4RKIF2I)) ([pdf](zotero://open-pdf/library/items/UD2BMZ4N?page=3&annotation=59XVSJNI)) 提供了一种优化方法帮助提高视觉质量并减少重新缓冲事件.  \n即使用适用于传统视频的实时通信协议WebRTC或尚未标准化的基于QUIC的HTTP/3协议。这些协议可以通过建立多路复用的UDP连接来实现独立的多流数据传输，从而减少延迟。这部分还提供了使用QUIC协议的适应版本来可靠地传输关键帧，同时不保证检索其他帧的优化方法，这样就可以以更高的视觉质量和更少的重新缓冲事件流式传输视频。这种方法同样可以应用于体积媒体，通过优先考虑某些关键帧以及组成场景的最重要的空间区域和对象，并利用可靠和不可靠的传输来及时检索不同的对象和质量表示。\n\nGoogle报告, 在使用QUIC流式传输YouTube时, 发现桌面用户的重新缓冲率下降了18.0%, 尽管最近的研究显示了较低的改进水平.\n\n### Intelligent Network Components \n提到了两种方法来改善 6DoF 视频流媒体的效率：\n1.  多接入边缘计算 [[MEC(Multi-Access Edge Computing)]] ：这种方法可以让设备根据需要访问云/雾端资源，并且可以在边缘进行战略内容缓存。这对 6DoF 内容流媒体尤其重要，因为在有新任务请求时，服务器/网络需要迅速决定是否应该为将来的请求存储内容。\n    \n2.  SAND (Server and Network-Assisted DASH)：这是动态自适应流媒体标准 (Dynamic Adaptive Streaming over HTTP, DASH) 的一种扩展，它允许将网络、服务器、代理、缓存和客户端的实时信息与用户设备共享，帮助客户端在速率调整方面做出更好的决策，并优先从最有前途的位置（例如附近的缓存）请求内容。这个原则可以应用于 6DoF 媒体传递，因为 SAND 允许在使用附近的代理和缓存时显著减少延迟，同时还可以实时共享视频质量和 QoE 相关的度量。\n\n随着 5G 和软件化网络的出现，进一步的优化将变得越来越重要，因此将来应该得到更多的研","id":"eaaa5933eaf2d77b","x":3434,"y":-4417,"width":579,"height":425},
		{"type":"text","text":"## Content Encoding\n这篇文章的\"Content Encoding\"部分提到了2种压缩技术，用于减少存储和传输资源的数量。\n-  Mekuria et al. 的 codec\n-  V-PCC \n\n### Mekuria et al. 的 codec, \n使用时间相关性的编解码器, Mekuria 等人的编解码器，它使用点云帧之间的相关性来实现更好的压缩性能，通过将点云的边界框递归划分为八个子部分，对应树形结构的八个子节点，并通过迭代最近点算法计算转换，然后通过应用四元数量化方案进行压缩。\n\n### V-PCC \n(video-based point cloud compression) codec, 将点云分解为几何和纹理信息的两个单独的视频序列的编解码器。V-PCC。将点云分解为一系列补丁，然后将不同的补丁合并成两个分开的视频序列，分别捕捉几何和纹理信息。然后对两者都进行传统的视频编码技术进行压缩。总的来说，V-PCC可以在同一比特率下获得更高的视觉质量，并提供了 100 到 500 的有损压缩比。但是，V-PCC 的编码时间对于视频点播来说并不重要，但在直播视频的端到端延迟中有很大的贡献。\n\n\n并提出了一些未来的研究方向，例如\n- 开发用于高动态范围图像的编解码器，\n- 在网络传输过程中如何保证质量。\n\n使用基于树结构的方法将点云帧之间的相关性转化为压缩性能。基于两维网格的正交投影，为了进一步减少处理的数据量，可以使用称为剔除的过程删除冗余数据。","id":"9b566c053431d46f","x":3431,"y":-4653,"width":579,"height":202},
		{"type":"text","text":"##  V-Pcc\n\n### V-Pcc 的压缩过程\nVideo-Based Point Cloud Compression (V-Pcc )\n假设我们有一个点云数据，其中包含了一个物体的三维信息。这个点云数据有许多独立的点，每个点都有自己的位置和其他信息。我们希望将这个点云数据压缩成更小的大小，以便存储和传输。\n\n在视频基础点云压缩（V-PCC）的压缩过程中，我们首先会将点云数据分成若干个时间段。每个时间段内的点云数据都会有一定的变化趋势。然后，我们会对每个时间段的点云数据进行压缩，并将其存储在一个画面中。这样，我们就可以将点云数据压缩成视频的形式了。\n\n最后，我们会将所有画面组合在一起，形成一个视频文件。这个视频文件就是我们压缩后的点云数据。我们可以将这个视频文件存储在硬盘上，或者将其通过网络传输到其他地方。\n\n这就是视频基础点云压缩（V-PCC）的压缩过程的大致流程。通过这种方法，我们可以将点云数据压缩成更小的大小，从而使其更加方便存储和传输。\n\n---\n\n在视频基础点云压缩（V-PCC）的压缩过程中，纹理信息和几何信息是分别压缩到一个时间段的画面中的。\n\n也就是说，我们会将每个时间段的点云数据分成两部分进行压缩：一部分是几何信息，另一部分是纹理信息。几何信息会使用点云压缩方法进行压缩，而纹理信息会使用视频压缩技术进行压缩。最后，我们会将压缩后的几何信息和纹理信息分别存储在一个画面中，以形成一个时间段的点云数据。","id":"64fd1d55232060bd","x":4237,"y":-5375,"width":720,"height":445},
		{"type":"text","text":"MSE（Mean Squared Error）：MSE是一种常用的图像失真度度量方法，可以用来测量压缩后图像与原始图像之间的差异。MSE值越小，图像质量越高。","id":"4192f269ff64a4bc","x":4395,"y":-3885,"width":250,"height":60},
		{"type":"file","file":"SSIM（Structural Similarity Index).md","id":"bc7fa4f484eabb99","x":4395,"y":-3822,"width":250,"height":65},
		{"type":"file","file":"PSNR (peak signal-to-noise ratio).md","id":"b6796c2b96aa4107","x":4395,"y":-3730,"width":250,"height":69},
		{"type":"text","text":"# 研究, 新的点云压缩算法\n开发新的点云压缩算法。随着点云数据量的不断增长，有效的压缩算法将变得越来越重要。你可以考虑开发新的压缩算法，使其具有更高的效率和品质。","id":"9c4f7ac1b3f6d73d","x":2062,"y":-3615,"width":448,"height":152,"color":"6"},
		{"type":"text","text":"为了提升传输效率和体验质量, 开发新型的体积媒体编码方法","id":"2fd943b30b1f98d2","x":2062,"y":-3422,"width":448,"height":99},
		{"type":"text","text":"# 视觉感知指标\n\n失真度、清晰度、帧率等","id":"f2ece9f8b768cebe","x":2617,"y":-3885,"width":1299,"height":262},
		{"type":"text","text":"帧率","id":"f61a9009f8544e0b","x":4055,"y":-3539,"width":250,"height":60},
		{"type":"text","text":"## \"Content Capture and Representation\"部分\n捕捉三维物体的三种技术方案: \n-  基于图像\n-  基于点云\n-  基于网格\n\n### 基于图像的方案\n（即在网格上放置多个摄像机的设置）\n基于图像的解决方案需要在不同角度和倾斜处表示图像，通常使用相机阵列(即多个摄像机排列在网格上的布置)捕获对象。\n\n### 基于点云的方案\n（所谓的点云）\n基于体积媒体的解决方案将对象表示为一系列点(所谓的点云)，其中每个点都包含几何(x、y、z位置)和纹理(例如RGB值)的信息。根据每个点的位置，可以从任何观察角度渲染对象。内容可以通过专用摄像机设置或使用基于光探测和测距(LiDAR)的摄像机捕获。\n\n### 基于网格的方案\n基于网格的方案使用三角形来表示三维物体。顶点的坐标和三角形的纹理分量可以根据用户的位置和焦点来渲染物体。基于网格的方案可以更好地利用传统的图形管道，如 mipmaps 或各向异性过滤器，且在较大的比特率下显示的质量较高。但是，基于点云的方案在较低的比特率下表现更优，并且更容易支持动态和拓扑变化的捕捉。此外，它们具有比网格更简单的平铺和剔除功能。","id":"a257b5ce909e61ca","x":3431,"y":-4883,"width":579,"height":197},
		{"type":"file","file":"网络协议 Transport Protacol.md","id":"809340aa57695c27","x":3798,"y":-6212,"width":400,"height":400},
		{"type":"text","text":"失真度","id":"3279037cc241a722","x":4055,"y":-3885,"width":250,"height":60},
		{"type":"text","text":"清晰度","id":"34f888468d0c7a43","x":4055,"y":-3721,"width":250,"height":60},
		{"type":"text","text":"特征相似性（FSIM）\n自然图像质量评估器（NIQE）","id":"277f3b3485705af4","x":4696,"y":-3855,"width":250,"height":60},
		{"type":"text","text":"# 在数字教育领域的应用\n\n帮助学生理解和学习各种复杂的知识和技能。\n### 医学教育\n在医学教育中，volumetric media 可以用于展示人体内部的结构和功能，帮助医生和医学生更好地理解人体的工作原理。\n### 工程教育\n在工程教育中，volumetric media 可以用于展示机械装置的工作原理，帮助工程师和学生更好地理解机械装置的设计和操作。\n### 建筑教育\n在建筑教育中，volumetric media 可以用于展示建筑物的外观和内部结构，帮助建筑师和学生更好地理解建筑物的设计和构造。","id":"913bf9a2b96e1330","x":1315,"y":-3316,"width":636,"height":354},
		{"type":"text","text":"# 大小\n一个主要的挑战是点云数据的大小。点云数据往往具有非常大的数据量，并且需要大量的带宽和存储空间来传输和存储。这对于移动应用程序尤其困难，因为它们通常具有有限的存储和带宽资源。此外，点云数据的大小也可能对其处理和分析速度产生影响。","id":"696b8e874bcac86e","x":483,"y":-4540,"width":448,"height":209},
		{"type":"text","text":"# 研究, 特定应用场景遇到的挑战和机会\n研究点云在特定应用场景中的应用。你可以考虑研究点云在特定领域，如机器人导航、汽车自动驾驶、医疗保健、航空航天等，中的应用情况。你可以探讨如何在这些领域中使用点云，以及遇到的挑战和机会。","id":"29790edb3c2976f2","x":1386,"y":-4101,"width":455,"height":152,"color":"6"},
		{"type":"text","text":"# 研究, 更好的点云可视化技术\n研究, 新的点云可视化技术使其具有\n- 更好的性能\n- 用户体验","id":"74055f3fba8e08b7","x":1386,"y":-4321,"width":448,"height":193,"color":"6"},
		{"type":"text","text":"# 研究, 改进现有的点云处理算法\n改进现有的点云处理算法。点云处理算法可以用于模型重建、特征提取、曲面重建等应用。你可以考虑改进现有的算法，使其具有更高的准确性和效率。","id":"fb1f28d9369186a0","x":1386,"y":-4511,"width":450,"height":152,"color":"6"},
		{"type":"text","text":"## 从被动观看转变为主动参与\n| “user’s shift from a rather passive to an active role, real-time interaction with the content becomes crucial.” ([Hooft et al., 2020, p. 49](zotero://select/library/items/Z4RKIF2I)) ([pdf](zotero://open-pdf/library/items/UD2BMZ4N?page=1&annotation=JCI6KWWS)) 由于用户从被动观看转变为主动参与，与内容的实时交互变得至关重要。","id":"8bb798fa80ff8f52","x":1617,"y":-2780,"width":448,"height":212},
		{"type":"text","text":"## Visual Sovereignty (视觉主权)\n从visual sovereignty(视觉主权)和图像深度的角度看体积媒体的发展前景","id":"027a5757a91e74c0","x":1617,"y":-2912,"width":448,"height":121},
		{"type":"text","text":"# 研究, 拓展与其他数据类型的联系\n探究点云与其他数据类型的联系。\n- 图像\n- 视频\n- 音频等\n与这些数据类型的相结合。以及如何有效地处理这些多源数据。","id":"9f89e85521d80e1b","x":2065,"y":-3276,"width":448,"height":322,"color":"6"},
		{"type":"text","text":"# From Capturing to Rendering: Volumetric Media Delivery with Six Degrees of Freedom\n\n\n探讨了利用六自由度 (6DoF) 技术来传输体积媒体编码的状态和挑战. \n\n为了满足交互式应用（如远程手术和虚拟现实）所设定的低延迟和高带宽要求，人们进行 holographic-type content(全息类型内容) 的分发效率的研究。\n最近的研究使得六自由度（6DoF）成为了沉浸式媒体的可能，在这种情况下，用户可以移动头部并在场景中改变位置。\n- 我们基于体积媒体介绍了6DoF应用的现状和挑战，重点关注了提供这种服务所需的关键方面。\n- 此外，我们提出了一项主观研究的结果，以突出未来研究的相关方向。\n","id":"3272a2f0d8a45548","x":2616,"y":-5375,"width":686,"height":1431,"color":"1"},
		{"type":"text","text":"## Point Cloud(云点)\n点云是三维数据表示方法. \n常用于三维计算机视觉和几何处理的数据结构。\n点云可以通过各种技术获得，如laser scanning(激光扫描)、structured light(结构光)或stereo vision(立体视觉)。","id":"fcf93c941827380b","x":-474,"y":-4139,"width":469,"height":190},
		{"type":"text","text":"## point cloud fractal(点云分形):\n\n点云分形是一种使用分形算法表示三维空间信息的方法。点云分形可以用于表示复杂的几何形状，并且可以通过调整参数来控制细节程度。","id":"ac29c2366f10c2e0","x":-465,"y":-3907,"width":460,"height":156},
		{"type":"text","text":"# 挑战\n使用效率和可靠性","id":"3fc79c67c708492f","x":89,"y":-4010,"width":250,"height":122},
		{"type":"text","text":"# 传输性能\n一个主要的挑战是点云数据的大小。点云数据往往具有非常大的数据量，并且需要大量的带宽和存储空间来传输和存储。这对于移动应用程序尤其困难，因为它们通常具有有限的存储和带宽资源。此外，点云数据的大小也可能对其处理和分析速度产生影响。","id":"03cc74913b2024ae","x":483,"y":-4282,"width":448,"height":209},
		{"type":"text","text":"# 体验质量, 准确性\n另一个挑战是点云数据的质量。点云数据可能存在噪声、离群值和其他质量问题，这可能会影响点云的准确性和可靠性。因此，点云处理算法必须能够有效地应对这些问题，以便产生可信的结果。","id":"63ee28f68a95fb95","x":483,"y":-3980,"width":448,"height":185},
		{"type":"text","text":"# 应用场景\n三维计算机视觉\n模型重建\n激光扫描\n机器人导航\n汽车自动驾驶\n医疗保健, 足远程手术\n航空航天等","id":"c86458fcacd42b8d","x":483,"y":-3416,"width":448,"height":276},
		{"type":"text","text":"# 数据分析难度高\n由于点云数据通常包含大量的细节和几何信息，因此处理和分析这些数据也可能是一个挑战。这可能需要使用专业的算法和工具，例如多维尺度法、解析几何、曲面重建等。","id":"63dbea433defb9b9","x":771,"y":-3749,"width":395,"height":210},
		{"type":"text","text":"# Deep authoring - an AI Tool set for creating immersive MultiMedia experiences\n\nhttps://www.zotero.org/users/8931831/items/ZJ7PPMRL\n\n","id":"72971dc266c11da5","x":2702,"y":-1120,"width":686,"height":749,"color":"1"},
		{"type":"file","file":"zotero_mdnotes/@2008jennettMeasuringDefiningExperience.md","id":"40793338fbd294cc","x":2738,"y":-2954,"width":557,"height":1292,"color":"1"},
		{"type":"file","file":"认知理论小结.md","id":"94487d8e51706b52","x":3359,"y":-2954,"width":878,"height":115},
		{"type":"file","file":"Immersion 沉浸感超越了flow, CA 和 presence的认知理论.md","subpath":"#Immersion 沉浸感超越了flow, CA 和 presence的认知理论","id":"5da95e3071e536ce","x":3359,"y":-2723,"width":418,"height":415},
		{"type":"text","text":"## Deep authoring \n是一种人工智能技术，它可以自动生成文本或音频内容。Deep authoring通常使用神经网络模型来生成文本，这些模型可以通过学习大量的文本数据来模拟人类的写作风格和语言表达方式。Deep authoring可以用于自动生成新闻报道、社交媒体帖子、广告文本等内容。它还可以用于生成音频内容，例如自动生成音频广告或语音助手的语音响应。","id":"806ef101fab01687","x":3437,"y":-1120,"width":459,"height":229},
		{"type":"text","text":"## Person Segmentation 和 Depth Estimation 结合起来使用时\n\n人体分割（person segmentation）和深度估计（depth estimation）是两种不同的图像处理技术\n\n当人体分割和深度估计结合起来使用时，可以使用深度估计来分析人体的三维姿态。这种技术可以用于视频分析、运动跟踪和机器人控制等应用中。","id":"d1618ce5d41d2cd4","x":3437,"y":-851,"width":459,"height":480},
		{"type":"text","text":"## Person Segmentation\n\n人体分割是指在图像中识别人体部位并将其与背景分开的技术。这种技术通常用于人体姿态估计、人脸识别和视频分析等应用中。","id":"a342d3da9d0b88cf","x":3923,"y":-851,"width":459,"height":159},
		{"type":"text","text":"## Depth Estimation\n\n深度估计是指分析图像中物体的深度信息，从而估计物体在空间中的三维位置的技术。这种技术通常用于虚拟现实、机器人控制和图像处理应用中。\n\n在图像处理应用中，双目深度估计可以用于分割图像中的前景和背景，从而实从而实现抠图或提取特定物体的功能。例如，你可以使用双目深度估计来提取人物或动物在图像中的三维模型，然后使用这些模型来进行视频编辑、虚拟现实或机器人控制等应用。","id":"ef77273340c84032","x":3923,"y":-667,"width":459,"height":296},
		{"type":"file","file":"Flow（流）.md","id":"b5ba0c04761111e2","x":3819,"y":-2723,"width":418,"height":92},
		{"type":"file","file":"Cognitive （认知吸收, Cognitive absorption）.md","id":"b15e1dd30581dbe0","x":3819,"y":-2604,"width":418,"height":123},
		{"type":"text","text":"## 360° Depth Estimation(深度估计)\n\n360° depth estimation refers to the process of estimating the distance of objects in a 360° field of view, using sensors or algorithms. This can be used in a variety of applications, such as robotics, virtual reality, and autonomous vehicles. There are several methods for estimating depth in a 360° field of view, including stereo vision, time-of-flight sensors, and structure-from-motion algorithms. These methods can be used individually or in combination, depending on the requirements of the application and the available hardware.\n\n360°深度估计是指使用传感器或算法来估计360°视场中物体的距离的过程。这可用于各种应用，如机器人技术、虚拟现实和自动驾驶汽车。有几种方法可以估计360°视场中的深度，包括立体视觉、飞行时间传感器和从运动中获得结构的算法。这些方法可以单独或组合使用，这取决于应用的要求和可用的硬件。\n\n360°深度估计是一种图像处理技术，它可以通过分析图像中物体的深度信息来估计物体在空间中的三维位置。这种技术通常用于虚拟现实、机器人控制和图像处理应用中。 360°深度估计的方法包括基于深度图的方法、基于特征的方法、基于学习的方法和基于模型的方法。希望这些信息能帮助到你！\n\n360°深度估计可以用于很多不同的应用场景。以下是几个例子：\n\n  \n\n虚拟现实：360°深度估计可以用于计算虚拟世界中的物体的位置，从而使用户能够与虚拟物体进行交互。\n\n  \n\n机器人控制：360°深度估计可以用于控制机器人的运动，使机器人能够避开障碍物。\n\n  \n\n图像处理：360°深度估计可以用于分析图像中的深度信息，从而提高图像的质量。例如，可以使用360°深度估计来消除摄像机抖动对图像质量的影响。","id":"953718ea2bf78486","x":4408,"y":-667,"width":459,"height":296},
		{"type":"file","file":"Presence（存在）.md","id":"9513f88d46759125","x":3819,"y":-2438,"width":418,"height":130},
		{"type":"file","file":"zotero_mdnotes/@2020meynetPCQMFullReferenceQuality.md","id":"1cc301bc384e5fbb","x":4481,"y":-2773,"width":499,"height":400,"color":"1"},
		{"type":"file","file":"zotero_mdnotes/@2021quachDeepPerceptualMetric.md","id":"f32dbb7acb0603d9","x":4481,"y":-2256,"width":686,"height":526,"color":"1"},
		{"type":"text","text":"MTF（Modulation Transfer Function）：MTF是一种常用的图像清晰度度量方法，可以用来测量图像的对比度和细节表现能力。MTF值越高，图像质量越高。","id":"533b7c8028630865","x":4395,"y":-3607,"width":250,"height":60},
		{"type":"text","text":"### QoS 服务质量 (Quality of Service, )\n指的是电信服务的整体性能，包括带宽、延迟和错误率等参数。它通常用来描述网络或应用的性能，并且通常以服务提供商和用户之间的服务级别协议 (SLA) [[SLA (服务级别协议 Service Level Agreement)]] 的形式来衡量。\n### 衡量参数\nQuality of Service (QoS) 通常使用以下参数来衡量：\n\n-   带宽：衡量网络或应用能够提供的最大数据传输速率。\n-   延迟：衡量数据从发送到接收的时间。\n-   错误率：衡量数据传输中出现的错误的比率。\n\nQoS 还可以使用其他参数，如丢包率、抖动等来衡量。","id":"2ba319b76235eb92","x":4237,"y":-4412,"width":459,"height":131},
		{"type":"text","text":"### QoE 体验质量 (Quality of Experience, ) \n指的是用户使用产品或服务的整体质量。这包括客观因素，如服务的性能和可靠性，以及主观因素，如用户对服务的感知满意度。QoE 通常用来从用户的角度衡量产品或服务的效果。\n\n### 衡量参数\nQuality of Experience (QoE) 通常使用下列参数来衡量：\n\n-   客观指标：包括网络或应用的性能和可靠性。\n-   主观指标：包括用户对服务的感知满意度，这可以通过调查、问卷调查或其他方式来测量。\n\nQoE 还可以使用其他参数，如用户对服务的使用频率、用户对服务的偏好等来衡量。","id":"604137dcdaab1877","x":4237,"y":-4253,"width":459,"height":309},
		{"type":"file","file":"SLA (服务级别协议 Service Level Agreement).md","id":"3fc90a183c996142","x":4237,"y":-4779,"width":459,"height":285},
		{"type":"text","text":"# PCQM \n提出于 [[@2020meynetPCQMFullReferenceQuality]] \n\n可感知的视觉质量对于压缩算法的优化和评估\n\nPoint Cloud Quality Metric（PCQM）是一种测试模型，用于测量点云质量(measure of the quality of a point cloud)。点云是由三维坐标点构成的集合，常用来表示三维空间中的物体或场景。\n\nPCQM测试模型可以测量点云的几何质量和空间质量。\n几何质量指的是点云的几何特征，例如点的分布、密度和几何形状。\n空间质量指的是点云的空间特征，例如点之间的距离、法线和颜色。\n\nPCQM测试模型通常用于测试三维扫描设备、三维建模软件或其他三维系统的性能。它可以帮助评估点云的质量，并确定哪些系统能够提供最高质量的点云。","id":"ec0ba1c8b1f23246","x":5097,"y":-2839,"width":610,"height":181},
		{"type":"text","text":"# 优于PCQA\n优于现有的[[PCQA (point cloud quality assessment)]] 模型","id":"775f9d5f1e311914","x":5260,"y":-2288,"width":447,"height":89},
		{"type":"file","file":"PCQA (point cloud quality assessment).md","id":"ab1f09fc27d91e91","x":5260,"y":-2159,"width":447,"height":133},
		{"type":"text","text":"帧率计数器：帧率计数器是一种常用的测试视觉帧率的工具，可以用来测量图像在显示器上的刷新频率。帧率计数器可以测量每秒显示的帧数，也可以测量每一帧的显示时间。","id":"c95795c549cb0e78","x":4395,"y":-3449,"width":250,"height":54},
		{"type":"text","text":"眼动跟踪仪：眼动跟踪仪是一种常用的测试视觉帧率的工具，可以用来测量人眼在观看动态图像时的眼动轨迹。眼动\r\n跟踪仪可以通过观察眼球的位置和运动，测量人眼在观看动态图像时的帧率。","id":"69aa34328949b631","x":4395,"y":-3375,"width":250,"height":60},
		{"type":"text","text":"PNSR（Perceptual SNR）：PNSR是一种常用的图像清晰度度量方法，可以用来测量图像的对比度和细节表现能\n力。PNSR是基于人类视觉系统的特点而设计的，可以更准确地反映人眼感知的图像质量。\nPNSR值越高，图像质量越高。","id":"f9160734ff6d92af","x":4395,"y":-3528,"width":250,"height":50},
		{"type":"file","file":"HLS (HTTP Live Streaming).md","id":"56d70f5911b67913","x":5304,"y":-4605,"width":459,"height":296},
		{"type":"file","file":"DASH (Dynamic Adaptive Streaming over HTTP).md","id":"c416a88af5138e42","x":4802,"y":-4608,"width":459,"height":299},
		{"type":"text","text":"FFmpeg\n以VP9格式编码视频并将其打包成TS容器\nencoding video in the VP9 format and packaging it into a TS container","id":"1210565108581d5f","x":4802,"y":-4285,"width":947,"height":187,"color":"2"},
		{"type":"file","file":"WebRTC.md","id":"25d0c50f6c56f6fc","x":5010,"y":-3987,"width":463,"height":419},
		{"type":"text","text":"视频媒体协议DASH, HLS 通常如何生成或制作? ","id":"36f1ac4a1e298f29","x":4802,"y":-4731,"width":961,"height":78,"color":"2"},
		{"type":"text","text":"\n# An overview of ongoing point cloud compression standardization activities: video-based (V-PCC) and geometry-based (G-PCC)\n[Zotero](https://www.zotero.org/users/8931831/items/6PWXUVN8)\n摘要--由于增强现实和虚拟现实体验的日益普及，人们对多维度捕捉现实世界并以沉浸的方式将其呈现给用户的兴趣空前高涨。分布这样的表征使用户能够在多感官的三维媒体体验中自由浏览。不幸的是，这种表现形式需要大量的数据，在今天的网络上传输是不可行的。内容链中采用的高效压缩技术需求量很大，是增强型和虚拟现实应用民主化的关键组成部分。作为处理多媒体的主要标准化组织之一，移动图像专家组（MPEG）发现了这一趋势，并在最近开始建立一个紧凑地表示3D点云的开放标准，它是非常著名的2D像素的3D等效物。本文介绍了这个正在进行的标准化工作的主要发展和技术方面。\n\n\nMoving Picture Experts Group, MPEG, as one of the main standardization groups dealing with multimedia, identified the trend and started recently the process of building an open standard for compactly representing 3D point clouds, which are the 3D equivalent of the very well-known 2D pixels.  \n  \nVolumetric visual data describes a 3D scene and objects with its geometry and respective attributes , plus any temporal changes. Such data is typically computer-generated from 3D models, or is captured from real-world scenes using a variety of solutions such as.  \n  \nBecause volumetric video describes a complete 3D scene or object, such data can be visualized from any viewpoint. Therefore, volumetric video is a key enabling technology for any AR, VR, or MR applications, especially for providing Six Degrees of Freedom viewing capabilities. While MPEG in prior standards has already addressed the coding of 3D worlds , specifically computer-generated worlds, recently it launched an ambitious road map of technologies for coding representations of real 3D scenes . One of these technologies is called Point Cloud Compression and is expected to be delivered as an ISO standard in the beginning of 2020.  \n  \nSuch point cloud data presents new challenges to the signal processing and compression research community. Previous compression solutions for volumetric visual representations either focused on computer-generated content , or suffered from low spatial and temporal compression performance , when dealing with captured natural content. 3D sensor signals, scene geometry needs an efficient representation that is scalable in level of detail and efficient in compression, while its photometric attributes are a new class of signal that is not sampled on an uniform Euclidean grid and therefore needs new sampling, filtering, and transform tools to represent and compress. Many emerging applications including immersive VR/MR video, automotive/robotic navigation, and medical imaging require the capture and processing of 3D scene/object geometry data.  \n  \nThis data, in its most primitive form, consists of a collection of points called a point cloud. This section will introduce some of the aspects of point cloud data. Such polygonal meshes are well suited for compact representation of dense surfaces, but they have problems representing non-manifold structures. Key advantages of a point cloud representation over polygonal meshes are its flexibility to represent non-manifold geometry and its real-time processing potential as there is no need to store, maintain, or process surface topological information.  \n  \nFor efficient processing of point cloud data, each point is quantized into a cubic grid composed of 2−d 2−d 2−d size voxels which are formed from volumetric subdivision, up to d levels of detail , of a 1 1 1 cubic root voxel. Resulting voxels may be mapped into an octree data structure to create a voxelized octree, which facilitates, in turn, the traversal, search, and access of the neighboring voxels ,. 3D point cloud data finds applications in many fields, including cultural heritage/museums, 3D free viewpoint video, real-time immersive telepresence, content VR viewing with interactive parallax, mobile mapping, and autonomous navigation , . Regarding cultural heritage applications, point cloud data scans are used to archive and visualize objects in museums including historical statues and buildings , .  \n  \nThe goal of immersive video is to go beyond higher image quality and to provide a higher sense of 3D user experience and interactivity. Real-time 3D telepresence is one of the key applications of immersive video and 3D point clouds, for which a collection of random and unrelated points is a preferred data representation format because of its simplicity for visualization, filtering and editing. Holoportation and 8i’s volumetric video technology . Variations of immersive video include HMD based VR and 3D free viewpoint sports replay and broadcasting , which may not require real time processing and may in addition contain mesh based graphical data content.  \n  \nSuch media-related use cases may usually contain between 100,000 and 10,000,000 point locations and color attributes with 8-10 bits per color component , along with as some sort of temporal information, similar to frames in a video sequence. LIDAR, camera captured images and localization data measured with GPS and an inertial measurement unit .  \n  \n**To address this wide range of applications, the MPEG**  \n  \nThere already exist many standards to compress images, video, and LIDAR sensor data, so the objective of this emerging PCC standard is not to compress the raw sensor data, but to compress the point cloud representations of the objects or scenes captured by the sensors. The coding techniques developed here are generally designed to be agnostic of the specific sensors used to create the point cloud data, so it is assumed that prior to compression, 3D data from different sensors was fused to generate the point cloud representation to be compressed. An example of a sensor system used to dynamically acquire data for mobile mapping and autonomous navigation purposes is shown in Fig. By combining the relative LIDAR-captured point locations along with the location of the vehicle, the point locations can be converted to absolute coordinates relative to a fixed origin of a geographic coordinate system.  \n  \nFixed RGB cameras mounted on the vehicle capture image sequences or video. These data are fused in a post-capture processing operation so that each point, in addition to having a LIDAR-captured reflectance attribute, can have a single RGB color attribute associated with it. The fusing process can also clean the data, e. To capture high-resolution real-time point clouds of moving objects such as people for applications such as AR/VR/MR, Fig.  \n  \nAn example of a method for capturing voxelized point clouds using only cameras is described in . The same types of sensors described here can also be used to acquire data for generating point clouds of static objects such as buildings and their interiors, objects and assemblies for industrial and cultural heritage applications, and terrain features. By fusing data from aerial images and LIDAR scans along with ground based LIDAR and imaging data, point cloud models of cities can be generated, as demonstrated in . Capturing point clouds of cultural and historical objects or archeological sites can also be done with these kinds of sensors.  \n  \nA high-level overview of various methods for acquiring point cloud representations of cultural objects can be found in . There has been plenty of work on point cloud compression in the past, but most works aim only at the compression of static point clouds, instead of time-varying point clouds as needed for AR/VR/MR applications. For example, a point cloud codec was introduced in based on octree composition. This method could operate in real time, as the XOR prediction is simple and fast.  \n  \nIn , an extension to this framework was introduced, combining the octree-based codec with a common image codec for color attribute coding. Introduced a time-varying point cloud codec that can predict graph-encoded octree structures between adjacent frames. The method uses spectral wavelet-based features to achieve this and an encoding of differences to achieve a lossless encoding. This method also includes the color coding method from , which defines small subgraphs based on the octree of the point cloud.\n\nIn comparison to point clouds, 3D objects are often coded as 3D meshes, for which a significant number of compression methods were developed. Early work on mesh compression includes , , . While these methods are promising, it seems that methods based on 3D point clouds can result in coding with even less overhead and more flexible progressive rendering capabilities, as the point cloud format is simpler to acquire and process. There are international standards for mesh compression , defined, which are greatly beneficial for interoperability between devices and services.  \n  \nStatic dynamic augmented reality. Therefore, these formats are not directly applicable to immersive and augmented 3D object-based video combining real and virtual content. These applications typically deal with photo-realistic meshes and point clouds of millions of points acquired from 3D scanners and/or computer vision algorithms.  \n  \n**3D meshes and point cloud content were contributed, and a practical streaming prototype was developed as part of the**  \n  \nThe block diagram of the data flow in immersive communications is shown in Fig. In such systems real-time communication processing is important as is the resilience to noisy data and handling of dense point clouds. The advantage of this approach is that composite rendering in scenes facilitates AR, VR, and free view point functionalities. However, for this use case, existing MPEG standards for 3D graphics were found to be less suitable due to the fact that several requirements like noise resilience and low encoder latency were not fully addressed.  \n  \nDuring the CfP development period, evaluation metrics were developed in a series of experiments, requirements, and use case assessments. To have a baseline for determining target bitrates and distortions, a recent hybrid octree-image point cloud codec for tele-immersive video was chosen as anchor. Quality metrics described in , were selected for the objective quality assessment. These metrics are referred as point-to-point and point to plane geometry distortion metrics.  \n  \nIn the first geometry metric , the comparison is such that the Mean Square Error between the reconstructed point and the closest corresponding points in the reference point cloud is calculated. In the second geometry metric , the MSE is calculated between the reconstructed point and the surface plane in the given reference test data. Surface normals are provided with the reference test data to facilitate the computation of the surface planes. Peak signal-tonoise-ratios are obtained based on the 3D volume resolution for geometry, and respectively for color depths for each color channel.  \n  \nBjontegaard-delta metrics are derived comparing the distortions against the anchor implementation at predefined target bitrates . The results of the subjective assessment were almost in line with the objective evaluations . The average confidence interval for the subjective evaluation of static content was 0.48 MOS values, while that of dynamic content was 0.34 MOS values. Interpolationbased prediction occupancy symbols reconstructed residuals Inv.  \n  \n  \n  \nMBit/s, representing 0.2% to 5% of the original uncompressed data. In addition to using objective metrics, a subjective evaluation methodology was defined that consisted of rendering the point clouds using a virtual camera path and then performing the quality assessment via techniques similar to those used to evaluate video quality. For the subjective assessment only three static objects and three dynamic scenes were considered among the total of 30 test objects considered in the overall CfP. The entire set of 19 static objects, five dynamic objects and six dynamic acquisition scenes were considered for the objective evaluation.  \n  \nThis reduced subjective test set was due to the need to minimize the effort required to complete the tests and because dynamic acquisition scenes are typically processed by a computer and are not directly viewed as a final product. The subjective visual quality assessment of the static and dynamic scenes was made possible by using a point cloud renderer designed by Technicolor . This software allows specifying a camera view path, displaying a static representation of a point cloud, rotating it on three axes, and zooming. When rotating and zooming a static object, it is possible to record a track of all the movements.  \n  \nThe recorded tracks are used to create video clips. The same process is applied on dynamic sequences, where the video clips were produced by rotating the object while playing it out.  \n  \n**The L-PCC codec was designed to efficiently compress**  \n  \nBecause of such characteristics, L-PCC compresses first the point cloud geometry information by exploiting an octree-based encoding strategy. Section V-A describes the octree-based coding process for the geometry information.  \n  \nFinally, an interpolation-based prediction module that is used to further improve the coding efficiency of the attribute values by exploiting spatial correlations as well as the quantization and dequantization steps that are applied on the residuals are described in Section V-D. Generating octree structure by recursive subdivision. Let Xi = i=1...N be the set of 3D positions associated with the points of the input point cloud.  \n  \n**X̃i =**  \n  \nAfter quantization, an optional process that removes duplicate points may be applied. It consists of merging points sharing the same quantized positions into a single point.  \n  \n**An octree structure is then built by recursively subdividing**  \n  \nOn the decoder side, the decoding process starts by reading from the bitstream the bounding box B. The same octree structure is then built by subdividing B according to the subdivision codes read from the bitstream.\n\n**However, a practical encoder implementation could be designed by considering different computational complexity and**  \n  \nThe S-PCC architecture comprises an encoder and decoder, which in turn comprise various modules, as shown in Fig. Communication between modules is done by passing lists of point locations and/or point attributes. The input to the S-PCC encoder is the original, uncompressed point cloud.  \n  \n**Upsample original**  \n  \nBlock diagram of the S-PCC geometry encoder and decoder. Furthermore, the original point colors are typically expressed in an RGB color space.  \n  \n**The transformation from world to frame coordinates may be specified by using the parameters translation =**  \n  \nSpecifically, the locations of all points within a voxel are quantized to the voxel center, and the attributes of all points within the voxel are averaged and assigned to the voxel. A voxel is said to be occupied if it contains any point of the point cloud. The blockwidth is constant, W = 2depth−` , where ` = level is a parameter to the encoder and is passed to the decoder in the bitstream header. The use of blocks to represent geometry is important for spatial random access, view-dependent coding and rendering, parallel processing, out-of-core processing for large datasets, and the formation of \"slices\" and other units for network packetization and error resilience.  \n  \nEntropy encoding of blocks. Currently, the occupancy bytes are entropy-coded. If the depth of the tree is large enough, then there is at most one point in each voxel, and thus the geometry of the original point cloud can be represented losslessly, up to depth bits of precision for each spatial component.  \n  \n**If ` = level is less than depth, then the blocks are**  \n  \nS-PCC represents the geometry within each block as a surface that intersects each edge of the block at most once. The collection of vertices is a list , k = 1, . Although there are other surfaces that can be parametrized by such a set of vertices, for example, as an implicit surface of a Bézier Volume, S-PCC uses triangles as they are particularly friendly to standard graphics processing. Entropy encoding of vertices.  \n  \nVertices, nominally being intersections of a surface with edges of a block, are shared across neighboring blocks, not only guaranteeing continuity across blocks of the reconstructed surface, but also reducing the number of bits required to code the collection of vertices. The set of vertices is coded in two blocks is computed, and a bit vector determines which edges contain a vertex and which do not.  \n  \nEntropy encoding. The quantized, transformed coefficients are entropy-encoded using RLGR . The output from the S-PCC encoder and the input to the decoder is a bitstream that comprises a geometry bitstream, a color bitstream, and a bitstream header. The bitstream header contains parameters needed to decode the geometry and color bitstreams, namely depth, level, geomStepsize, colorStepsize, translation, and scale.  \n  \nBlock diagram of S-PCC color encoder and decoder. The S-PCC encoder contains an instantiation of the geometry decoder. Details of the geometry decoder module are described in Section VI-C. The output of the geometry decoder is a list of refined vertices , r = 1, .  \n  \n, Nref , of the re-colored points, using information from the already-available locations , r = 1, . All the refined vertices within a voxel are quantized to the voxel center, and the attributes of the refined vertices within the voxel are averaged and assigned to the voxel. This produces a list of voxel colors, n = 1, . , Nvox , along with a list of the associated voxel locations , n = 1, .  \n  \n, Nvox , are transform-coded, analogously to a color image, by a spatial transform, quantizer, and entropy coder. Details of the geometry decoder are shown in Fig. The occupancy bytes of the octree are entropy-decoded and the octree is reconstructed. If the level of the octree is equal to the depth as indicated in the bitstream header, then this is a lossless representation of the geometry at that level of precision.  \n  \nEntropy decoding of vertices. If the level of the octree is smaller than the depth, then this is a lossy representation of the geometry, and vertices are entropy-decoded, in two blocks is computed, and a bit vector is entropy-decoded to indicate which edges contain a vertex and which do not. Secondly, for each edge that contains a vertex, the position of the vertex along the edge is entropy-decoded and dequantized, resulting in a list of vertices , k = 1, . Vertex reconstruction.  \n  \nFor each block, the vertices on the block edges determine a surface through the block. The method of triangulation is defined so that the triangulation is unique given the vertices on the block edges.\n\nThe purpose of the refined vertices is to create geometry at a spatial resolution greater than or equal to the spatial resolution of the color information. The list of these refined vertices is the output of the geometry decoder.  \n  \n**The color decoder module decompresses the color bitstream into decoded color components , n =**  \n  \n, Nref , as side information. The main philosophy behind V-PCC is to leverage existing video codecs for compressing the geometry and texture information of a dynamic point cloud. , an occupancy map and auxiliary patch information, are also generated and compressed separately. It should be noted that the metadata information represents a relatively small amount of the overall bitstream.  \n  \nThe bulk of the information is handled by the video codec. Subsection VII-B details the image generation and padding processes, which transform the point cloud geometry and texture information into temporally correlated, piecewise smooth, 2D images suited for coding using traditional video codecs. The processes of generating the auxiliary patch information and occupancy map are described in subsections VII-C and VII-D, respectively. Subsection VII-E describes the smoothing module and the geometry and texture reconstruction processes.  \n  \n**Packing smoothed**  \n  \n, Nvox . The inverse transform uses, as side information, the list of decoded voxel locations , n = 1, .  \n  \nLeveraging traditional video codecs to encode point clouds requires mapping the input point cloud to a regular 2D grid. The objective is to find a temporally-coherent low-distortion injective mapping that would assign each point of the 3D point cloud to a cell of the 2D grid. Maximizing the temporal coherency and minimizing the distance/angle distortions enables the video encoder to take full advantage of the temporal and spatial correlations of the point cloud geometry and attributes signals. An injective mapping guarantees that all the input points are captured by the geometry and attributes images and could be reconstructed without loss.  \n  \n**Iterative refinement procedure refined final patches**  \n  \nThe padding process aims at filling the empty space between patches in an attempt to generate a piecewise smooth image that may be better suited for video coding. V-PCC uses a simple padding strategy, which processes each block of T T pixels independently. If the block has both empty and filled pixels, then the empty pixels are iteratively filled with the average value of their non-empty neighbors. Example of geometry and texture images.  \n  \nThe packing process aims at mapping the extracted patches onto a 2D grid, while trying to minimize the unused space and to guarantee that every T T block of the grid is associated with a unique patch. V-PCC uses a simple packing strategy that iteratively tries to insert patches into a W H grid. W and H are user defined parameters, which correspond to the resolution of the geometry/texture images that will be encoded. The patch location is determined through an exhaustive search that is performed in raster scan order.  \n  \nThe first location that can guarantee an overlapping-free insertion of the patch is selected and the grid cells covered by the patch are marked as used. If no empty space in the current resolution image can fit a patch then the height H of the grid is temporarily doubled and the search is performed again. At the end of the process, H is reduced so as to account only for the used grid cells. The image generation process exploits the 3D to 2D mapping computed during the packing process to store the geometry and texture of the point cloud as images.  \n  \n14 shows an example of generated geometry and texture images.  \n  \n**In order for the decoder to be able to reconstruct the**  \n  \nThe patch metadata is predicted and arithmetically encoded. Instead of explicitly encoding the index I, its position J is arithmetically encoded. This can lead to better coding efficiency. The occupancy map consists of a binary map that indicates for each cell of the grid whether it belongs to the empty space or to the point cloud.  \n  \nThe occupancy map compression leverages the auxiliary information described in the previous subsection, in order to detect the empty T T blocks . The remaining blocks are encoded using the following process.  \n  \n**The occupancy map could be encoded with a precision of**  \n  \nB0 = 2 or B0 = 4 result in visually acceptable results, while significantly reducing the number of bits required to encode the occupancy map.  \n  \n**S ELECTION**  \n  \nThe occupancy map compression module first associates binary values with all B0 B0 sub-blocks belonging to the same T T block. If the block is non-full, additional information indicating the location of the full/empty sub-blocks is encoded by using the following strategy. The smoothing procedure aims at alleviating potential discontinuities that may arise at the patch boundaries due to compression artifacts. The point cloud geometry reconstruction process exploits the occupancy map information in order to detect the non-empty pixels in the geometry/texture images/layers.  \n  \nThe 3D positions of the points associated with those pixels are computed by leveraging the auxiliary block/patch information and the geometry images. More precisely, let P be the point associated with the pixel, let be the 3D location of the patch to which it belongs, and let be its 2D bounding box. OF OBJECTIVE EVALUATION RESULTS.  \n  \n  \n  \nThe full sets of objective and subjective results, including RD-curves, are available in and , where S-PCC is denoted as \"P02\" and V-PCC as \"P07\". Only one solution was submitted as proposal for LIDAR point cloud compression. No subjective evaluation was carried out for L-PCC. Out of these, the proposal described in Section VI scored the highest in the objective and subjective evaluations.  \n  \nBDBR, and 15% Luma BDBR bit savings were achieved, compared to the anchor. The subjective evaluation showed conclusive results, as seen in the example shown in Fig.\n\nThe benefits of V-PCC over the anchor in terms of visual quality are clearly visible and in line with the objective evaluation results, e. as where g is the luma component of the geometry image.  \n  \nSteinbach, \"Real-time compression of point cloud streams,\" in Int. Conf. Mekuria, K. Chou, \"Motion-compensated compression of dynamic voxelized point clouds,\" IEEE Trans. Mekuria, Z. Mekuria, C. Zhang, \"Real-time high-resolution sparse voxelization with application to image-based modeling,\" in Proceedings of the 5th High-Performance Graphics Conference, 2013.  \n  \nGopi, \"A generic scheme for progressive point cloud coding,\" IEEE Trans. Computer Graphics, vol. Proceedings of the 3rd Eurographics / IEEE VGTC Conference on PointBased Graphics, ser. Frossard, \"Graph-based compression of dynamic 3D point cloud sequences,\" IEEE Trans.  \n  \nRossignac, \"Geometric compression through topological surgery,\" ACM Trans. Mekuria, P. Cesar, and D. Bulterman, \"Low complexity connectivity driven dynamic geometry compression for 3D tele-immersion,\" in Int. Cesar, \"A basic geometry driven mesh coding scheme with surface simplification for 3DTI,\" in IEEE COMSOC MMTC ELetter, vol. Daras, \"On human time-varying mesh compression exploiting activity-related characteristics,\" in Int. Magnenat-Thalmann, \"A novel compression framework for 3D time-varying meshes,\" in Int.  \n  \nMekuria, M. Mekuria, P. Cesar, and D. Conf. Mekuria, S. International Telecommunication Union, \"Subjective video quality assessment methods for multimedia applications,\" Rec. Cesar, E. Chou, \"Compression of 3D point clouds using a region-adaptive hierarchical transform,\" IEEE Trans. Malvar, \"Adaptive run-length/Golomb-Rice encoding of quantized generalized gaussian sources with unknown statistics,\" in Data Compression Conference , 2006.  \n  \nResearch Leader of the Volumetric Video Coding team at Nokia Technologies in Tampere, Finland.  \n  \n**Before joining Nokia, he held a Marie SkłodowskaCurie fellowship as Experienced Researcher with**  \n  \nHe is currently an editor of the ISO/IEC committee draft for video-based point cloud compression and co-chair of the MPEG Ad hoc group on System Technologies for V-PCC.  \n  \nHe leads a research team with focus on Augmented Reality, Cloud Computing, Games and Interactive Media and regularly presents results in journals and at speaking engagements worldwide.  \n  \n**Vittorio Baroncini received his Bacca Laurea in**  \n  \nSince 2004, he is Chairman of MPEG Test Group. Expert in objective and subjective video quality assessment and is currently acting as consultant in the area of visual quality assessment. Maja Krivokuća received her Bachelor of Engineering degree in Computer Systems Engineering and her Ph.  \n  \nHe is cochair of the MPEG Ad hoc group on Point Cloud Coding.  \n  \nFrom 2004 to 2006, he was Monbusho Research fellow at the Institute for Laser Engineering, Osaka, Japan. Research Scientist at Canon Research Centre France.\n\n**Pablo Cesar leads the Distributed and Interactive**  \n  \nTransactions on Multimedia and IEEE Transactions of Multimedia.  \n  \nEngineering from Northwestern University in 2004. Staff Researcher/Sr. Manager with Samsung Research America’s Multimedia Standards Research. His research interests include point cloud and light field compression, graph signal processing and deep learning.  \n  \n**Bell Laboratories, the Xerox Palo Alto Research**  \n  \nWashington, the Chinese University of Hong Kong, Joan Llach received his M. Technicolor R&I France, where he leads a research team working on compression, transmission and interactive applications for video. Llach has actively participated in several standardization efforts and is an editor of the ISO/IEC committee draft for video-based point cloud compression . Group Committee since 2005, especially focusing on 3D graphics compression. 3D mesh compression.  \n  \nCompression Ah-Hoc Group and editor of the ISO/IEC committee draft for video-based point cloud compression and the working draft for geometry-based point cloud compression .  \n  \n**Rufael Mekuria received his MSc in 2011 from**  \n  \nin 2017 from VU University.  \n  \n**Informatica from 2011 to 2016 and joined Unified**  \n  \nStreaming/CodeShop in 2016, where he is leading standardisation and research efforts.  \n  \n**Point Cloud Coding Ad Hoc group in 2014 and continues work on PCC at Point Cloud Compression**  \n  \nHe has been with Sony Corporation, Tokyo, Japan, since 2004. Coding Experts Group and ISO/IEC Moving Pictures Experts Group since 2011. Multimedia Computing Group of Delft University of Technology, The Netherlands, in 2018.  \n  \n**Ali Tabatabai is currently a consultant and technical advisor to Sony US Research Center and Sony**  \n  \nTokyo R&D Center. In his last position as a VP at Sony US Research Center he was responsible for research activities related to VR/AR capture and next generation video compression. Tourapis is a Software Standards Engineer with Apple Inc. and represents Apple in several standardization activities including MPEG, ITU, and SMPTE. Labs, and Dolby Laboratories, among others, and has made several key contributions to standards such as MPEG-4 AVC and HEVC.","id":"b1bfa8c0007986a8","x":5021,"y":-5093,"width":686,"height":238,"color":"1"},
		{"id":"3678932e07f2aee5","type":"file","file":"Emerging MPEG Standards for\nPoint Cloud Compression.md","x":5021,"y":-5375,"width":686,"height":223,"color":"1"},
		{"type":"text","text":"### (What) 此前对该主题进行了哪些研究, 从事该研究已具备的基础, 所选择课题国内外研究现状? ","id":"0441dc1f09ffaf9d","x":-7622,"y":-4258,"width":460,"height":138},
		{"type":"file","file":"2.Literature review (VPCCRP).md","id":"db79a0f70c6add5a","x":-8044,"y":-4258,"width":380,"height":321,"color":"5"},
		{"type":"text","text":"### 为什么你是做这项研究的合适人选","id":"eb952a00baa2c289","x":-7622,"y":-4097,"width":460,"height":85},
		{"type":"text","text":"Microsoft, Facebook, Google 等重要机构在体积媒体研究领域的贡献；","id":"e7828b1db845a078","x":-7622,"y":-4006,"width":460,"height":69},
		{"type":"text","text":"### 基于V-PCC的艺术教育应用分析\n\n","id":"916cb9fc16978ad4","x":-7559,"y":-3469,"width":460,"height":597},
		{"type":"file","file":"3.Design and Methods (VPCCRP).md","id":"019b10df04e20130","x":-8044,"y":-3469,"width":380,"height":647,"color":"5"},
		{"type":"text","text":"### 为完成研究所拟定的研究计划","id":"c6383a93230a542d","x":-7559,"y":-2872,"width":460,"height":50},
		{"type":"text","text":"\n\nVideo Point Cloud 数据中空间信息的重要性. 例如，如果一个人有关于某个地点的负面记忆，那么他或她在后来再次来到该地点时可能会有负面的感知。\n\n\nMultidimensional Digital Memory（多维数字记忆体）\nMultimedia Hyperdimensional Archive（MHDA）是由Video Point Cloud 构成的媒体格式，它是一种比图片、文字、视频等传统数字记忆媒体相比, 它储存更高维度的数字记忆。 Video Point Cloud将声音、颜色、形状、空间和时间多维度的特征结合在一起，形成一个全方位、丰富的多媒体空间，让用户更好地感知，记忆与想象事件或经历。\n\n总之，3D视频有可能彻底改变我们与数字媒体互动的方式，以及我们保存人类文明的方式。它捕捉和存储高维数字记忆的能力提供了一个新的沉浸感和感知水平，使用户能够更好地记忆和想象事件或经历。随着我们的社会更加重视非物质遗产而不是物质遗产，3D视频可以被认为是保存人类历史的多维度超级档案。\n\n总的来说，本文旨在探索3D视频作为一种立体数字记忆形式的潜力，以及它如何改变我们目前对数字互动的理解和对保存人类文明的影响。\n","id":"721944473d22b365","x":-7559,"y":-2602,"width":676,"height":550},
		{"type":"text","text":"对知识的贡献, 明确你的研究的重要性","id":"17e6ba9456f249f3","x":-7559,"y":-2728,"width":460,"height":87},
		{"type":"file","file":"4.Contribution to knowledge (VPCCRP).md","id":"e9b272eb29e9451d","x":-8044,"y":-2684,"width":380,"height":598,"color":"5"},
		{"type":"text","text":"本文的主要贡献是提出了一种提高三维点云视频保持定位精度的压缩方法，优点是可确保3D云点在传播后的视觉感知体验, 沉浸感，以及互动性。我们的方法着重于提高3D点云视频的压缩效率，从而降低带宽成本，扩大3D点云视频的潜在应用。此外，我们通过一个有意义的案例评估压缩方法的有效性，并证明它与现有方法相比的优越性能。有望为未来的时间序列压缩技术优化研究提供了一个基准。此外，本文还强调了视频点云数据中空间信息的重要性，以及3D视频在彻底改变我们与数字媒体的互动方式和保护人类文明方面的潜力。","id":"55e1646baa69d9ed","x":-7559,"y":-2014,"width":676,"height":290,"color":"4"},
		{"type":"text","text":"#  \"Exploring the Potential of Video Point Cloud as a Multidimensional Digital Memory\"\n# “探索视频点云作为多维数字记忆的潜力”","id":"1ff88517baa2f9d6","x":-9359,"y":-4145,"width":791,"height":174},
		{"type":"text","text":"(个人的立体动态记忆), 保存和分享更立体与真实的技艺体验","id":"dc95086c0db8546f","x":-7760,"y":-4689,"width":278,"height":62},
		{"type":"text","text":"储存","id":"458e835b1b684c30","x":-8020,"y":-4687,"width":250,"height":120},
		{"type":"text","text":"(历史动态记忆)\n记录和储存非物质遗产档案","id":"1184b74527044857","x":-7760,"y":-4597,"width":324,"height":63},
		{"type":"text","text":"传输","id":"8e6c01df99f7b15a","x":-8020,"y":-4464,"width":250,"height":60},
		{"type":"text","text":"# Volumetric Media","id":"3f6ead12da675bbf","x":-1424,"y":-3981,"width":311,"height":74},
		{"type":"text","text":"# 三维数据表示方法 \n## Point Cloud(云点)\n点云是三维数据表示方法. \n常用于三维计算机视觉和几何处理的数据结构。\n\n## 3D mesh(三维网格):\n\n三维网格是由许多三角形构成的三维模型。三维网格可以用于表示物体表面的几何信息，也可以用于表示体媒体中的密度信息。\n\n## volume data(体数据):\n\n体数据是三维空间中的格点数据，可以用于表示体媒体中的密度信息。体数据通常使用网格或者立方体来表示，每个格点都有一个密度值。\n\n## normal field(法线场):\n\n法线场是三维空间中的矢量场，表示物体表面的法线信息。法线场可以用于表示物体表面的凹凸程度，也可以用于表示光照信息。\n\n## point cloud fractal(点云分形):\n\n点云分形是一种使用分形算法表示三维空间信息的方法。点云分形可以用于表示复杂的几何形状，并且可以通过调整参数来控制细节程度。","id":"96ba050e6dd2bf45","x":-1017,"y":-4139,"width":477,"height":413},
		{"type":"text","text":"重申主要目标, 将焦点带回您自己的项目","id":"fb923cc96128e012","x":-3508,"y":-3668,"width":328,"height":50},
		{"type":"text","text":"试验: 探索一种云点压缩算法(Cloud point compression algorithms) \n在采集获得的云点数据中, \n- [[基于投影的编码原则 (Projection-based coding principle) ]]\n","id":"343305c491b89925","x":-3508,"y":-3593,"width":328,"height":387},
		{"type":"text","text":"编码压缩","id":"e0a9354454546aa6","x":-3508,"y":-3130,"width":328,"height":95},
		{"type":"text","text":"比对方法: 比较同一数据的无损和有损压缩版本的质量","id":"936932a5ec20142e","x":-2884,"y":-3543,"width":250,"height":67},
		{"type":"text","text":"云点质量 ","id":"0bd8c1ed85681e5c","x":-2884,"y":-3618,"width":1245,"height":50,"color":"3"},
		{"type":"file","file":"Rate Distortion Curve.md","id":"657fc45819eea2a7","x":-2303,"y":-3497,"width":288,"height":138},
		{"type":"text","text":"位置信息失真度","id":"606329456a986e57","x":-2884,"y":-3458,"width":250,"height":60},
		{"type":"text","text":"评估方法 \n比较点云的分布(**distributions**)。这涉及到比较每个点云中的点的分布，例如通过查看点坐标的方差或偏度。\n~~比较点云的密度(**densities**)。计算每个点云中每单位体积的点的数量并比较密度。~~\n~~比较点云的形状(**shapes**)。这涉及到比较点云的整体形状，例如，通过对点进行表面拟合并比较所产生的形状。~~\n~~比较点云的细节(**details**)。这涉及到比较点云的细节，例如通过查看点的局部结构或点之间的差异。~~","id":"4d9a3086e0730bb3","x":-2884,"y":-3382,"width":535,"height":280},
		{"type":"file","file":"PSNR (peak signal-to-noise ratio).md","id":"b7ea4a4a4622fe6c","x":-2303,"y":-3331,"width":288,"height":104},
		{"type":"file","file":"SSIM（Structural Similarity Index).md","id":"b07633e03cc614f5","x":-2303,"y":-3179,"width":288,"height":77},
		{"type":"file","file":"Video compression codec (视频压缩编解码器).md","id":"78fede389c0f4862","x":-2884,"y":-3051,"width":444,"height":170},
		{"type":"text","text":"彩色压缩造成的 [[Compression Artifacts (压缩失真)]]","id":"06f90c6f5f314ef3","x":-2392,"y":-2987,"width":467,"height":50},
		{"type":"file","file":"Video Container formats(容器格式).md","id":"5d155deead9d5981","x":-2884,"y":-2843,"width":444,"height":104},
		{"type":"text","text":"网络效率评估","id":"6281c3fcbcb550fa","x":-2884,"y":-2690,"width":1245,"height":50,"color":"3"},
		{"type":"file","file":"媒体流协议.md","id":"f0e06beeffdbfc05","x":-2884,"y":-2575,"width":400,"height":219},
		{"type":"text","text":"## Compression Efficiency (压缩效率)\n\n相关论文 --`|Compression Efficiency - an overview | ScienceDirect Topics` [sciencedirect](https://www.sciencedirect.com/topics/computer-science/compression-efficiency)\n","id":"c155b5bbc378d199","x":-1961,"y":-3485,"width":331,"height":383},
		{"type":"text","text":"# PCQM \n提出于 [[@2020meynetPCQMFullReferenceQuality]] \n\n可感知的视觉质量对于压缩算法的优化和评估\n\nPoint Cloud Quality Metric（PCQM）是一种测试模型，用于测量点云质量(measure of the quality of a point cloud)。点云是由三维坐标点构成的集合，常用来表示三维空间中的物体或场景。\n\nPCQM测试模型可以测量点云的几何质量和空间质量。\n几何质量指的是点云的几何特征，例如点的分布、密度和几何形状。\n空间质量指的是点云的空间特征，例如点之间的距离、法线和颜色。\n\nPCQM测试模型通常用于测试三维扫描设备、三维建模软件或其他三维系统的性能。它可以帮助评估点云的质量，并确定哪些系统能够提供最高质量的点云。","id":"0ebdfef6f05e67f4","x":-1850,"y":-2994,"width":279,"height":57},
		{"type":"text","text":"视觉感知指标","id":"8285b63d0a3fa09e","x":-1850,"y":-2918,"width":279,"height":77},
		{"type":"text","text":"主观评价\n\n可能涉及到让人类评估人员查看或聆听数据，并在一个量表上对其质量进行评分","id":"45813df7855fe9e7","x":-1563,"y":-3143,"width":139,"height":253},
		{"type":"text","text":"客观质量指标","id":"f0b0a0e75a5cdef9","x":-1563,"y":-3458,"width":139,"height":261},
		{"type":"text","text":"编码算法 ","id":"7714effce3d46d1b","x":-2884,"y":-3832,"width":1245,"height":50,"color":"3"},
		{"type":"text","text":"目标","id":"632b3c392154250e","x":-2884,"y":-3747,"width":250,"height":50},
		{"type":"text","text":"用8 bit 的图形容器储存 13 bit 的数据","id":"51c587678ef9d684","x":-2634,"y":-3747,"width":313,"height":50},
		{"type":"text","text":"## 线上教学平台的开发\n以下将利用新的V-PCC 算法, 以及AR技术开发一个在线数字雕塑课程平台: \n\n1.  目标受众：美院雕塑系二年级学生, 约60人。\n2.  产品技术：待定，Three.js 开发网页端平台 或使用 Unity 开发手机AR 应用。\n3.  课程计划：课程内容为一节肖像泥塑课程, 让学生理解头骨空间特征.\n4.  课程资料：包括一个教师进行泥塑示范(由使用多台 Kinect 拍摄的云点影像), 一个数字模特, 以及在特定空间设置答题卡等可交互立体元素。\n6. 在线平台： 将搭建服务器对媒体流储存或发送, 设计服务器中流媒体分段加载及客户端缓冲机制, 确保端对端低延迟的传送. 如直播场景中将对直播协议进一步研究, 可能采用 WebRTC 建立两个客户端浏览器之间点对点（Peer-to-Peer）的连接. \n","id":"a8084d40c4e0c591","x":-5649,"y":-3177,"width":521,"height":460},
		{"type":"text","text":"# 压缩编码的应用分析\n\n实验旨在对V-PCC（基于视频的点云压缩）媒体进行实际应用，以了解其在不同用例中的有效性。一方面，该研究旨在评估V-PCC在不同场景下的压缩效率。另一方面，将开发一个学习环境来评估V-PCC在艺术教育领域的影响。\n\n本实验将从两个环节展开: \n1) 线上教学平台的开发\n2) 线上教学场景的评估\n\n","id":"11bdcb34b6b608cd","x":-6145,"y":-3126,"width":426,"height":430},
		{"type":"text","text":"利用新的V-PCC算法和增强现实（AR）技术，开发一个在线教学平台的举措。该平台针对的是一所艺术学院的二年级雕塑学生，估计有60名学生入学。\n\n目前正在考虑开发该平台所采用的技术，潜在的选择包括利用Three.js来创建一个基于网络的平台，或整合Unity来开发一个移动AR应用程序。\n\n该平台的课程设计是提供一个线上AR 雕塑课，重点是向学生传授对头骨解剖的空间特征的理解。课程材料将包括使用多个Kinect摄像头拍摄的雕塑教师进行课程的视频演示，以及一个数字模型和一些可互动的三维元素放置在特定空间位置的测验卡。\n\n为了确保V-PCC内容的无缝和高效传输，将建立一个服务器来存储和传输媒体流。媒体分段加载和客户端缓冲机制的实施将保证低延迟的端到端传输。在流媒体直播的情况下，可能会对流媒体直播协议进行进一步研究，有可能实施WebRTC来建立客户端浏览器之间的点对点连接。","id":"f0be9db1f621faa4","x":-4999,"y":-3126,"width":521,"height":409,"color":"4"},
		{"type":"text","text":"## 线上评估控制台\n\n我们先对参与者进行调研，一方面了解他们的学习需求和期望，调整课程内容和UX设计, 另一方面有助于我们设计有效评估指标. \n与网页分析平台相似, 在教学场景各个重要对象设置Hook 事件,  以识别用户行为, 通过用户行为进行以下两项评估任务: \n1. 关于Video Point Cloud 的性能评估报告\n2. 关于Video Point Cloud 的影响评估报告\n\n关于Video Point Cloud 的性能评估报告(technical evaluation study)：\n\n虽然目前这项工作还未开始, 但我们会特别关注数字场景中\n用户到达特定地点花费的时间以及用户与特定对象互动以及对3D空间和其中物体的理解程度极交互次数将能很好地反映 V-PCC 信息与色彩信息的失真,  点云的分布(distributions)偏度, 压缩效率(compression efficiency).\n\n关于Video Point Cloud 的影响评估报告 (impact assessment study): \n跟踪课程进度, 收集行为反馈来获得例如: 参与度(engagement),  回访率 (Retention), 跳出率(Bounce Rate),学生成绩(outcomes) 等指标, \n\n以进一步获得研究结论 并验证一些假设: \n例如我们假设\n视频点云的空间信息和AR独特的空间引导能力的结合有可能提高学生的参与度(例如我们可能可以看到他们在教学过程中开始主动寻找的最佳视角。)，因为他们可能在教学过程中开始主动寻找最佳视角。\n我们假设\n视频点云作为一种立体数字记忆的形式，可以更轻松唤醒学生的感知、记忆和想象力，使抽象概念的展示更加有效，帮助学生掌握复杂的技能。因此，它可以提供一个更有吸引力和互动的学习体验。","id":"f3d598a5d7242c7e","x":-5649,"y":-2684,"width":521,"height":430},
		{"type":"text","text":"提出的系统是一个在线评估控制台，旨在收集关于参与者的学习需求和期望的洞察力，以便调整课程内容和用户体验设计(UX design)。该系统旨在通过在教学场景中的各种关键对象上设置 \"钩子事件 \"来识别用户行为，类似于网络分析平台。\n这使得该系统能够执行两项评价任务。\n\n一个技术评估研究(A technical evaluation study)，重点是性能指标，如用户的参与和与特定对象的互动，到达特定位置所花费的时间，以及对三维空间和场景中的物体的理解，这有助于我们反映V-PCC信息和颜色信息失真，点云分布倾斜，压缩效率。\n\n一个影响评估研究(An impact assessment study)，跟踪课程进度，收集参与度、保留率、跳出率、学生成绩等行为反馈，获得研究结论，验证假设，如通过视频点云的空间信息和AR独特的空间引导能力的结合，可能提高参与度。","id":"526d69c85685773a","x":-5002,"y":-2684,"width":521,"height":430,"color":"4"},
		{"type":"text","text":"This study aims to contribute to knowledge in the field of 3D video as a form of volumetric digital memory, and its potential impact on the way we interact with digital media and preserve human civilization.\n\nFirstly, we examine the potential of 3D video to change the way we interact with digital media. The introduction of 3D video recording in everyday life scenarios offers a new level of immersion and perception, allowing users to better remember and imagine events or experiences. Video Point Clouds, which combine sound, color, shape, space, and time into a multi-dimensional feature, create a immersive multimedia space, allowing users to better perceive, remember and imagine events or experiences.\n\nAdditionally, we explore the potential of 3D video in preserving human civilization. Currently, most museum 3D archives are mostly static data, with few non-material heritage archives. Due to the ability to capture and store high-dimensional digital memory, 3D video provides a new way to preserve non-material heritage and human history. As our society places more emphasis on non-material heritage rather than material heritage, 3D video can be considered a multi-dimensional super archive.\n\nIn the field of education, 3D video as a volumetric digital memory can easily evoke students' perception, memory, and imagination, and as educators, can more effectively present information to students.\n\nMoreover, we proposed a new method to improve the accuracy of 3D point cloud video positioning information, while optimizing visual perception experience, enhancing the immersive experience of videos and interactivity. Our method focuses on improving the compression efficiency of 3D point cloud video, thus reducing bandwidth costs and expanding the potential applications of 3D cloud point video. Finally, we evaluated our method through extensive experiments and demonstrated its superior performance compared to existing methods.","id":"6d19faf147bf5ae3","x":-5996,"y":-1975,"width":676,"height":290},
		{"type":"text","text":"### 应用场景\n\n- 对于个人, \n\t- 保存和分享更立体与真实的经历.\n\t- 以教育领域为主题, 策划一次 AR 线上雕塑课程, 参与者约60人, 根据以上感知指数进行用户调研.  \n- 对于文化以博物馆为例: \n\t- 可以记录和储存非物质遗产档案\n\t- 结合AR 能将历史和今天的事件相连接。发挥着重要作用。","id":"652c8cdf0a4d22c3","x":-6145,"y":-2641,"width":426,"height":332},
		{"type":"text","text":"3D视频领域近年来出现了大量的研究活动。这些论文可以根据其研究重点分为几个主要类别。\n\n首先，有几篇论文专注于[[三维视频的质量评估The Quality Assessment Of 3d Video]]，特别是在点云成像的背景下。例如。\n- 论文2讨论了点云成像质量评估中的用户互动性问题，而 \n- 论文14探讨了在体积散射介质中使用相位空间测量进行三维成像。\n- 论文15提出了PCQM，一个用于彩色三维点云的全参考质量指标。\n\n其次，其他论文关注了[[沉浸式计算Immersive Computing]]领域的进展，特别是在移动沉浸式计算方面。例如。\n- 论文3讨论了这个领域的研究挑战和未来的道路。\n\n第三，有一些论文专注于[[3d Object Assessment(三维物体评估)]] 领域。例如。\n- 论文7讨论了正在进行的点云压缩标准化活动的问题。\n- 论文16讨论了Holoportation，一种实时的虚拟三维传送。\n\n第四，有几篇论文专注于[[3D视频中的沉浸体验The Experience Of Immersion In 3d Video]]，特别是在游戏方面。比如说。\n- 论文10介绍了一项关于测量和定义游戏中的沉浸感的研究，以及 \n- 论文11讨论了Project Starline，一个高保真的远程呈现系统。\n\n第五。\n- 论文1讨论了全息图时代的未来主义愿景。\n- 论文4讨论了用于3D合成视图评估的新质量指标的发展。\n- 论文5讨论了Lenslet光场图像编码。\n- 论文6讨论了三维物体的最佳视图选择的基准。\n- 论文8讨论了基于信息最大化的对比度失真图像的无参照物质量度量。\n- 论文9讨论了从捕捉到渲染。论文9讨论了从捕捉到渲染：具有六个自由度的体积化媒体传输。\n- 论文12讨论了格式塔心理学的原理。\n- 论文13讨论了为密集的三维物体重建学习有效的点云生成。\n- 论文17讨论了Holoportation。实时的虚拟三维传送。\n\n总的来说，很明显，3D视频研究是一个活跃和快速发展的领域，新的技术和进展经常被提出和研究。然而，在提高三维视频的质量方面仍有许多工作要做，特别是在点云成像、沉浸式计算和物体评估等领域。","id":"377e91b8fb710b74","x":-5829,"y":-5294,"width":985,"height":878,"color":"4"},
		{"type":"text","text":"1\tA Futuristic Vision of the Age of Holograms, 2016.\t[zotero](zotero://select/library/items/Z47WECED)\n2\tAlexiou, Evangelos, and Touradj Ebrahimi. “Exploiting User Interactivity in Quality Assessment of Point Cloud Imaging,” June 1, 2019, 1–6.\t[zotero](zotero://select/library/items/8AULPZFR)\n3\tBo Han, and Bo Han. “Mobile Immersive Computing: Research Challenges and the Road Ahead.” IEEE Communications Magazine 57, no. 10 (September 16, 2019): 112–18.\t[zotero](zotero://select/library/items/VRTX8A9M)\n4\tBosc, Emilie, Romuald Pepion, Patrick Le Callet, Martin Koppel, Patrick Ndjiki-Nya, Muriel Pressigout, and Luce Morin. “Towards a New Quality Metric for 3-D Synthesized View Assessment.” IEEE Journal of Selected Topics in Signal Processing 5, no. 7 (November 2011): 1332–43.\t[zotero](zotero://select/library/items/ZJ7PPMRL)\n5\tBrites, Catarina, João Ascenso, and Fernando Pereira. “Lenslet Light Field Image Coding: Classifying, Reviewing and Evaluating.” IEEE Transactions on Circuits and Systems for Video Technology 31, no. 1 (January 2021): 339–54.\t[zotero](zotero://select/library/items/K6NZN7IH)\n6\tDutagaci, Helin, Chun Pan Cheung, and Afzal Godil. “A Benchmark for Best View Selection of 3D Objects,” October 25, 2010, 45–50.\t[zotero](zotero://select/library/items/IAGPU4QN)\n7\tGraziosi, Danillo B., O. Nakagami, S. Kuma, Alexandre Zaghetto, Teruhiko Suzuki, and Ali Tabatabai. “An Overview of Ongoing Point Cloud Compression Standardization Activities: Video-Based (V-PCC) and Geometry-Based (G-PCC).” APSIPA Transactions on Signal and Information Processing 9, no. 1 (January 1, 2020).\t[zotero](zotero://select/library/items/EGKH4WRU)\n8\tGu, Ke, Weisi Lin, Guangtao Zhai, Xiaokang Yang, Wenjun Zhang, and Chang Wen Chen. “No-Reference Quality Metric of Contrast-Distorted Images Based on Information Maximization.” IEEE Transactions on Cybernetics 47, no. 12 (December 2017): 4559–65.\t[zotero](zotero://select/library/items/33MUE352)\n9\tHooft, Jeroen van der, Maria Torres Vega, Tim Wauters, Christian Timmerer, Ali C. Begen, Filip De Turck, Raimund Schatz, and Raimund Schatz. “From Capturing to Rendering: Volumetric Media Delivery with Six Degrees of Freedom.” IEEE Communications Magazine 58, no. 10 (2020): 49–55.\t[zotero](zotero://select/library/items/KA2GWMM9)\n10\tJennett, Charlene, Anna L. Cox, Paul Cairns, Samira Dhoparee, Andrew Epps, Tim Tijs, and Alison Walton. “Measuring and Defining the Experience of Immersion in Games.” International Journal of Human-Computer Studies \\/ International Journal of Man-Machine Studies 66, no. 9 (September 1, 2008): 641–61.\t[zotero](zotero://select/library/items/963BABZM)\n11\tJoseph, G. Desloge, Tommy Fortes, E. M. Gomez, Sascha Häberling, Hugues Hoppe, Andy, et al. “Project Starline: A High-Fidelity Telepresence System,” 2021.\t[zotero](zotero://select/library/items/JYFJ2HRR)\n12\tKurt Koffka, and K. Koffka. “Principles of Gestalt Psychology,” January 1, 1935.\t[zotero](zotero://select/library/items/CZRUF3EC)\n13\tLin, Chen-Hsuan, Chen Kong, and Simon Lucey. “Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction.” Proceedings of the AAAI Conference on Artificial Intelligence 32, no. 1 (April 27, 2018).\t[zotero](zotero://select/library/items/6NIGR5WH)\n14\tLiu, Hsiou-Yuan, Eric Jonas, Lei Tian, Jingshan Zhong, Benjamin Recht, and Laura Waller. “3D Imaging in Volumetric Scattering Media Using Phase-Space Measurements.” Optics Express 23, no. 11 (June 1, 2015): 14461–71.\t[zotero](zotero://select/library/items/RSKCPNQN)\n15\tMeynet, Gabriel, Yana Nehme, Julie Digne, and Guillaume Lavoué. “PCQM: A Full-Reference Quality Metric for Colored 3D Point Clouds,” May 26, 2020, 1–6.\t[zotero](zotero://select/library/items/VS9D3CJG)\n16\tOrts-Escolano, Sergio, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, et al. “Holoportation: Virtual 3D Teleportation in Real-Time.” In Proceedings of the 29th Annual Symposium on User Interface Software and Technology, 741–54. Tokyo Japan: ACM, 2016.\t[zotero](zotero://select/library/items/NX9INNMD)\n17\tPagés, Rafael, Konstantinos Amplianitis, David S. Monaghan, David Monaghan, Jan Ondřej, and Aljosa Smolic. “Affordable Content Creation for Free-Viewpoint Video and VR/AR Applications.” Journal of Visual Communication and Image Representation 53 (May 1, 2018): 192–201.\t[zotero](zotero://select/library/items/7UZY4TGS)\n18\tPantos, Roger, and William May. “\t[zotero](zotero://select/library/items/5Q64MSQC)\n19\tPark, Joun Sup, Philip A. Chou, Philip A. Chou, Philip A. Chou, and Jenq-Neng Hwang. “Rate-Utility Optimized Streaming of Volumetric Media for Augmented Reality.” IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, no. 1 (February 11, 2019): 149–62.\t[zotero](zotero://select/library/items/ZH4MCBWJ)\n20\t———. “Volumetric Media Streaming for Augmented Reality.” 2018 IEEE Global Communications Conference (GLOBECOM), December 1, 2018, 1–6.\t[zotero](zotero://select/library/items/HUEKBHFZ)\n21\tPearce, Warren, Suay M. Özkula, Amanda K. Greene, Lauren Teeling, Jennifer S. Bansard, Janna Joceli Omena, and Elaine Teixeira Rabello. “Visual Cross-Platform Analysis: Digital Methods to Research Social Media Images.” Information, Communication & Society 23, no. 2 (January 28, 2020): 161–80.\t[zotero](zotero://select/library/items/FHZ6KAXJ)\n22\tQi Liu, Honglei Su, Zhengfang Duanmu, Wentao Liu, and Zhou Wang. “Perceptual Quality Assessment of Colored 3D Point Clouds.” IEEE Transactions on Visualization and Computer Graphics, 2021.\t[zotero](zotero://select/library/items/CI8NGCFG)\n23\tQuach, Maurice, Aladine Chetouani, Giuseppe Valenzise, and Frederic Dufaux. “A Deep Perceptual Metric for 3D Point Clouds.” ArXiv:2102.12839 [Cs, Eess], February 25, 2021.\t[zotero](zotero://select/library/items/H5UWHVIE)\n24\tRossiSilvia, Silvia Rossi, OzcinarCagri, Cagri Ozcinar, SmolicAljosa, Aljosa Smolic, ToniLaura, and Laura Toni. “Do Users Behave Similarly in VR? Investigation of the User Influence on the System Design” 16, no. 2 (May 19, 2020): 1–26.\t[zotero](zotero://select/library/items/UBIPB3HK)\n25\tRusu, Radu Bogdan, and Steve Cousins. “3D Is Here: Point Cloud Library (PCL),” May 9, 2011, 1–4.\t[zotero](zotero://select/library/items/GUVTDIFH)\n26\tSaito, Shunsuke, Tomas Simon, Jason Saragih, and Hanbyul Joo. “PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization,” 84–93, 2020.\t[zotero](zotero://select/library/items/7KX5S4VA)\n27\tSchnabel, Ruwen, and Reinhard Klein. “Octree-Based Point-Cloud Compression,” July 29, 2006, 111–21.\t[zotero](zotero://select/library/items/7NV2UM5E)\n28\tSchreer, Oliver, Ingo Feldmann, Sylvain Renault, Marcus Zepp, Markus Worchel, Peter Eisert, and Peter Kauff. “Capture and 3D Video Processing of Volumetric Video,” September 1, 2019, 4310–14.\t[zotero](zotero://select/library/items/RWSICA3Q)\n29\tSchwarz, Sebastian, Marius Preda, Vittorio Baroncini, Madhukar Budagavi, Pablo Cesar, Philip A. Chou, Robert A. Cohen, et al. “Emerging MPEG Standards for Point Cloud Compression.” IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, no. 1 (March 2019): 133–48.\t[zotero](zotero://select/library/items/UZJMZT8D)\n30\tSeshadrinathan, Kalpana, Rajiv Soundararajan, Alan C. Bovik, and Lawrence K. Cormack. “Study of Subjective and Objective Quality Assessment of Video.” IEEE Transactions on Image Processing 19, no. 6 (June 1, 2010): 1427–41.\t[zotero](zotero://select/library/items/7WAEPZCZ)\n31\tSeufert, Michael, Sebastian Egger, Martin Slanina, Thomas Zinner, Tobias Hobfeld, and Phuoc Tran-Gia. “A Survey on Quality of Experience of\t[zotero](zotero://select/library/items/ZWBVR7AS)\n32\tSlater, Mel, Martin Usoh, and Anthony Steed. “Depth of Presence in Virtual Environments.” Presence: Teleoperators & Virtual Environments 3, no. 2 (January 1, 1994): 130–44.\t[zotero](zotero://select/library/items/EWQ2HAEK)\n33\tSodagar, I. “The MPEG-DASH Standard for Multimedia Streaming Over the Internet.” IEEE MultiMedia 18, no. 4 (October 1, 2011): 62–67.\t[zotero](zotero://select/library/items/QF2GH7SQ)\n34\tSparrow, Betsy, Jenny Liu, and Daniel M. Wegner. “Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips.” Science 333, no. 6043 (August 5, 2011): 776–78.\t[zotero](zotero://select/library/items/PX6DG56S)\n35\tTakacs, Barnabas, and Zsuzsanna Vincze. “Deep Authoring - an AI Tool Set for Creating Immersive MultiMedia Experiences.” Multimedia Tools and Applications 80, no. 20 (August 2021): 31105–34.\t[zotero](zotero://select/library/items/HMGF4B9J)\n36\tVlahakis, V., M. Ioannidis, John N. Karigiannis, M. Tsotros, M. Gounaris, Didier Stricker, Tim Gleue, P. Daehne, L. Almeida, and Luis Almeida. “Archeoguide: An Augmented Reality Guide for Archaeological Sites.” IEEE Computer Graphics and Applications 22, no. 5 (September 1, 2002): 52–60.\t[zotero](zotero://select/library/items/L4SFXI3S)\n37\tWang, Yue, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. “Dynamic Graph CNN for Learning on Point Clouds.” ACM Transactions on Graphics 38, no. 5 (October 10, 2019): 146:1-146:12.\t[zotero](zotero://select/library/items/S69ILLRG)\n38\tWitmer, Bob G., Michael J. Singer, and Michael J. Singer. “Measuring Presence in Virtual Environments: A Presence Questionnaire.” Teleoperators and Virtual Environments 7, no. 3 (June 1, 1998): 225–40.\t[zotero](zotero://select/library/items/T7388E4J)\n39\tYu, Lequan, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. “PU-Net: Point Cloud Upsampling Network,” 2790–99, 2018.\t[zotero](zotero://select/library/items/Z4RKIF2I)\n40\tZerman, Emin, Radhika Kulkarni, and Aljosa Smolic. “User Behaviour Analysis of Volumetric Video in Augmented Reality.” 2021 13th International Conference on Quality of Multimedia Experience (QoMEX), June 14, 2021, 129–32.\t[zotero](zotero://select/library/items/HNHEQD2K)\n41\tZerman, Emin, Néill O’Dwyer, Gareth W. Young, and Aljosa Smolic. “A Case Study on the Use of Volumetric Video in Augmented Reality for Cultural Heritage,” 2020.\t[zotero](zotero://select/library/items/6PWXUVN8)\n42\tZerman, Emin, Cagri Ozcinar, Pan Gao, and Aljosa Smolic. “Textured Mesh vs Coloured Point Cloud: A Subjective Study for Volumetric Video Compression,” May 26, 2020, 1–6.\t[zotero](zotero://select/library/items/ID9RK3R4)","id":"cfa32d155a0a2c92","x":-4844,"y":-5294,"width":538,"height":878},
		{"type":"text","text":"$$ \\begin{align} \\text{color } depth\\_to\\_ccmap(\\text{int } depth) \\\\ \\text{int } P1 &= 255 \\\\ \\text{int } P2 &= 10 \\\\ \\text{int } factor1 &= 17 \\\\ \\text{int } factor2 &= 25 \\\\ \\text{int } R &= \\lfloor\\frac{depth}{P1}\\rfloor*factor1 \\\\ \\text{int } _G &= depth\\ \\%\\ P1 \\\\ \\text{int } G &= \\lfloor\\frac{_G}{P2}\\rfloor*P2 \\\\ \\text{int } B &= \\lfloor\\frac{_G\\ \\%\\ P2}{factor2}\\rfloor \\\\ \\text{return } \\text{color}(R,G,B) \\\\ \\end{align} $$","id":"6074487d2c49d77a","x":-4637,"y":-4324,"width":527,"height":316},
		{"type":"text","text":"### 将13bit的深度数据无损储存到8bit 的彩色图中\n\n将13位深度数据无损地存储到8位彩色图像中是具有挑战性的。因为8位颜色值只能表示256种不同的颜色，而13位深度值可以表示8192种不同的深度。\n\n","id":"55d2293835e2a022","x":-4637,"y":-3999,"width":527,"height":142},
		{"type":"text","text":"# 设计过程\n[[V-PCC 容器格式归纳]] 的优化过程通常包括两个过程, 1) 基于静态的压缩. 2) 基于时间序列的压缩\n\n- 基于静态的压缩. 将探索新的编码方法, 将13bit 深度无损压缩到8bit 图像中.\n\t- 基于投影的编码原理是在MPEG关于点云压缩的提案征集（CfP）中提出的 [[基于投影的编码原则 (Projection-based coding principle) ]] 将三维点云数据转换成一组代表点云的二维图像。\n\t- 为了完整保留原始深度信息, 我们需要考虑一种新的方式以取代 如[[Linear map (线性映射)]] , [[Delta encoding (差分编码)]]等常用的有损压缩的技术。\n\t- 我们的目标则是在 [[RLE(Run-length encoding游程编码的技术)]] , [[DPCM (差分编码)]]等无损压缩的基础上提出更好的压缩算法, 达到[[编码技术(Data Compression Technique)]]的目的.\n- 基于时间序列的压缩, 则是对处理后的二维图像, 使用 [[Video compression codec (视频压缩编解码器)]] 进行编码. 如H.264或HEVC。这一步骤可能可以利用图像中的[[时间和空间冗余(temporal and spatial redundancy)]] 来减少最终数据大小。\n- 最后，编码后的视频帧被打包成一种[[Video Container formats(容器格式)]]，生成多种可供传输的实验样本, 如MP4或WebRTC 以备接下来的分析。\n---\n","id":"e32008166e985799","x":-5649,"y":-3413,"width":625,"height":50},
		{"type":"text","text":"# 失真和效率指标分析\n该分析主要涉及对上述时间序列压缩技术的最佳选择的调查\n我们根据不同的试验结果围绕视觉感知体验指标, 例如: \n- 对位置信息与色彩信息的失真, \n- 点云的分布(**distributions**)偏度,\n- 压缩效率(compression efficiency), 以及网络效率等.\n分别以客观与主观方法展开评估, 以衡量时间序列压缩方法是否具有突破性.\n客观评估:\n使用 [[Rate Distortion Curve]] , [[PSNR (peak signal-to-noise ratio)]] , [[SSIM（Structural Similarity Index)]] , [[PCQA (point cloud quality assessment)]] 作为压缩效率与失真之间的权衡指标.\n主观评估: 以教育领域为主题, 策划一次 AR 线上雕塑课程, 参与者约60人, 根据以上感知指数进行用户调研.  \n客观评估和主观的用户调研, 一方面将有助于该编码设计在时间序列压缩的最终结论,  并对接下来的案例研究提供试验依据. \n\n```\n\n```","id":"ea500511bc3633a2","x":-5649,"y":-3329,"width":625,"height":50},
		{"type":"text","text":"Microsoft, Facebook, Google 等重要机构在体积媒体研究领域的贡献；","id":"d153cccc954c4ffd","x":-5550,"y":-3794,"width":426,"height":91},
		{"type":"text","text":"V-PCC（视频点云压缩）的压缩编码过程包括静态压缩和时间序列压缩两个过程。\n\n首先，对于静态压缩，研究者提出了基于投影的编码原则，该原则将三维点云数据转化为一组二维图像，以完整的保留原始深度信息，并避免使用有损压缩技术，如线性映射和差分编码。因此，我们的目标是在无损压缩的基础上，如RLE（游程编码）和DPCM（差分编码），提出更好的压缩算法，并将13位深度无损压缩到8位图像中。\n\n接着，对于时间序列压缩，我们可以使用视频压缩编解码器，如H.264或HEVC，对处理后的二维图像进行编码，利用图像中的时间和空间冗余，来减小最终数据大小。\n\n最后，编码后的视频帧将被打包成视频容器格式，生成多种可供传输的实验样本，如MP4和WebRTC，以备接下来的分析。 总之，V-PCC（视频点云压缩）的压缩编码过程包括两个独立的步骤：一是静态压缩，即基于投影的编码原则，利用无损压缩技术，将13位深度无损压缩到8位图像中；二是时间序列压缩，利用视频压缩编解码器，将处理后的二维图像进行编码，从而减少最终数据大小，最终被打包成视频容器格式，生成多种可供传输的实验样本，以备接下来的分析。","id":"7a75570156491423","x":-4967,"y":-3510,"width":521,"height":150,"color":"4"},
		{"type":"text","text":"这项分析的目的是研究时间序列压缩技术的最佳选择。通过评估不同方法的失真和效率指标，我们的研究旨在确定压缩时间序列数据的最有希望的方法。\n\n实验结果基于视觉感知经验指数，如位置和颜色信息的失真、点云分布的偏斜度、压缩效率和网络效率。为了全面评估这些方法，我们采用了客观和主观的方法。\n\n客观评价是通过使用速率失真曲线、PSNR（峰值信噪比）、SSIM（结构相似度指数）和PCQA（点云质量评估）作为压缩效率和失真之间的平衡指标。另一方面，主观评价是通过一个针对教育领域的在线AR雕刻课程进行的，大约有60名参与者。根据上述感知指数，对参与者进行调查。\n\n通过结合客观和主观评价，我们的研究旨在就最适合时间序列压缩的编码设计得出最终结论，并为未来的案例研究提供实验证据。我们的研究结果将有助于推动时间序列压缩技术的发展，并对这些技术的有效性进行全面评估。","id":"1d164f6122c23736","x":-4967,"y":-3321,"width":521,"height":118,"color":"4"},
		{"type":"text","text":"其原理在于对目前流行的图像媒体格式进行改造, 如图像: Webp, 视频格式: Webm , 它的特点是拥有 RGBA 四个通道, 编码过程则是将采集的色彩与深度信息在四个通道中进行重新分配, 使文件能在储存色域数据之余, 拥有空间深度数据的储存能力. 其中特殊的编码规则能根据采集质量拓展空间深度.","id":"942f8cacfdec9d50","x":-4637,"y":-3811,"width":625,"height":176},
		{"type":"text","text":"# 3D Digital Memory Visualization. 立体数字记忆体","id":"78e9a3447a950d6b","x":-6323,"y":-5798,"width":621,"height":151},
		{"type":"text","text":"# Multimedia Hyperdimensional Archive（MHDA）多媒体超维档案","id":"8dfd2f22450f79eb","x":-6323,"y":-5593,"width":621,"height":134},
		{"type":"text","text":"Introduction 引言\n\n在数字时代，我们记录和保存记忆的方式已经发生了重大转变。随着3D技术的出现，捕捉和存储高维数字记忆的能力有了一个全新的层面。本文旨在探索3D视频作为立体数字记忆形式的潜力，研究它如何改变我们目前对数字互动的理解及其对保存人类文明的影响。\n\n首先，我们将深入研究3D视频改变我们与数字媒体互动方式的潜力。在日常生活场景中引入3D视频记录，提供了一种新的沉浸感和感知水平，使用户能够更好地记忆和想象事件或经历。视频点云，将声音、颜色、形状、空间和时间结合成一个多维特征，创造了一个沉浸式的多媒体空间，使用户能够更好地感知、记忆和想象事件或经历。\n\n此外，我们将研究3D视频在保存人类文明方面的潜力。目前，大多数博物馆的三维档案大多是静态数据，很少有非物质遗产档案。由于能够捕捉和存储高维数字记忆，3D视频为保存非物质遗产和人类历史提供了一种新的方式。当我们的社会更加重视非物质遗产而不是物质遗产时，3D视频可以被认为是一个多维度的超级档案。\n\n在教育领域, 3D视频作为一种立体的数字记忆体, 能容易地唤起学生的感知、记忆和想象力, 作为教育工作者能更有效地为学生呈现抽象的概念, 从而让学生掌握复杂的技能. 例如线上. 数字形式的雕塑课程。","id":"cb35b9fb8b326a00","x":-6309,"y":-5294,"width":460,"height":738},
		{"type":"text","text":"研究计划: 立体数字记忆体的研究\n\n研究背景:\n随着科技的进步，立体数字记忆体（3D media images）已经成为一种潜在的竞争对手，它们能够提供更高维度的记录形式。3D媒体有可能改变人们对数字交互模式的现有观念，将3D视频记录普及到日常生活场景。对于个人来说，3D图像可以储存更高维度的数字记忆，而Video Point Cloud能够将声音、颜色、形状、空间和时间多维度的特征结合在一起，形成一个全方位、丰富的多媒体空间。\n\n研究目标:\n\n通过对立体数字记忆体的研究，探究3D媒体图像相对于2D形式在未来竞争力的差异。\n深入了解Video Point Cloud在多媒体空间中的应用。\n分析3D图像如何能够更好地记录和保存人类文明和非物质遗产。\n探讨立体数字记忆体对数字交互模式的潜在影响。\n研究方法:\n\n文献综述法，对相关研究进行归纳和总结。\n实验法，通过实验验证立体数字记忆体的可行性和优越性。\n案\n例分析法，收集和分析已有3D媒体图像和Video Point Cloud的应用案例。\n\n调查法，对专家和使用者进行问卷调查，了解他们对立体数字记忆体的看法和需求。\n研究期限: 12个月\n\n研究预期成果:\n\n建立立体数字记忆体的理论框架，阐述其在未来竞争力方面的优势。\n提出3D媒体图像和Video Point Cloud的应用方法和技术指南。\n提出关于3D图像在记录和保存人类文明和非物质遗产方面的建议和解决方案。\n提出对数字交互模式的潜在影响的研究结论。\n以上就是立体数字记忆体研究的计划, 更细致的研究方案可能还需要根据研究的具体内容和目的来继续讨论和完善,\n研究计划实施过程中，还需要注意以下事项:\n\n数据收集: 我们需要确保数据的准确性和可靠性，在收集数据时要采用合理的方法和标准。\n\n数据分析: 我们需要运用合适的统计分析方法和技术来分析数据，以得出显著的结论。\n\n保证研究的可重复性: 我们需要确保研究的可重复性, 尽量使用重复研究或类比研究来验证和巩固结论。\n\n研究的可推广性: 我们需要考虑研究结果的可推广性，以便将研究结果应用到其他领域或行业中。\n\n保证研究的合法性: 我们需要确保研究合法,如遵循专业行为准则和隐私政策,确保研究的公正性和道德性。","id":"c0495de6e1aaaccb","x":-6145,"y":-3910,"width":426,"height":162},
		{"type":"text","text":"# 压缩编码方法\n探索一种便于传输与储存的 Video-based point cloud 编码方式","id":"8121ab8fbbb34d91","x":-6145,"y":-3393,"width":426,"height":184},
		{"id":"a3172ebd86b6fc4a","type":"file","file":"V_Abstract.md","x":-8080,"y":-6100,"width":660,"height":300},
		{"type":"text","text":"### 我们的目标是 ( 如何解决这些问题? )\n\n1. 这将提高3D视频中位置信息的准确性，使人类的视觉体验更好。例如将解决13位数据储存到8位图像中.\n\n2. 它还将使3D视频的压缩效率更高，这将降低互联网成本并允许更多的使用。\n\n3. 最后, 标准化, 一种让每个人都容易使用的解码和发送媒体的方式。","id":"0d79bb603804c5fe","x":-8000,"y":-5180,"width":460,"height":338},
		{"type":"text","text":"### 存在这些问题\n\n1. 数字教育并非成熟\n2. 学习成果\n3. ","id":"892bcdc591c2092c","x":-7990,"y":-5560,"width":460,"height":140},
		{"type":"text","text":"数字教育并非成熟","id":"e910f91177657983","x":-7600,"y":-5360,"width":460,"height":140}
	],
	"edges":[
		{"id":"0804acf4199627fa","fromNode":"fcf93c941827380b","fromSide":"right","toNode":"3fc79c67c708492f","toSide":"left"},
		{"id":"4038caa864ee70ba","fromNode":"3fc79c67c708492f","fromSide":"right","toNode":"696b8e874bcac86e","toSide":"left"},
		{"id":"705cb0176f38fc7f","fromNode":"3fc79c67c708492f","fromSide":"right","toNode":"63ee28f68a95fb95","toSide":"left"},
		{"id":"77468cde7c65dedd","fromNode":"c86458fcacd42b8d","fromSide":"right","toNode":"29790edb3c2976f2","toSide":"left"},
		{"id":"d077ab8f13e7bf3c","fromNode":"9c4f7ac1b3f6d73d","fromSide":"left","toNode":"696b8e874bcac86e","toSide":"right"},
		{"id":"6cb402c3cebcc8a0","fromNode":"9c4f7ac1b3f6d73d","fromSide":"left","toNode":"63ee28f68a95fb95","toSide":"right"},
		{"id":"c32ff847f093848e","fromNode":"9c4f7ac1b3f6d73d","fromSide":"left","toNode":"29790edb3c2976f2","toSide":"right"},
		{"id":"ceb369b551e08f65","fromNode":"fb1f28d9369186a0","fromSide":"left","toNode":"696b8e874bcac86e","toSide":"right"},
		{"id":"1ddccf563d4e54c1","fromNode":"fb1f28d9369186a0","fromSide":"left","toNode":"63ee28f68a95fb95","toSide":"right"},
		{"id":"dac2979e96e06611","fromNode":"74055f3fba8e08b7","fromSide":"left","toNode":"63ee28f68a95fb95","toSide":"right"},
		{"id":"c5716c06b8ad0b75","fromNode":"74055f3fba8e08b7","fromSide":"left","toNode":"696b8e874bcac86e","toSide":"right"},
		{"id":"a9808425b98d2694","fromNode":"96ba050e6dd2bf45","fromSide":"right","toNode":"fcf93c941827380b","toSide":"left"},
		{"id":"12613da4934c3b3b","fromNode":"96ba050e6dd2bf45","fromSide":"right","toNode":"ac29c2366f10c2e0","toSide":"left"},
		{"id":"cce6d405791e997b","fromNode":"c86458fcacd42b8d","fromSide":"right","toNode":"913bf9a2b96e1330","toSide":"left"},
		{"id":"1d44143dc4b87f70","fromNode":"9c4f7ac1b3f6d73d","fromSide":"left","toNode":"03cc74913b2024ae","toSide":"right"},
		{"id":"accaa08ab3fd0adc","fromNode":"fb1f28d9369186a0","fromSide":"left","toNode":"03cc74913b2024ae","toSide":"right"},
		{"id":"94f6bc867234395f","fromNode":"74055f3fba8e08b7","fromSide":"left","toNode":"03cc74913b2024ae","toSide":"right"},
		{"id":"b7fddd888fbbbdea","fromNode":"3fc79c67c708492f","fromSide":"right","toNode":"c86458fcacd42b8d","toSide":"left"},
		{"id":"7d2611da0833b37f","fromNode":"9b566c053431d46f","fromSide":"right","toNode":"64fd1d55232060bd","toSide":"left"},
		{"id":"7d90774746bbc16b","fromNode":"3272a2f0d8a45548","fromSide":"right","toNode":"c1b03e01e3eda849","toSide":"left"},
		{"id":"0da8d6d984bc912c","fromNode":"696b8e874bcac86e","fromSide":"bottom","toNode":"03cc74913b2024ae","toSide":"top"},
		{"id":"2074804d981f41c9","fromNode":"03cc74913b2024ae","fromSide":"bottom","toNode":"63ee28f68a95fb95","toSide":"top"},
		{"id":"54f4c3f13e2e0c21","fromNode":"63ee28f68a95fb95","fromSide":"bottom","toNode":"c86458fcacd42b8d","toSide":"top"},
		{"id":"fb6d37ada9066535","fromNode":"63dbea433defb9b9","fromSide":"bottom","toNode":"c86458fcacd42b8d","toSide":"top"},
		{"id":"76f7a6825287b746","fromNode":"9c4f7ac1b3f6d73d","fromSide":"right","toNode":"ec0ba1c8b1f23246","toSide":"left"},
		{"id":"2674ba08e3cbb2ce","fromNode":"f2ece9f8b768cebe","fromSide":"right","toNode":"3279037cc241a722","toSide":"left"},
		{"id":"170b8876b56b3876","fromNode":"3279037cc241a722","fromSide":"right","toNode":"4192f269ff64a4bc","toSide":"left"},
		{"id":"d735814bf2de911a","fromNode":"34f888468d0c7a43","fromSide":"right","toNode":"533b7c8028630865","toSide":"left"},
		{"id":"c5a10f85f91c3a41","fromNode":"34f888468d0c7a43","fromSide":"right","toNode":"f9160734ff6d92af","toSide":"left"},
		{"id":"d45c2124d9702fb5","fromNode":"f61a9009f8544e0b","fromSide":"right","toNode":"c95795c549cb0e78","toSide":"left"},
		{"id":"538e9c5100b08eea","fromNode":"f61a9009f8544e0b","fromSide":"right","toNode":"69aa34328949b631","toSide":"left"},
		{"id":"40139a34b10c5eb4","fromNode":"f2ece9f8b768cebe","fromSide":"right","toNode":"34f888468d0c7a43","toSide":"left"},
		{"id":"5b55da384ef9e8cd","fromNode":"f2ece9f8b768cebe","fromSide":"right","toNode":"f61a9009f8544e0b","toSide":"left"},
		{"id":"6b280cd135c91a01","fromNode":"f32dbb7acb0603d9","fromSide":"right","toNode":"775f9d5f1e311914","toSide":"left"},
		{"id":"c39b95ae0ec911ec","fromNode":"300e890e57946fd9","fromSide":"right","toNode":"d73aac9527916b17","toSide":"left"},
		{"id":"4796ba98607f4093","fromNode":"343305c491b89925","fromSide":"right","toNode":"0bd8c1ed85681e5c","toSide":"left"},
		{"id":"7578278a4567c0c0","fromNode":"3279037cc241a722","fromSide":"right","toNode":"b6796c2b96aa4107","toSide":"left"},
		{"id":"a9208526caf16d96","fromNode":"3279037cc241a722","fromSide":"right","toNode":"bc7fa4f484eabb99","toSide":"left"},
		{"id":"7cd9d32bfd64beec","fromNode":"1cc301bc384e5fbb","fromSide":"right","toNode":"ec0ba1c8b1f23246","toSide":"left"},
		{"id":"0b0ee1fb7809f7b0","fromNode":"80c94c9e96c077a7","fromSide":"right","toNode":"300e890e57946fd9","toSide":"left"},
		{"id":"1935b2f69ffc7d17","fromNode":"343305c491b89925","fromSide":"right","toNode":"6281c3fcbcb550fa","toSide":"left"},
		{"id":"8c7849d537f615c5","fromNode":"916cb9fc16978ad4","fromSide":"right","toNode":"c0495de6e1aaaccb","toSide":"left","label":"1"},
		{"id":"3d9ab51051db24d6","fromNode":"916cb9fc16978ad4","fromSide":"right","toNode":"11bdcb34b6b608cd","toSide":"left","label":"3"},
		{"id":"1392c634261457d4","fromNode":"c0495de6e1aaaccb","fromSide":"right","toNode":"d153cccc954c4ffd","toSide":"left"},
		{"id":"bfddd42b7dbad8aa","fromNode":"8121ab8fbbb34d91","fromSide":"right","toNode":"e32008166e985799","toSide":"left"},
		{"id":"bb863cc8f8cb1c36","fromNode":"8121ab8fbbb34d91","fromSide":"right","toNode":"ea500511bc3633a2","toSide":"left"},
		{"id":"c2d5b69602ba0710","fromNode":"916cb9fc16978ad4","fromSide":"right","toNode":"8121ab8fbbb34d91","toSide":"left","label":"2"},
		{"id":"2fe3cee3f4a42a68","fromNode":"11bdcb34b6b608cd","fromSide":"right","toNode":"a8084d40c4e0c591","toSide":"left"},
		{"id":"5ee01abbb4dbab95","fromNode":"11bdcb34b6b608cd","fromSide":"right","toNode":"f3d598a5d7242c7e","toSide":"left"},
		{"id":"a96ef690879a34b3","fromNode":"a8084d40c4e0c591","fromSide":"right","toNode":"f0be9db1f621faa4","toSide":"left"},
		{"id":"7f1dac91f6eb3759","fromNode":"f3d598a5d7242c7e","fromSide":"right","toNode":"526d69c85685773a","toSide":"left"},
		{"id":"85ce58f7aef00fab","fromNode":"e32008166e985799","fromSide":"right","toNode":"7a75570156491423","toSide":"left"},
		{"id":"1ba2124a82d15440","fromNode":"ea500511bc3633a2","fromSide":"right","toNode":"1d164f6122c23736","toSide":"left"},
		{"id":"82069b45f79a6b81","fromNode":"1ff88517baa2f9d6","fromSide":"right","toNode":"e3e4e15821089ea3","toSide":"left"},
		{"id":"c059c63faeb4bc28","fromNode":"1ff88517baa2f9d6","fromSide":"right","toNode":"db79a0f70c6add5a","toSide":"left"},
		{"id":"ca6b60fd7e11f8b7","fromNode":"1ff88517baa2f9d6","fromSide":"right","toNode":"019b10df04e20130","toSide":"left"},
		{"id":"9b78a433da74592b","fromNode":"1ff88517baa2f9d6","fromSide":"right","toNode":"e9b272eb29e9451d","toSide":"left"},
		{"id":"24e7ca29f2d6a4eb","fromNode":"916cb9fc16978ad4","fromSide":"right","toNode":"fb923cc96128e012","toSide":"left"}
	]
}